// -----// IR Dump Before DefaultTppPasses (default-tpp-passes) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before TransformDropSchedule (transform-drop-schedule) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before RewriteBatchMatmulToMatmul (rewrite-batch-matmul-to-matmul) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before TppMapping (tpp-mapping) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before ConvInitSimplify (conv-init-simplify) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before PackConv2DNhwcHwcf (pack-conv2DNhwcHwcf) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before PackConv2DNchwFchw (pack-conv2DNchwFchw) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before RewriteConvToMatmulOrBrgemm (rewrite-conv-to-matmul-or-brgemm) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before PackMatmul (pack-matmul) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d1)>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = linalg.matmul ins(%collapsed, %transposed : tensor<64x262144xf32>, tensor<262144x128xf32>) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg1, %3 : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<64x128xf32>
    %5 = linalg.generic {indexing_maps = [#map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%4 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %6 = arith.cmpf ugt, %in, %cst : f32
      %7 = arith.select %6, %in, %cst : f32
      linalg.yield %7 : f32
    } -> tensor<64x128xf32>
    return %5 : tensor<64x128xf32>
  }
}


// -----// IR Dump After PackMatmul (pack-matmul) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1) -> (d1)>
#map4 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_1 = tensor.pack %2 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%pack_1 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %9 = arith.mulf %in, %in_2 : f32
      %10 = arith.addf %out, %9 : f32
      linalg.yield %10 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %6 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %7 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel"]} ins(%arg1, %unpack : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %9 = arith.addf %in, %in_2 : f32
      linalg.yield %9 : f32
    } -> tensor<64x128xf32>
    %8 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel"]} ins(%7 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %9 = arith.cmpf ugt, %in, %cst : f32
      %10 = arith.select %9, %in, %cst : f32
      linalg.yield %10 : f32
    } -> tensor<64x128xf32>
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before PackVNNI (pack-vnni) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1) -> (d1)>
#map4 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_1 = tensor.pack %2 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%pack_1 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %9 = arith.mulf %in, %in_2 : f32
      %10 = arith.addf %out, %9 : f32
      linalg.yield %10 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %6 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %7 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel"]} ins(%arg1, %unpack : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %9 = arith.addf %in, %in_2 : f32
      linalg.yield %9 : f32
    } -> tensor<64x128xf32>
    %8 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel"]} ins(%7 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %9 = arith.cmpf ugt, %in, %cst : f32
      %10 = arith.select %9, %in, %cst : f32
      linalg.yield %10 : f32
    } -> tensor<64x128xf32>
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before PropagatePackUnPack (propagate-pack-and-unpack) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1) -> (d1)>
#map4 = affine_map<(d0, d1) -> (d0, d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_1 = tensor.pack %2 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%pack_1 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %9 = arith.mulf %in, %in_2 : f32
      %10 = arith.addf %out, %9 : f32
      linalg.yield %10 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %6 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %7 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel"]} ins(%arg1, %unpack : tensor<128xf32>, tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %9 = arith.addf %in, %in_2 : f32
      linalg.yield %9 : f32
    } -> tensor<64x128xf32>
    %8 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel"]} ins(%7 : tensor<64x128xf32>) outs(%1 : tensor<64x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %9 = arith.cmpf ugt, %in, %cst : f32
      %10 = arith.select %9, %in, %cst : f32
      linalg.yield %10 : f32
    } -> tensor<64x128xf32>
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump After PropagatePackUnPack (propagate-pack-and-unpack) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_1 = tensor.pack %2 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%pack_1 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_7: f32, %out: f32):
      %14 = arith.mulf %in, %in_7 : f32
      %15 = arith.addf %out, %14 : f32
      linalg.yield %15 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %6 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %7 = tensor.empty() : tensor<2x4x32x32xf32>
    %8 = tensor.empty() : tensor<4x32xf32>
    %pack_2 = tensor.pack %arg1 inner_dims_pos = [0] inner_tiles = [32] into %8 : tensor<128xf32> -> tensor<4x32xf32>
    %9 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_3 = tensor.pack %unpack inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %9 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %10 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%pack_2, %pack_3 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%7 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_7: f32, %out: f32):
      %14 = arith.addf %in, %in_7 : f32
      linalg.yield %14 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack_4 = tensor.unpack %10 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %11 = tensor.empty() : tensor<2x4x32x32xf32>
    %12 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_5 = tensor.pack %unpack_4 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %12 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %13 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%pack_5 : tensor<2x4x32x32xf32>) outs(%11 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %14 = arith.cmpf ugt, %in, %cst : f32
      %15 = arith.select %14, %in, %cst : f32
      linalg.yield %15 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack_6 = tensor.unpack %13 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack_6 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before ConstantFoldPack (constant-fold-pack) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_1 = tensor.pack %2 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%pack_1 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_7: f32, %out: f32):
      %14 = arith.mulf %in, %in_7 : f32
      %15 = arith.addf %out, %14 : f32
      linalg.yield %15 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %6 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %7 = tensor.empty() : tensor<2x4x32x32xf32>
    %8 = tensor.empty() : tensor<4x32xf32>
    %pack_2 = tensor.pack %arg1 inner_dims_pos = [0] inner_tiles = [32] into %8 : tensor<128xf32> -> tensor<4x32xf32>
    %9 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_3 = tensor.pack %unpack inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %9 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %10 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%pack_2, %pack_3 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%7 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_7: f32, %out: f32):
      %14 = arith.addf %in, %in_7 : f32
      linalg.yield %14 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack_4 = tensor.unpack %10 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %11 = tensor.empty() : tensor<2x4x32x32xf32>
    %12 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_5 = tensor.pack %unpack_4 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %12 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %13 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%pack_5 : tensor<2x4x32x32xf32>) outs(%11 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %14 = arith.cmpf ugt, %in, %cst : f32
      %15 = arith.select %14, %in, %cst : f32
      linalg.yield %15 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack_6 = tensor.unpack %13 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack_6 : tensor<64x128xf32>
  }
}


// -----// IR Dump After ConstantFoldPack (constant-fold-pack) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %15 = arith.mulf %in, %in_6 : f32
      %16 = arith.addf %out, %15 : f32
      linalg.yield %16 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %7 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %8 = tensor.empty() : tensor<2x4x32x32xf32>
    %9 = tensor.empty() : tensor<4x32xf32>
    %pack_1 = tensor.pack %arg1 inner_dims_pos = [0] inner_tiles = [32] into %9 : tensor<128xf32> -> tensor<4x32xf32>
    %10 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_2 = tensor.pack %unpack inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %10 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %11 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%pack_1, %pack_2 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%8 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %15 = arith.addf %in, %in_6 : f32
      linalg.yield %15 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack_3 = tensor.unpack %11 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %12 = tensor.empty() : tensor<2x4x32x32xf32>
    %13 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_4 = tensor.pack %unpack_3 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %13 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %14 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%pack_4 : tensor<2x4x32x32xf32>) outs(%12 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %15 = arith.cmpf ugt, %in, %cst : f32
      %16 = arith.select %15, %in, %cst : f32
      linalg.yield %16 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack_5 = tensor.unpack %14 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack_5 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before SimplifyAndCanonicalizePack (simplify-pack) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %15 = arith.mulf %in, %in_6 : f32
      %16 = arith.addf %out, %15 : f32
      linalg.yield %16 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %7 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %8 = tensor.empty() : tensor<2x4x32x32xf32>
    %9 = tensor.empty() : tensor<4x32xf32>
    %pack_1 = tensor.pack %arg1 inner_dims_pos = [0] inner_tiles = [32] into %9 : tensor<128xf32> -> tensor<4x32xf32>
    %10 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_2 = tensor.pack %unpack inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %10 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %11 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%pack_1, %pack_2 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%8 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_6: f32, %out: f32):
      %15 = arith.addf %in, %in_6 : f32
      linalg.yield %15 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack_3 = tensor.unpack %11 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    %12 = tensor.empty() : tensor<2x4x32x32xf32>
    %13 = tensor.empty() : tensor<2x4x32x32xf32>
    %pack_4 = tensor.pack %unpack_3 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %13 : tensor<64x128xf32> -> tensor<2x4x32x32xf32>
    %14 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%pack_4 : tensor<2x4x32x32xf32>) outs(%12 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %15 = arith.cmpf ugt, %in, %cst : f32
      %16 = arith.select %15, %in, %cst : f32
      linalg.yield %16 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack_5 = tensor.unpack %14 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack_5 : tensor<64x128xf32>
  }
}


// -----// IR Dump After SimplifyAndCanonicalizePack (simplify-pack) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %12 = arith.mulf %in, %in_1 : f32
      %13 = arith.addf %out, %12 : f32
      linalg.yield %13 : f32
    } -> tensor<2x4x32x32xf32>
    %8 = tensor.empty() : tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %9 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%expanded, %7 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%8 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %12 = arith.addf %in, %in_1 : f32
      linalg.yield %12 : f32
    } -> tensor<2x4x32x32xf32>
    %10 = tensor.empty() : tensor<2x4x32x32xf32>
    %11 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<2x4x32x32xf32>) outs(%10 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %12 = arith.cmpf ugt, %in, %cst : f32
      %13 = arith.select %12, %in, %cst : f32
      linalg.yield %13 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %11 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %12 = arith.mulf %in, %in_1 : f32
      %13 = arith.addf %out, %12 : f32
      linalg.yield %13 : f32
    } -> tensor<2x4x32x32xf32>
    %8 = tensor.empty() : tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %9 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%expanded, %7 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%8 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %12 = arith.addf %in, %in_1 : f32
      linalg.yield %12 : f32
    } -> tensor<2x4x32x32xf32>
    %10 = tensor.empty() : tensor<2x4x32x32xf32>
    %11 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<2x4x32x32xf32>) outs(%10 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %12 = arith.cmpf ugt, %in, %cst : f32
      %13 = arith.select %12, %in, %cst : f32
      linalg.yield %13 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %11 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %12 = arith.mulf %in, %in_1 : f32
      %13 = arith.addf %out, %12 : f32
      linalg.yield %13 : f32
    } -> tensor<2x4x32x32xf32>
    %8 = tensor.empty() : tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %9 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%expanded, %7 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%8 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %12 = arith.addf %in, %in_1 : f32
      linalg.yield %12 : f32
    } -> tensor<2x4x32x32xf32>
    %10 = tensor.empty() : tensor<2x4x32x32xf32>
    %11 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<2x4x32x32xf32>) outs(%10 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %12 = arith.cmpf ugt, %in, %cst : f32
      %13 = arith.select %12, %in, %cst : f32
      linalg.yield %13 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %11 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %12 = arith.mulf %in, %in_1 : f32
      %13 = arith.addf %out, %12 : f32
      linalg.yield %13 : f32
    } -> tensor<2x4x32x32xf32>
    %8 = tensor.empty() : tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %9 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%expanded, %7 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%8 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %12 = arith.addf %in, %in_1 : f32
      linalg.yield %12 : f32
    } -> tensor<2x4x32x32xf32>
    %10 = tensor.empty() : tensor<2x4x32x32xf32>
    %11 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%9 : tensor<2x4x32x32xf32>) outs(%10 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %12 = arith.cmpf ugt, %in, %cst : f32
      %13 = arith.select %12, %in, %cst : f32
      linalg.yield %13 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %11 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump After CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %10 = arith.mulf %in, %in_1 : f32
      %11 = arith.addf %out, %10 : f32
      linalg.yield %11 : f32
    } -> tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%expanded, %7 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%5 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %10 = arith.addf %in, %in_1 : f32
      linalg.yield %10 : f32
    } -> tensor<2x4x32x32xf32>
    %9 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : tensor<2x4x32x32xf32>) outs(%5 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %10 = arith.cmpf ugt, %in, %cst : f32
      %11 = arith.select %10, %in, %cst : f32
      linalg.yield %11 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %9 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump After Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %10 = arith.mulf %in, %in_1 : f32
      %11 = arith.addf %out, %10 : f32
      linalg.yield %11 : f32
    } -> tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%expanded, %7 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%5 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %10 = arith.addf %in, %in_1 : f32
      linalg.yield %10 : f32
    } -> tensor<2x4x32x32xf32>
    %9 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : tensor<2x4x32x32xf32>) outs(%5 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %10 = arith.cmpf ugt, %in, %cst : f32
      %11 = arith.select %10, %in, %cst : f32
      linalg.yield %11 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %9 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump Before TileConsumerAndFuseProducers (tile-consumer-and-fuse-producers) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d2, d3, d5)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d1, d2, d5, d4)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5) -> (d0, d1, d3, d4)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d1, d3)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %transposed = linalg.transpose ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) permutation = [1, 0] 
    %1 = tensor.empty() : tensor<64x128xf32>
    %2 = linalg.fill ins(%cst : f32) outs(%1 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %3 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %4 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %transposed outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %5 = tensor.empty() : tensor<2x4x32x32xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<2x4x32x32xf32>) -> tensor<2x4x32x32xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "reduction", "parallel", "parallel", "reduction"]} ins(%pack, %pack_0 : tensor<2x8192x32x32xf32>, tensor<4x8192x32x32xf32>) outs(%6 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %10 = arith.mulf %in, %in_1 : f32
      %11 = arith.addf %out, %10 : f32
      linalg.yield %11 : f32
    } -> tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%expanded, %7 : tensor<4x32xf32>, tensor<2x4x32x32xf32>) outs(%5 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %10 = arith.addf %in, %in_1 : f32
      linalg.yield %10 : f32
    } -> tensor<2x4x32x32xf32>
    %9 = linalg.generic {indexing_maps = [#map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%8 : tensor<2x4x32x32xf32>) outs(%5 : tensor<2x4x32x32xf32>) {
    ^bb0(%in: f32, %out: f32):
      %10 = arith.cmpf ugt, %in, %cst : f32
      %11 = arith.select %10, %in, %cst : f32
      linalg.yield %11 : f32
    } -> tensor<2x4x32x32xf32>
    %unpack = tensor.unpack %9 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %2 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump After TileConsumerAndFuseProducers (tile-consumer-and-fuse-producers) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %6 = tensor.empty() : tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %6) -> (tensor<2x4x32x32xf32>) {
      %8 = tensor.empty() : tensor<32x32xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %10 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.generic {indexing_maps = [#map2, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %10 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %14 = arith.addf %in, %in_4 : f32
        linalg.yield %14 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<2x4x32x32xf32> to tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%12 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %14 = arith.cmpf ugt, %in, %cst : f32
        %15 = arith.select %14, %in, %cst : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %13 into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x4x32x32xf32>
      }
    }
    %unpack = tensor.unpack %7 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump Before SimplifyAndCanonicalizePack (simplify-pack) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %6 = tensor.empty() : tensor<2x4x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %6) -> (tensor<2x4x32x32xf32>) {
      %8 = tensor.empty() : tensor<32x32xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %10 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.generic {indexing_maps = [#map2, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %10 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %14 = arith.addf %in, %in_4 : f32
        linalg.yield %14 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<2x4x32x32xf32> to tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%12 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %14 = arith.cmpf ugt, %in, %cst : f32
        %15 = arith.select %14, %in, %cst : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %13 into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x4x32x32xf32>
      }
    }
    %unpack = tensor.unpack %7 inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %3 : tensor<2x4x32x32xf32> -> tensor<64x128xf32>
    return %unpack : tensor<64x128xf32>
  }
}


// -----// IR Dump After SimplifyAndCanonicalizePack (simplify-pack) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %6 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %7 = affine.apply #map2(%arg3)
      %8 = affine.apply #map2(%arg4)
      %9 = tensor.empty() : tensor<32x32xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %11 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%10 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %12 = tensor.empty() : tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %11 : tensor<32xf32>, tensor<32x32xf32>) outs(%12 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %15 = arith.addf %in, %in_4 : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%7, %8] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %14 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%13 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %15 = arith.cmpf ugt, %in, %cst : f32
        %16 = arith.select %15, %in, %cst : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %14 into %arg5[%7, %8] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %6 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %6 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %7 = affine.apply #map2(%arg3)
      %8 = affine.apply #map2(%arg4)
      %9 = tensor.empty() : tensor<32x32xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %11 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%10 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %12 = tensor.empty() : tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %11 : tensor<32xf32>, tensor<32x32xf32>) outs(%12 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %15 = arith.addf %in, %in_4 : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%7, %8] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %14 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%13 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %15 = arith.cmpf ugt, %in, %cst : f32
        %16 = arith.select %15, %in, %cst : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %14 into %arg5[%7, %8] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %6 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %6 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %7 = affine.apply #map2(%arg3)
      %8 = affine.apply #map2(%arg4)
      %9 = tensor.empty() : tensor<32x32xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %11 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%10 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %12 = tensor.empty() : tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %11 : tensor<32xf32>, tensor<32x32xf32>) outs(%12 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %15 = arith.addf %in, %in_4 : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%7, %8] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %14 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%13 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %15 = arith.cmpf ugt, %in, %cst : f32
        %16 = arith.select %15, %in, %cst : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %14 into %arg5[%7, %8] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %6 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %6 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %7 = affine.apply #map2(%arg3)
      %8 = affine.apply #map2(%arg4)
      %9 = tensor.empty() : tensor<32x32xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %11 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%10 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %12 = tensor.empty() : tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %11 : tensor<32xf32>, tensor<32x32xf32>) outs(%12 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %15 = arith.addf %in, %in_4 : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%7, %8] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %14 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%13 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %15 = arith.cmpf ugt, %in, %cst : f32
        %16 = arith.select %15, %in, %cst : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %14 into %arg5[%7, %8] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %6 : tensor<64x128xf32>
  }
}


// -----// IR Dump After CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %6 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %7 = affine.apply #map2(%arg3)
      %8 = affine.apply #map2(%arg4)
      %9 = tensor.empty() : tensor<32x32xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %11 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%10 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %12 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %11 : tensor<32xf32>, tensor<32x32xf32>) outs(%9 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %14 = arith.addf %in, %in_4 : f32
        linalg.yield %14 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%7, %8] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%12 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %14 = arith.cmpf ugt, %in, %cst : f32
        %15 = arith.select %14, %in, %cst : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %13 into %arg5[%7, %8] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %6 : tensor<64x128xf32>
  }
}


// -----// IR Dump After Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %6 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %7 = affine.apply #map2(%arg3)
      %8 = affine.apply #map2(%arg4)
      %9 = tensor.empty() : tensor<32x32xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %11 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%10 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %12 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %11 : tensor<32xf32>, tensor<32x32xf32>) outs(%9 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %14 = arith.addf %in, %in_4 : f32
        linalg.yield %14 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%7, %8] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%12 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %14 = arith.cmpf ugt, %in, %cst : f32
        %15 = arith.select %14, %in, %cst : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %13 into %arg5[%7, %8] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %6 : tensor<64x128xf32>
  }
}


// -----// IR Dump After TppMapping (tpp-mapping) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %6 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %7 = affine.apply #map2(%arg3)
      %8 = affine.apply #map2(%arg4)
      %9 = tensor.empty() : tensor<32x32xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %11 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%10 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %12 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %11 : tensor<32xf32>, tensor<32x32xf32>) outs(%9 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %14 = arith.addf %in, %in_4 : f32
        linalg.yield %14 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%7, %8] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%12 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %14 = arith.cmpf ugt, %in, %cst : f32
        %15 = arith.select %14, %in, %cst : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %13 into %arg5[%7, %8] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %6 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before LowerPacksAndUnPacks (lower-packs-unpacks) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %pack = tensor.pack %collapsed inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %4 : tensor<64x262144xf32> -> tensor<2x8192x32x32xf32>
    %5 = tensor.empty() : tensor<4x8192x32x32xf32>
    %pack_0 = tensor.pack %1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1] inner_tiles = [32, 32] into %5 : tensor<262144x128xf32> -> tensor<4x8192x32x32xf32>
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %6 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %7 = affine.apply #map2(%arg3)
      %8 = affine.apply #map2(%arg4)
      %9 = tensor.empty() : tensor<32x32xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%9 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %pack[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %pack_0[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %11 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_1 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%10 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %12 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_2, %11 : tensor<32xf32>, tensor<32x32xf32>) outs(%9 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_4: f32, %out: f32):
        %14 = arith.addf %in, %in_4 : f32
        linalg.yield %14 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_3 = tensor.extract_slice %arg5[%7, %8] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%12 : tensor<32x32xf32>) outs(%extracted_slice_3 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %14 = arith.cmpf ugt, %in, %cst : f32
        %15 = arith.select %14, %in, %cst : f32
        linalg.yield %15 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %13 into %arg5[%7, %8] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %6 : tensor<64x128xf32>
  }
}


// -----// IR Dump After LowerPacksAndUnPacks (lower-packs-unpacks) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before DecomposeAggregatedOps (decompose-aggregated-ops) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before Bufferize (bufferize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before DuplicateFill (duplicate-fill) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before EmptyTensorElimination (eliminate-empty-tensors) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = tensor.empty() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = tensor.empty() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = tensor.empty() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = tensor.empty() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = tensor.empty() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = bufferization.alloc_tensor() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = bufferization.alloc_tensor() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = bufferization.alloc_tensor() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = bufferization.alloc_tensor() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = bufferization.alloc_tensor() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump Before OneShotBufferize (one-shot-bufferize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: tensor<128x262144xf32>, %arg1: tensor<128xf32>, %arg2: tensor<64x512x512xf32>) -> tensor<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapsed = tensor.collapse_shape %arg2 [[0], [1, 2]] : tensor<64x512x512xf32> into tensor<64x262144xf32>
    %0 = bufferization.alloc_tensor() : tensor<262144x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<128x262144xf32>) outs(%0 : tensor<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<262144x128xf32>
    %2 = bufferization.alloc_tensor() : tensor<64x128xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<64x128xf32>) -> tensor<64x128xf32>
    %4 = bufferization.alloc_tensor() : tensor<2x8192x32x32xf32>
    %5 = scf.forall (%arg3, %arg4) in (2, 8192) shared_outs(%arg5 = %4) -> (tensor<2x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %collapsed[%9, %10] [32, 32] [1, 1] : tensor<64x262144xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<2x8192x32x32xf32>
      }
    }
    %6 = bufferization.alloc_tensor() : tensor<4x8192x32x32xf32>
    %7 = scf.forall (%arg3, %arg4) in (4, 8192) shared_outs(%arg5 = %6) -> (tensor<4x8192x32x32xf32>) {
      %9 = affine.apply #map2(%arg4)
      %10 = affine.apply #map2(%arg3)
      %extracted_slice = tensor.extract_slice %1[%9, %10] [32, 32] [1, 1] : tensor<262144x128xf32> to tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %extracted_slice into %arg5[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : tensor<32x32xf32> into tensor<4x8192x32x32xf32>
      }
    }
    %expanded = tensor.expand_shape %arg1 [[0, 1]] : tensor<128xf32> into tensor<4x32xf32>
    %8 = scf.forall (%arg3, %arg4) in (2, 4) shared_outs(%arg5 = %3) -> (tensor<64x128xf32>) {
      %9 = affine.apply #map2(%arg3)
      %10 = affine.apply #map2(%arg4)
      %11 = bufferization.alloc_tensor() : tensor<32x32xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice = tensor.extract_slice %5[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<2x8192x32x32xf32> to tensor<8192x32x32xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : tensor<4x8192x32x32xf32> to tensor<8192x32x32xf32>
      %13 = linalg.batch_reduce_matmul ins(%extracted_slice, %extracted_slice_0 : tensor<8192x32x32xf32>, tensor<8192x32x32xf32>) outs(%12 : tensor<32x32xf32>) -> tensor<32x32xf32>
      %extracted_slice_1 = tensor.extract_slice %expanded[%arg4, 0] [1, 32] [1, 1] : tensor<4x32xf32> to tensor<32xf32>
      %14 = linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_1, %13 : tensor<32xf32>, tensor<32x32xf32>) outs(%11 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %in_3: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        linalg.yield %16 : f32
      } -> tensor<32x32xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[%9, %10] [32, 32] [1, 1] : tensor<64x128xf32> to tensor<32x32xf32>
      %15 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%14 : tensor<32x32xf32>) outs(%extracted_slice_2 : tensor<32x32xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.cmpf ugt, %in, %cst : f32
        %17 = arith.select %16, %in, %cst : f32
        linalg.yield %17 : f32
      } -> tensor<32x32xf32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %15 into %arg5[%9, %10] [32, 32] [1, 1] : tensor<32x32xf32> into tensor<64x128xf32>
      }
    }
    return %8 : tensor<64x128xf32>
  }
}


// -----// IR Dump After OneShotBufferize (one-shot-bufferize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_8: f32, %out: f32):
        %2 = arith.addf %in, %in_8 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      %subview_7 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.copy ins(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_7 : memref<32x32xf32, strided<[128, 1], offset: ?>>)
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before DropEquivalentBufferResults (drop-equivalent-buffer-results) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_8: f32, %out: f32):
        %2 = arith.addf %in, %in_8 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      %subview_7 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.copy ins(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_7 : memref<32x32xf32, strided<[128, 1], offset: ?>>)
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before FinalizingBufferize (finalizing-bufferize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_8: f32, %out: f32):
        %2 = arith.addf %in, %in_8 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      %subview_7 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.copy ins(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_7 : memref<32x32xf32, strided<[128, 1], offset: ?>>)
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_8: f32, %out: f32):
        %2 = arith.addf %in, %in_8 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      %subview_7 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.copy ins(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_7 : memref<32x32xf32, strided<[128, 1], offset: ?>>)
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_8: f32, %out: f32):
        %2 = arith.addf %in, %in_8 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      %subview_7 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.copy ins(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_7 : memref<32x32xf32, strided<[128, 1], offset: ?>>)
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      linalg.copy ins(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>)
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      linalg.copy ins(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>)
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before DropEquivalentBufferResults (drop-equivalent-buffer-results) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ExpandRealloc (expand-realloc) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before OwnershipBasedBufferDeallocation (ownership-based-buffer-deallocation) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After OwnershipBasedBufferDeallocation (ownership-based-buffer-deallocation) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %false = arith.constant false
    %false_0 = arith.constant false
    %true = arith.constant true
    %false_1 = arith.constant false
    %true_2 = arith.constant true
    %true_3 = arith.constant true
    %true_4 = arith.constant true
    %false_5 = arith.constant false
    %false_6 = arith.constant false
    %false_7 = arith.constant false
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_8 : memref<64x128xf32>)
    %alloc_9 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %false_35 = arith.constant false
      %false_36 = arith.constant false
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %false_37 = arith.constant false
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %false_38 = arith.constant false
      %subview_39 = memref.subview %alloc_9[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_39 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %false_35 = arith.constant false
      %false_36 = arith.constant false
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %false_37 = arith.constant false
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %false_38 = arith.constant false
      %subview_39 = memref.subview %alloc_10[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_39 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %true_35 = arith.constant true
      %false_36 = arith.constant false
      %false_37 = arith.constant false
      %false_38 = arith.constant false
      %false_39 = arith.constant false
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_40 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_40 : memref<32x32xf32>)
      %false_41 = arith.constant false
      %subview = memref.subview %alloc_9[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %false_42 = arith.constant false
      %subview_43 = memref.subview %alloc_10[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_43 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_40 : memref<32x32xf32>)
      %false_44 = arith.constant false
      %subview_45 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_45, %alloc_40 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_40 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_52: f32, %out: f32):
        %3 = arith.addf %in, %in_52 : f32
        linalg.yield %3 : f32
      }
      %false_46 = arith.constant false
      %subview_47 = memref.subview %alloc_8[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_40 : memref<32x32xf32>) outs(%subview_47 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %3 = arith.cmpf ugt, %in, %cst : f32
        %4 = arith.select %3, %in, %cst : f32
        linalg.yield %4 : f32
      }
      %base_buffer_48, %offset_49, %sizes_50:2, %strides_51:2 = memref.extract_strided_metadata %alloc_40 : memref<32x32xf32> -> memref<f32>, index, index, index, index, index
      bufferization.dealloc (%base_buffer_48 : memref<f32>) if (%true_35)
    }
    %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %arg0 : memref<128x262144xf32> -> memref<f32>, index, index, index, index, index
    %base_buffer_11, %offset_12, %sizes_13, %strides_14 = memref.extract_strided_metadata %arg1 : memref<128xf32> -> memref<f32>, index, index, index
    %base_buffer_15, %offset_16, %sizes_17:3, %strides_18:3 = memref.extract_strided_metadata %arg2 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %base_buffer_19, %offset_20, %sizes_21:2, %strides_22:2 = memref.extract_strided_metadata %alloc : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
    %base_buffer_23, %offset_24, %sizes_25:2, %strides_26:2 = memref.extract_strided_metadata %alloc_8 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
    %base_buffer_27, %offset_28, %sizes_29:4, %strides_30:4 = memref.extract_strided_metadata %alloc_9 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
    %base_buffer_31, %offset_32, %sizes_33:4, %strides_34:4 = memref.extract_strided_metadata %alloc_10 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
    %0 = bufferization.dealloc (%base_buffer, %base_buffer_11, %base_buffer_15, %base_buffer_19, %base_buffer_23, %base_buffer_27, %base_buffer_31 : memref<f32>, memref<f32>, memref<f32>, memref<f32>, memref<f32>, memref<f32>, memref<f32>) if (%false_5, %false_6, %false_7, %true_4, %true_3, %true_2, %true) retain (%alloc_8 : memref<64x128xf32>)
    return %alloc_8 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %false = arith.constant false
    %false_0 = arith.constant false
    %true = arith.constant true
    %false_1 = arith.constant false
    %true_2 = arith.constant true
    %true_3 = arith.constant true
    %true_4 = arith.constant true
    %false_5 = arith.constant false
    %false_6 = arith.constant false
    %false_7 = arith.constant false
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_8 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_8 : memref<64x128xf32>)
    %alloc_9 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %false_35 = arith.constant false
      %false_36 = arith.constant false
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %false_37 = arith.constant false
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %false_38 = arith.constant false
      %subview_39 = memref.subview %alloc_9[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_39 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_10 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %false_35 = arith.constant false
      %false_36 = arith.constant false
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %false_37 = arith.constant false
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %false_38 = arith.constant false
      %subview_39 = memref.subview %alloc_10[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_39 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %true_35 = arith.constant true
      %false_36 = arith.constant false
      %false_37 = arith.constant false
      %false_38 = arith.constant false
      %false_39 = arith.constant false
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_40 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_40 : memref<32x32xf32>)
      %false_41 = arith.constant false
      %subview = memref.subview %alloc_9[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %false_42 = arith.constant false
      %subview_43 = memref.subview %alloc_10[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_43 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_40 : memref<32x32xf32>)
      %false_44 = arith.constant false
      %subview_45 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_45, %alloc_40 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_40 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_52: f32, %out: f32):
        %3 = arith.addf %in, %in_52 : f32
        linalg.yield %3 : f32
      }
      %false_46 = arith.constant false
      %subview_47 = memref.subview %alloc_8[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_40 : memref<32x32xf32>) outs(%subview_47 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %3 = arith.cmpf ugt, %in, %cst : f32
        %4 = arith.select %3, %in, %cst : f32
        linalg.yield %4 : f32
      }
      %base_buffer_48, %offset_49, %sizes_50:2, %strides_51:2 = memref.extract_strided_metadata %alloc_40 : memref<32x32xf32> -> memref<f32>, index, index, index, index, index
      bufferization.dealloc (%base_buffer_48 : memref<f32>) if (%true_35)
    }
    %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %arg0 : memref<128x262144xf32> -> memref<f32>, index, index, index, index, index
    %base_buffer_11, %offset_12, %sizes_13, %strides_14 = memref.extract_strided_metadata %arg1 : memref<128xf32> -> memref<f32>, index, index, index
    %base_buffer_15, %offset_16, %sizes_17:3, %strides_18:3 = memref.extract_strided_metadata %arg2 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %base_buffer_19, %offset_20, %sizes_21:2, %strides_22:2 = memref.extract_strided_metadata %alloc : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
    %base_buffer_23, %offset_24, %sizes_25:2, %strides_26:2 = memref.extract_strided_metadata %alloc_8 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
    %base_buffer_27, %offset_28, %sizes_29:4, %strides_30:4 = memref.extract_strided_metadata %alloc_9 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
    %base_buffer_31, %offset_32, %sizes_33:4, %strides_34:4 = memref.extract_strided_metadata %alloc_10 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
    %0 = bufferization.dealloc (%base_buffer, %base_buffer_11, %base_buffer_15, %base_buffer_19, %base_buffer_23, %base_buffer_27, %base_buffer_31 : memref<f32>, memref<f32>, memref<f32>, memref<f32>, memref<f32>, memref<f32>, memref<f32>) if (%false_5, %false_6, %false_7, %true_4, %true_3, %true_2, %true) retain (%alloc_8 : memref<64x128xf32>)
    return %alloc_8 : memref<64x128xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %true = arith.constant true
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %3 = arith.addf %in, %in_7 : f32
        linalg.yield %3 : f32
      }
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %3 = arith.cmpf ugt, %in, %cst : f32
        %4 = arith.select %3, %in, %cst : f32
        linalg.yield %4 : f32
      }
      bufferization.dealloc (%alloc_3 : memref<32x32xf32>) if (%true)
    }
    %0 = bufferization.dealloc (%alloc, %alloc_0, %alloc_1, %alloc_2 : memref<262144x128xf32>, memref<64x128xf32>, memref<2x8192x32x32xf32>, memref<4x8192x32x32xf32>) if (%true, %true, %true, %true) retain (%alloc_0 : memref<64x128xf32>)
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferDeallocationSimplification (buffer-deallocation-simplification) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %true = arith.constant true
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %3 = arith.addf %in, %in_7 : f32
        linalg.yield %3 : f32
      }
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %3 = arith.cmpf ugt, %in, %cst : f32
        %4 = arith.select %3, %in, %cst : f32
        linalg.yield %4 : f32
      }
      bufferization.dealloc (%alloc_3 : memref<32x32xf32>) if (%true)
    }
    %0 = bufferization.dealloc (%alloc, %alloc_0, %alloc_1, %alloc_2 : memref<262144x128xf32>, memref<64x128xf32>, memref<2x8192x32x32xf32>, memref<4x8192x32x32xf32>) if (%true, %true, %true, %true) retain (%alloc_0 : memref<64x128xf32>)
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After BufferDeallocationSimplification (buffer-deallocation-simplification) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %true = arith.constant true
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      bufferization.dealloc (%alloc_3 : memref<32x32xf32>) if (%true)
    }
    bufferization.dealloc (%alloc : memref<262144x128xf32>) if (%true)
    bufferization.dealloc (%alloc_1 : memref<2x8192x32x32xf32>) if (%true)
    bufferization.dealloc (%alloc_2 : memref<4x8192x32x32xf32>) if (%true)
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before LowerDeallocations (bufferization-lower-deallocations) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %true = arith.constant true
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      bufferization.dealloc (%alloc_3 : memref<32x32xf32>) if (%true)
    }
    bufferization.dealloc (%alloc : memref<262144x128xf32>) if (%true)
    bufferization.dealloc (%alloc_1 : memref<2x8192x32x32xf32>) if (%true)
    bufferization.dealloc (%alloc_2 : memref<4x8192x32x32xf32>) if (%true)
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After LowerDeallocations (bufferization-lower-deallocations) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %true = arith.constant true
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      scf.if %true {
        memref.dealloc %alloc_3 : memref<32x32xf32>
      }
    }
    scf.if %true {
      memref.dealloc %alloc : memref<262144x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %true = arith.constant true
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      scf.if %true {
        memref.dealloc %alloc_3 : memref<32x32xf32>
      }
    }
    scf.if %true {
      memref.dealloc %alloc : memref<262144x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %true = arith.constant true
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      scf.if %true {
        memref.dealloc %alloc_3 : memref<32x32xf32>
      }
    }
    scf.if %true {
      memref.dealloc %alloc : memref<262144x128xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    }
    scf.if %true {
      memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    }
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ConvertBufferizationToMemRef (convert-bufferization-to-memref) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After Bufferize (bufferize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before TppLowering (tpp-lowering) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ConvertMemRefToXsmm (convert-memref-to-xsmm) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ConvertLinalgToXsmm (convert-linalg-to-xsmm) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
#map3 = affine_map<(d0, d1) -> (d1)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    linalg.fill ins(%cst : f32) outs(%alloc_0 : memref<64x128xf32>)
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%0, %1] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[262144, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %0 = affine.apply #map2(%arg4)
      %1 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%0, %1] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      linalg.copy ins(%subview : memref<32x32xf32, strided<[128, 1], offset: ?>>) outs(%subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>>)
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %0 = affine.apply #map2(%arg3)
      %1 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      linalg.fill ins(%cst : f32) outs(%alloc_3 : memref<32x32xf32>)
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      linalg.batch_reduce_matmul ins(%subview, %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>) outs(%alloc_3 : memref<32x32xf32>)
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      linalg.generic {indexing_maps = [#map3, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%subview_5, %alloc_3 : memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>) outs(%alloc_3 : memref<32x32xf32>) {
      ^bb0(%in: f32, %in_7: f32, %out: f32):
        %2 = arith.addf %in, %in_7 : f32
        linalg.yield %2 : f32
      }
      %subview_6 = memref.subview %alloc_0[%0, %1] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%alloc_3 : memref<32x32xf32>) outs(%subview_6 : memref<32x32xf32, strided<[128, 1], offset: ?>>) {
      ^bb0(%in: f32, %out: f32):
        %2 = arith.cmpf ugt, %in, %cst : f32
        %3 = arith.select %2, %in, %cst : f32
        linalg.yield %3 : f32
      }
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After ConvertLinalgToXsmm (convert-linalg-to-xsmm) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %3 = xsmm.unary.dispatch zero [32, 32, 1, 32] flags = (bcast_scalar) data_type = f32
      xsmm.unary zero(data_type = f32, %3, %cst, %alloc_3) : (i64, f32, memref<32x32xf32>) -> ()
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %4 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (none) data_type = f32
      xsmm.brgemm(data_type = f32, %4, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %5 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %5, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %6 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %6, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before FoldXsmmFlags (fold-xsmm-flags) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %3 = xsmm.unary.dispatch zero [32, 32, 1, 32] flags = (bcast_scalar) data_type = f32
      xsmm.unary zero(data_type = f32, %3, %cst, %alloc_3) : (i64, f32, memref<32x32xf32>) -> ()
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %4 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (none) data_type = f32
      xsmm.brgemm(data_type = f32, %4, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %5 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %5, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %6 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %6, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After FoldXsmmFlags (fold-xsmm-flags) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %3 = xsmm.unary.dispatch zero [32, 32, 1, 32] flags = (bcast_scalar) data_type = f32
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %4 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      %5 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (none) data_type = f32
      xsmm.brgemm(data_type = f32, %4, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %6 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %6, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %7 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %7, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before VerifyXsmmCalls (verify-xsmm-calls) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %3 = xsmm.unary.dispatch zero [32, 32, 1, 32] flags = (bcast_scalar) data_type = f32
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %4 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      %5 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (none) data_type = f32
      xsmm.brgemm(data_type = f32, %4, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %6 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %6, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %7 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %7, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After TppLowering (tpp-lowering) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %3 = xsmm.unary.dispatch zero [32, 32, 1, 32] flags = (bcast_scalar) data_type = f32
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %4 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      %5 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (none) data_type = f32
      xsmm.brgemm(data_type = f32, %4, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %6 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %6, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %7 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %7, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %3 = xsmm.unary.dispatch zero [32, 32, 1, 32] flags = (bcast_scalar) data_type = f32
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %4 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      %5 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (none) data_type = f32
      xsmm.brgemm(data_type = f32, %4, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %6 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %6, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %7 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %7, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %3 = xsmm.unary.dispatch zero [32, 32, 1, 32] flags = (bcast_scalar) data_type = f32
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %4 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      %5 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (none) data_type = f32
      xsmm.brgemm(data_type = f32, %4, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %6 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %6, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %7 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %7, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ConvertForAllToParallelOp (convert-forall-to-parallel) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (2, 8192) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.forall (%arg3, %arg4) in (4, 8192) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.forall (%arg3, %arg4) in (2, 4) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After ConvertForAllToParallelOp (convert-forall-to-parallel) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before LocalDialectsLowering (lower-local-dialects) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ConvertCheckToLoops (convert-check-to-loops) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ConvertPerfToLoops (convert-perf-to-loops) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before LoopInvariantCodeMotion (loop-invariant-code-motion) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%1, %2] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg4)
      %2 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%1, %2] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %3 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
      xsmm.unary identity(data_type = f32, %3, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %1 = affine.apply #map2(%arg3)
      %2 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%1, %2] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %1 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%6, %7] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %1, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %2 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg4)
      %7 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%6, %7] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %2, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
    %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
    %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%6, %7] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %1 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%6, %7] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %1, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %2 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg4)
      %7 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%6, %7] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %2, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
    %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
    %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%6, %7] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %1 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%6, %7] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %1, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %2 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg4)
      %7 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%6, %7] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %2, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
    %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
    %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%6, %7] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %1 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%6, %7] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %1, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %2 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg4)
      %7 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%6, %7] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %2, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
    %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
    %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%6, %7] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ConvertXsmmToFunc (convert-xsmm-to-func) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = xsmm.unary.dispatch zero [64, 128, 1, 128] flags = (bcast_scalar) data_type = f32
    xsmm.unary zero(data_type = f32, %0, %cst, %alloc_0) : (i64, f32, memref<64x128xf32>) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %1 = xsmm.unary.dispatch identity [32, 32, 262144, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%6, %7] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %1, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[262144, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %2 = xsmm.unary.dispatch identity [32, 32, 128, 32] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg4)
      %7 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%6, %7] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      xsmm.unary identity(data_type = f32, %2, %subview, %subview_3) : (i64, memref<32x32xf32, strided<[128, 1], offset: ?>>, memref<32x32xf32, strided<[32, 1], offset: ?>>) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %3 = xsmm.brgemm.dispatch [32, 32, 32, 32, 32, 32, 1024, 1024] flags = (beta_0) data_type = f32
    %4 = xsmm.binary.dispatch add [32, 32, 32, 32, 32] flags = (bcast_col_in0) data_type = f32
    %5 = xsmm.unary.dispatch relu [32, 32, 32, 128] flags = (none) data_type = f32
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %6 = affine.apply #map2(%arg3)
      %7 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      xsmm.brgemm(data_type = f32, %3, %subview, %subview_4, %alloc_3, %c8192_i64) : (i64, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>, memref<32x32xf32>, i64) -> ()
      %subview_5 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      xsmm.binary add(data_type = f32, %4, %subview_5, %alloc_3, %alloc_3) : (i64, memref<32xf32, strided<[1], offset: ?>>, memref<32x32xf32>, memref<32x32xf32>) -> ()
      %subview_6 = memref.subview %alloc_0[%6, %7] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      xsmm.unary relu(data_type = f32, %5, %alloc_3, %subview_6) : (i64, memref<32x32xf32>, memref<32x32xf32, strided<[128, 1], offset: ?>>) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After ConvertXsmmToFunc (convert-xsmm-to-func) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before ConvertPerfToFunc (convert-perf-to-func) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After LocalDialectsLowering (lower-local-dialects) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Postprocessing (postprocess) ('func.func' operation: @xsmm_unary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferHoisting (buffer-hoisting) ('func.func' operation: @xsmm_unary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @xsmm_unary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @xsmm_unary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @xsmm_unary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Postprocessing (postprocess) ('func.func' operation: @xsmm_binary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferHoisting (buffer-hoisting) ('func.func' operation: @xsmm_binary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @xsmm_binary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @xsmm_binary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @xsmm_binary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Postprocessing (postprocess) ('func.func' operation: @xsmm_brgemm_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferHoisting (buffer-hoisting) ('func.func' operation: @xsmm_brgemm_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @xsmm_brgemm_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @xsmm_brgemm_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @xsmm_brgemm_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Postprocessing (postprocess) ('func.func' operation: @xsmm_unary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferHoisting (buffer-hoisting) ('func.func' operation: @xsmm_unary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @xsmm_unary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @xsmm_unary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @xsmm_unary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Postprocessing (postprocess) ('func.func' operation: @xsmm_binary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferHoisting (buffer-hoisting) ('func.func' operation: @xsmm_binary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @xsmm_binary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @xsmm_binary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @xsmm_binary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Postprocessing (postprocess) ('func.func' operation: @xsmm_brgemm_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferHoisting (buffer-hoisting) ('func.func' operation: @xsmm_brgemm_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @xsmm_brgemm_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @xsmm_brgemm_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @xsmm_brgemm_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Postprocessing (postprocess) ('func.func' operation: @xsmm_unary_scalar_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferHoisting (buffer-hoisting) ('func.func' operation: @xsmm_unary_scalar_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @xsmm_unary_scalar_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @xsmm_unary_scalar_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @xsmm_unary_scalar_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Postprocessing (postprocess) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before BufferHoisting (buffer-hoisting) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      %intptr_18 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %18 = arith.index_cast %intptr_18 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      %intptr_19 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %20 = arith.index_cast %intptr_19 : index to i64
      %21 = llvm.inttoptr %20 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %19, %c0, %21, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_20 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %intptr_21 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %22 = arith.index_cast %intptr_21 : index to i64
      %23 = llvm.inttoptr %22 : i64 to !llvm.ptr<f32>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %24 = arith.index_cast %intptr_26 : index to i64
      %25 = llvm.inttoptr %24 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %23, %c0, %25, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After CSE (cse) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %15, %c0, %15, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_18 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %base_buffer_19, %offset_20, %sizes_21:2, %strides_22:2 = memref.extract_strided_metadata %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_23 = memref.extract_aligned_pointer_as_index %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %18 = arith.index_cast %intptr_23 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %15, %c0, %19, %offset_20) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After Cleanup (cleanup) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %15, %c0, %15, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_18 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %base_buffer_19, %offset_20, %sizes_21:2, %strides_22:2 = memref.extract_strided_metadata %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_23 = memref.extract_aligned_pointer_as_index %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %18 = arith.index_cast %intptr_23 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %15, %c0, %19, %offset_20) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After Postprocessing (postprocess) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %15, %c0, %15, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_18 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %base_buffer_19, %offset_20, %sizes_21:2, %strides_22:2 = memref.extract_strided_metadata %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_23 = memref.extract_aligned_pointer_as_index %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %18 = arith.index_cast %intptr_23 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %15, %c0, %19, %offset_20) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After DefaultTppPasses (default-tpp-passes) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %15, %c0, %15, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_18 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %base_buffer_19, %offset_20, %sizes_21:2, %strides_22:2 = memref.extract_strided_metadata %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_23 = memref.extract_aligned_pointer_as_index %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %18 = arith.index_cast %intptr_23 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %15, %c0, %19, %offset_20) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump Before MungeCallingConventions (refback-munge-calling-conventions) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<128x262144xf32>, %arg1: memref<128xf32>, %arg2: memref<64x512x512xf32>) -> memref<64x128xf32> {
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %arg2 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%arg0 : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_0 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_0 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_1 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_1[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_3 = memref.subview %alloc_2[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_4 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_4 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_5, %offset_6, %sizes_7:2, %strides_8:2 = memref.extract_strided_metadata %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_9 = memref.extract_aligned_pointer_as_index %subview_3 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_9 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_6) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %arg1 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_1[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_4 = memref.subview %alloc_2[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_5 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_5 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_6, %offset_7, %sizes_8:3, %strides_9:3 = memref.extract_strided_metadata %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_10 = memref.extract_aligned_pointer_as_index %subview_4 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_10 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_11 = memref.extract_aligned_pointer_as_index %alloc_3 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_11 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_7, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_12 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_13, %offset_14, %sizes_15, %strides_16 = memref.extract_strided_metadata %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_17 = memref.extract_aligned_pointer_as_index %subview_12 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_17 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_14, %15, %c0, %15, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_18 = memref.subview %alloc_0[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %base_buffer_19, %offset_20, %sizes_21:2, %strides_22:2 = memref.extract_strided_metadata %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_23 = memref.extract_aligned_pointer_as_index %subview_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %18 = arith.index_cast %intptr_23 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %15, %c0, %19, %offset_20) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_3 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_1 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_2 : memref<4x8192x32x32xf32>
    return %alloc_0 : memref<64x128xf32>
  }
}


// -----// IR Dump After MungeCallingConventions (refback-munge-calling-conventions) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %cast = memref.cast %arg0 : memref<*xf32> to memref<128x262144xf32>
    %cast_0 = memref.cast %arg1 : memref<*xf32> to memref<128xf32>
    %cast_1 = memref.cast %arg2 : memref<*xf32> to memref<64x512x512xf32>
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %cast_1 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%cast : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_2 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_6 = memref.subview %alloc_3[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_7 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_7 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_8, %offset_9, %sizes_10:2, %strides_11:2 = memref.extract_strided_metadata %subview_6 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_12 = memref.extract_aligned_pointer_as_index %subview_6 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_12 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_9) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_6 = memref.subview %alloc_4[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_7 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_7 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_8, %offset_9, %sizes_10:2, %strides_11:2 = memref.extract_strided_metadata %subview_6 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_12 = memref.extract_aligned_pointer_as_index %subview_6 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_12 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_9) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %cast_0 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_3[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_7 = memref.subview %alloc_4[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_8 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_8 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_9, %offset_10, %sizes_11:3, %strides_12:3 = memref.extract_strided_metadata %subview_7 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_13 = memref.extract_aligned_pointer_as_index %subview_7 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_13 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_14 = memref.extract_aligned_pointer_as_index %alloc_6 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_14 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_10, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_15 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_16, %offset_17, %sizes_18, %strides_19 = memref.extract_strided_metadata %subview_15 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_20 = memref.extract_aligned_pointer_as_index %subview_15 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_20 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_17, %15, %c0, %15, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_21 = memref.subview %alloc_2[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_21 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_21 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %18 = arith.index_cast %intptr_26 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %15, %c0, %19, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_6 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_3 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_4 : memref<4x8192x32x32xf32>
    %cast_5 = memref.cast %alloc_2 : memref<64x128xf32> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%cast_5) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %cast = memref.cast %arg0 : memref<*xf32> to memref<128x262144xf32>
    %cast_0 = memref.cast %arg1 : memref<*xf32> to memref<128xf32>
    %cast_1 = memref.cast %arg2 : memref<*xf32> to memref<64x512x512xf32>
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %cast_1 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %alloc = memref.alloc() {alignment = 64 : i64} : memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%cast : memref<128x262144xf32>) outs(%alloc : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %alloc_2 = memref.alloc() {alignment = 64 : i64} : memref<64x128xf32>
    %0 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %intptr = memref.extract_aligned_pointer_as_index %alloc_2 : memref<64x128xf32> -> index
    %1 = arith.index_cast %intptr : index to i64
    %2 = llvm.inttoptr %1 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %0, %cst, %2, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<2x8192x32x32xf32>
    %3 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%8, %9] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %subview_6 = memref.subview %alloc_3[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_7 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_7 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_8, %offset_9, %sizes_10:2, %strides_11:2 = memref.extract_strided_metadata %subview_6 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_12 = memref.extract_aligned_pointer_as_index %subview_6 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_12 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %3, %11, %offset, %13, %offset_9) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<4x8192x32x32xf32>
    %4 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg4)
      %9 = affine.apply #map2(%arg3)
      %subview = memref.subview %alloc[%8, %9] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %subview_6 = memref.subview %alloc_4[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:2, %strides:2 = memref.extract_strided_metadata %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_7 = memref.extract_aligned_pointer_as_index %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_7 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_8, %offset_9, %sizes_10:2, %strides_11:2 = memref.extract_strided_metadata %subview_6 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_12 = memref.extract_aligned_pointer_as_index %subview_6 : memref<32x32xf32, strided<[32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_12 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %4, %11, %offset, %13, %offset_9) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %cast_0 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %5 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %6 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %7 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %8 = affine.apply #map2(%arg3)
      %9 = affine.apply #map2(%arg4)
      %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<32x32xf32>
      %subview = memref.subview %alloc_3[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %subview_7 = memref.subview %alloc_4[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_8 = memref.extract_aligned_pointer_as_index %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %10 = arith.index_cast %intptr_8 : index to i64
      %11 = llvm.inttoptr %10 : i64 to !llvm.ptr<f32>
      %base_buffer_9, %offset_10, %sizes_11:3, %strides_12:3 = memref.extract_strided_metadata %subview_7 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> memref<f32>, index, index, index, index, index, index, index
      %intptr_13 = memref.extract_aligned_pointer_as_index %subview_7 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> -> index
      %12 = arith.index_cast %intptr_13 : index to i64
      %13 = llvm.inttoptr %12 : i64 to !llvm.ptr<f32>
      %intptr_14 = memref.extract_aligned_pointer_as_index %alloc_6 : memref<32x32xf32> -> index
      %14 = arith.index_cast %intptr_14 : index to i64
      %15 = llvm.inttoptr %14 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %5, %11, %offset, %13, %offset_10, %15, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_15 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %base_buffer_16, %offset_17, %sizes_18, %strides_19 = memref.extract_strided_metadata %subview_15 : memref<32xf32, strided<[1], offset: ?>> -> memref<f32>, index, index, index
      %intptr_20 = memref.extract_aligned_pointer_as_index %subview_15 : memref<32xf32, strided<[1], offset: ?>> -> index
      %16 = arith.index_cast %intptr_20 : index to i64
      %17 = llvm.inttoptr %16 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %6, %17, %offset_17, %15, %c0, %15, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_21 = memref.subview %alloc_2[%8, %9] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %base_buffer_22, %offset_23, %sizes_24:2, %strides_25:2 = memref.extract_strided_metadata %subview_21 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> memref<f32>, index, index, index, index, index
      %intptr_26 = memref.extract_aligned_pointer_as_index %subview_21 : memref<32x32xf32, strided<[128, 1], offset: ?>> -> index
      %18 = arith.index_cast %intptr_26 : index to i64
      %19 = llvm.inttoptr %18 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %7, %15, %c0, %19, %offset_23) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      memref.dealloc %alloc_6 : memref<32x32xf32>
      scf.yield
    }
    memref.dealloc %alloc : memref<262144x128xf32>
    memref.dealloc %alloc_3 : memref<2x8192x32x32xf32>
    memref.dealloc %alloc_4 : memref<4x8192x32x32xf32>
    %cast_5 = memref.cast %alloc_2 : memref<64x128xf32> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%cast_5) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %1 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %2 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %3 = llvm.extractvalue %0[1] : !llvm.struct<(i64, ptr)> 
    %4 = llvm.load %3 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %5 = builtin.unrealized_conversion_cast %4 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %6 = llvm.extractvalue %1[1] : !llvm.struct<(i64, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = builtin.unrealized_conversion_cast %7 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %9 = llvm.extractvalue %2[1] : !llvm.struct<(i64, ptr)> 
    %10 = llvm.load %9 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %11 = builtin.unrealized_conversion_cast %10 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %11 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %12 = llvm.mlir.constant(262144 : index) : i64
    %13 = llvm.mlir.constant(128 : index) : i64
    %14 = llvm.mlir.constant(1 : index) : i64
    %15 = llvm.mlir.constant(33554432 : index) : i64
    %16 = llvm.mlir.zero : !llvm.ptr
    %17 = llvm.getelementptr %16[%15] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %18 = llvm.ptrtoint %17 : !llvm.ptr to i64
    %19 = llvm.mlir.constant(64 : index) : i64
    %20 = llvm.add %18, %19  : i64
    %21 = llvm.call @malloc(%20) : (i64) -> !llvm.ptr
    %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
    %23 = llvm.mlir.constant(1 : index) : i64
    %24 = llvm.sub %19, %23  : i64
    %25 = llvm.add %22, %24  : i64
    %26 = llvm.urem %25, %19  : i64
    %27 = llvm.sub %25, %26  : i64
    %28 = llvm.inttoptr %27 : i64 to !llvm.ptr
    %29 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %30 = llvm.insertvalue %21, %29[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %31 = llvm.insertvalue %28, %30[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.mlir.constant(0 : index) : i64
    %33 = llvm.insertvalue %32, %31[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %12, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %13, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %13, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %14, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%5 : memref<128x262144xf32>) outs(%38 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %39 = llvm.mlir.constant(64 : index) : i64
    %40 = llvm.mlir.constant(128 : index) : i64
    %41 = llvm.mlir.constant(1 : index) : i64
    %42 = llvm.mlir.constant(8192 : index) : i64
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[%42] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.mlir.constant(64 : index) : i64
    %47 = llvm.add %45, %46  : i64
    %48 = llvm.call @malloc(%47) : (i64) -> !llvm.ptr
    %49 = llvm.ptrtoint %48 : !llvm.ptr to i64
    %50 = llvm.mlir.constant(1 : index) : i64
    %51 = llvm.sub %46, %50  : i64
    %52 = llvm.add %49, %51  : i64
    %53 = llvm.urem %52, %46  : i64
    %54 = llvm.sub %52, %53  : i64
    %55 = llvm.inttoptr %54 : i64 to !llvm.ptr
    %56 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %57 = llvm.insertvalue %48, %56[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %55, %57[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.mlir.constant(0 : index) : i64
    %60 = llvm.insertvalue %59, %58[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %39, %60[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = llvm.insertvalue %40, %61[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %63 = llvm.insertvalue %40, %62[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %64 = llvm.insertvalue %41, %63[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %65 = builtin.unrealized_conversion_cast %64 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %66 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %67 = llvm.extractvalue %64[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.ptrtoint %67 : !llvm.ptr to i64
    %69 = builtin.unrealized_conversion_cast %68 : i64 to index
    %70 = arith.index_cast %69 : index to i64
    %71 = llvm.inttoptr %70 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %66, %cst, %71, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %72 = llvm.mlir.constant(2 : index) : i64
    %73 = llvm.mlir.constant(8192 : index) : i64
    %74 = llvm.mlir.constant(32 : index) : i64
    %75 = llvm.mlir.constant(32 : index) : i64
    %76 = llvm.mlir.constant(1 : index) : i64
    %77 = llvm.mlir.constant(1024 : index) : i64
    %78 = llvm.mlir.constant(8388608 : index) : i64
    %79 = llvm.mlir.constant(16777216 : index) : i64
    %80 = llvm.mlir.zero : !llvm.ptr
    %81 = llvm.getelementptr %80[%79] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %82 = llvm.ptrtoint %81 : !llvm.ptr to i64
    %83 = llvm.mlir.constant(64 : index) : i64
    %84 = llvm.add %82, %83  : i64
    %85 = llvm.call @malloc(%84) : (i64) -> !llvm.ptr
    %86 = llvm.ptrtoint %85 : !llvm.ptr to i64
    %87 = llvm.mlir.constant(1 : index) : i64
    %88 = llvm.sub %83, %87  : i64
    %89 = llvm.add %86, %88  : i64
    %90 = llvm.urem %89, %83  : i64
    %91 = llvm.sub %89, %90  : i64
    %92 = llvm.inttoptr %91 : i64 to !llvm.ptr
    %93 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %94 = llvm.insertvalue %85, %93[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %95 = llvm.insertvalue %92, %94[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %96 = llvm.mlir.constant(0 : index) : i64
    %97 = llvm.insertvalue %96, %95[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %98 = llvm.insertvalue %72, %97[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %99 = llvm.insertvalue %73, %98[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %100 = llvm.insertvalue %74, %99[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %101 = llvm.insertvalue %75, %100[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %102 = llvm.insertvalue %78, %101[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %103 = llvm.insertvalue %77, %102[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %104 = llvm.insertvalue %75, %103[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %105 = llvm.insertvalue %76, %104[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = builtin.unrealized_conversion_cast %105 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %107 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %157 = affine.apply #map2(%arg3)
      %158 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%157, %158] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %159 = builtin.unrealized_conversion_cast %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %subview_0 = memref.subview %106[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %subview_0 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %161 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %162 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %163 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %164 = llvm.insertvalue %161, %163[0] : !llvm.struct<(ptr, ptr, i64)> 
      %165 = llvm.insertvalue %162, %164[1] : !llvm.struct<(ptr, ptr, i64)> 
      %166 = llvm.mlir.constant(0 : index) : i64
      %167 = llvm.insertvalue %166, %165[2] : !llvm.struct<(ptr, ptr, i64)> 
      %168 = llvm.extractvalue %159[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %169 = llvm.extractvalue %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %170 = llvm.extractvalue %159[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = llvm.extractvalue %159[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %172 = llvm.extractvalue %159[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = builtin.unrealized_conversion_cast %168 : i64 to index
      %174 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
      %176 = builtin.unrealized_conversion_cast %175 : i64 to index
      %177 = arith.index_cast %176 : index to i64
      %178 = llvm.inttoptr %177 : i64 to !llvm.ptr<f32>
      %179 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %180 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %181 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %182 = llvm.insertvalue %179, %181[0] : !llvm.struct<(ptr, ptr, i64)> 
      %183 = llvm.insertvalue %180, %182[1] : !llvm.struct<(ptr, ptr, i64)> 
      %184 = llvm.mlir.constant(0 : index) : i64
      %185 = llvm.insertvalue %184, %183[2] : !llvm.struct<(ptr, ptr, i64)> 
      %186 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %187 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %188 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %189 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %190 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %191 = builtin.unrealized_conversion_cast %186 : i64 to index
      %192 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %193 = llvm.ptrtoint %192 : !llvm.ptr to i64
      %194 = builtin.unrealized_conversion_cast %193 : i64 to index
      %195 = arith.index_cast %194 : index to i64
      %196 = llvm.inttoptr %195 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %107, %178, %173, %196, %191) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %108 = llvm.mlir.constant(4 : index) : i64
    %109 = llvm.mlir.constant(8192 : index) : i64
    %110 = llvm.mlir.constant(32 : index) : i64
    %111 = llvm.mlir.constant(32 : index) : i64
    %112 = llvm.mlir.constant(1 : index) : i64
    %113 = llvm.mlir.constant(1024 : index) : i64
    %114 = llvm.mlir.constant(8388608 : index) : i64
    %115 = llvm.mlir.constant(33554432 : index) : i64
    %116 = llvm.mlir.zero : !llvm.ptr
    %117 = llvm.getelementptr %116[%115] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %118 = llvm.ptrtoint %117 : !llvm.ptr to i64
    %119 = llvm.mlir.constant(64 : index) : i64
    %120 = llvm.add %118, %119  : i64
    %121 = llvm.call @malloc(%120) : (i64) -> !llvm.ptr
    %122 = llvm.ptrtoint %121 : !llvm.ptr to i64
    %123 = llvm.mlir.constant(1 : index) : i64
    %124 = llvm.sub %119, %123  : i64
    %125 = llvm.add %122, %124  : i64
    %126 = llvm.urem %125, %119  : i64
    %127 = llvm.sub %125, %126  : i64
    %128 = llvm.inttoptr %127 : i64 to !llvm.ptr
    %129 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %130 = llvm.insertvalue %121, %129[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %131 = llvm.insertvalue %128, %130[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %132 = llvm.mlir.constant(0 : index) : i64
    %133 = llvm.insertvalue %132, %131[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %134 = llvm.insertvalue %108, %133[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %135 = llvm.insertvalue %109, %134[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %136 = llvm.insertvalue %110, %135[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %137 = llvm.insertvalue %111, %136[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %138 = llvm.insertvalue %114, %137[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %139 = llvm.insertvalue %113, %138[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %140 = llvm.insertvalue %111, %139[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %141 = llvm.insertvalue %112, %140[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %142 = builtin.unrealized_conversion_cast %141 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %143 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %157 = affine.apply #map2(%arg4)
      %158 = affine.apply #map2(%arg3)
      %subview = memref.subview %38[%157, %158] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %159 = builtin.unrealized_conversion_cast %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %subview_0 = memref.subview %142[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %subview_0 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %161 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %162 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %163 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %164 = llvm.insertvalue %161, %163[0] : !llvm.struct<(ptr, ptr, i64)> 
      %165 = llvm.insertvalue %162, %164[1] : !llvm.struct<(ptr, ptr, i64)> 
      %166 = llvm.mlir.constant(0 : index) : i64
      %167 = llvm.insertvalue %166, %165[2] : !llvm.struct<(ptr, ptr, i64)> 
      %168 = llvm.extractvalue %159[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %169 = llvm.extractvalue %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %170 = llvm.extractvalue %159[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = llvm.extractvalue %159[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %172 = llvm.extractvalue %159[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = builtin.unrealized_conversion_cast %168 : i64 to index
      %174 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
      %176 = builtin.unrealized_conversion_cast %175 : i64 to index
      %177 = arith.index_cast %176 : index to i64
      %178 = llvm.inttoptr %177 : i64 to !llvm.ptr<f32>
      %179 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %180 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %181 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %182 = llvm.insertvalue %179, %181[0] : !llvm.struct<(ptr, ptr, i64)> 
      %183 = llvm.insertvalue %180, %182[1] : !llvm.struct<(ptr, ptr, i64)> 
      %184 = llvm.mlir.constant(0 : index) : i64
      %185 = llvm.insertvalue %184, %183[2] : !llvm.struct<(ptr, ptr, i64)> 
      %186 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %187 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %188 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %189 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %190 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %191 = builtin.unrealized_conversion_cast %186 : i64 to index
      %192 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %193 = llvm.ptrtoint %192 : !llvm.ptr to i64
      %194 = builtin.unrealized_conversion_cast %193 : i64 to index
      %195 = arith.index_cast %194 : index to i64
      %196 = llvm.inttoptr %195 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %143, %178, %173, %196, %191) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %8 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %144 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %145 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %146 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %157 = affine.apply #map2(%arg3)
      %158 = affine.apply #map2(%arg4)
      %159 = llvm.mlir.constant(32 : index) : i64
      %160 = llvm.mlir.constant(32 : index) : i64
      %161 = llvm.mlir.constant(1 : index) : i64
      %162 = llvm.mlir.constant(1024 : index) : i64
      %163 = llvm.mlir.zero : !llvm.ptr
      %164 = llvm.getelementptr %163[%162] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      %165 = llvm.ptrtoint %164 : !llvm.ptr to i64
      %166 = llvm.mlir.constant(64 : index) : i64
      %167 = llvm.add %165, %166  : i64
      %168 = llvm.call @malloc(%167) : (i64) -> !llvm.ptr
      %169 = llvm.ptrtoint %168 : !llvm.ptr to i64
      %170 = llvm.mlir.constant(1 : index) : i64
      %171 = llvm.sub %166, %170  : i64
      %172 = llvm.add %169, %171  : i64
      %173 = llvm.urem %172, %166  : i64
      %174 = llvm.sub %172, %173  : i64
      %175 = llvm.inttoptr %174 : i64 to !llvm.ptr
      %176 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %177 = llvm.insertvalue %168, %176[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %178 = llvm.insertvalue %175, %177[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %179 = llvm.mlir.constant(0 : index) : i64
      %180 = llvm.insertvalue %179, %178[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %181 = llvm.insertvalue %159, %180[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %182 = llvm.insertvalue %160, %181[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %183 = llvm.insertvalue %160, %182[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %184 = llvm.insertvalue %161, %183[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %subview = memref.subview %106[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %185 = builtin.unrealized_conversion_cast %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %subview_0 = memref.subview %142[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %186 = builtin.unrealized_conversion_cast %subview_0 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %187 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %188 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %189 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %190 = llvm.insertvalue %187, %189[0] : !llvm.struct<(ptr, ptr, i64)> 
      %191 = llvm.insertvalue %188, %190[1] : !llvm.struct<(ptr, ptr, i64)> 
      %192 = llvm.mlir.constant(0 : index) : i64
      %193 = llvm.insertvalue %192, %191[2] : !llvm.struct<(ptr, ptr, i64)> 
      %194 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %195 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %196 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %197 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %198 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %199 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %200 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %201 = builtin.unrealized_conversion_cast %194 : i64 to index
      %202 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %203 = llvm.ptrtoint %202 : !llvm.ptr to i64
      %204 = builtin.unrealized_conversion_cast %203 : i64 to index
      %205 = arith.index_cast %204 : index to i64
      %206 = llvm.inttoptr %205 : i64 to !llvm.ptr<f32>
      %207 = llvm.extractvalue %186[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %208 = llvm.extractvalue %186[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %209 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %210 = llvm.insertvalue %207, %209[0] : !llvm.struct<(ptr, ptr, i64)> 
      %211 = llvm.insertvalue %208, %210[1] : !llvm.struct<(ptr, ptr, i64)> 
      %212 = llvm.mlir.constant(0 : index) : i64
      %213 = llvm.insertvalue %212, %211[2] : !llvm.struct<(ptr, ptr, i64)> 
      %214 = llvm.extractvalue %186[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %215 = llvm.extractvalue %186[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %216 = llvm.extractvalue %186[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %217 = llvm.extractvalue %186[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %218 = llvm.extractvalue %186[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %219 = llvm.extractvalue %186[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %220 = llvm.extractvalue %186[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %221 = builtin.unrealized_conversion_cast %214 : i64 to index
      %222 = llvm.extractvalue %186[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %223 = llvm.ptrtoint %222 : !llvm.ptr to i64
      %224 = builtin.unrealized_conversion_cast %223 : i64 to index
      %225 = arith.index_cast %224 : index to i64
      %226 = llvm.inttoptr %225 : i64 to !llvm.ptr<f32>
      %227 = llvm.extractvalue %184[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %228 = llvm.ptrtoint %227 : !llvm.ptr to i64
      %229 = builtin.unrealized_conversion_cast %228 : i64 to index
      %230 = arith.index_cast %229 : index to i64
      %231 = llvm.inttoptr %230 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %144, %206, %201, %226, %221, %231, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_1 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %232 = builtin.unrealized_conversion_cast %subview_1 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %233 = llvm.extractvalue %232[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %234 = llvm.extractvalue %232[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %235 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %236 = llvm.insertvalue %233, %235[0] : !llvm.struct<(ptr, ptr, i64)> 
      %237 = llvm.insertvalue %234, %236[1] : !llvm.struct<(ptr, ptr, i64)> 
      %238 = llvm.mlir.constant(0 : index) : i64
      %239 = llvm.insertvalue %238, %237[2] : !llvm.struct<(ptr, ptr, i64)> 
      %240 = llvm.extractvalue %232[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %241 = llvm.extractvalue %232[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %242 = llvm.extractvalue %232[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %243 = builtin.unrealized_conversion_cast %240 : i64 to index
      %244 = llvm.extractvalue %232[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %245 = llvm.ptrtoint %244 : !llvm.ptr to i64
      %246 = builtin.unrealized_conversion_cast %245 : i64 to index
      %247 = arith.index_cast %246 : index to i64
      %248 = llvm.inttoptr %247 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %145, %248, %243, %231, %c0, %231, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_2 = memref.subview %65[%157, %158] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %249 = builtin.unrealized_conversion_cast %subview_2 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %250 = llvm.extractvalue %249[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %251 = llvm.extractvalue %249[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %252 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %253 = llvm.insertvalue %250, %252[0] : !llvm.struct<(ptr, ptr, i64)> 
      %254 = llvm.insertvalue %251, %253[1] : !llvm.struct<(ptr, ptr, i64)> 
      %255 = llvm.mlir.constant(0 : index) : i64
      %256 = llvm.insertvalue %255, %254[2] : !llvm.struct<(ptr, ptr, i64)> 
      %257 = llvm.extractvalue %249[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %258 = llvm.extractvalue %249[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %259 = llvm.extractvalue %249[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %260 = llvm.extractvalue %249[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %261 = llvm.extractvalue %249[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %262 = builtin.unrealized_conversion_cast %257 : i64 to index
      %263 = llvm.extractvalue %249[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %264 = llvm.ptrtoint %263 : !llvm.ptr to i64
      %265 = builtin.unrealized_conversion_cast %264 : i64 to index
      %266 = arith.index_cast %265 : index to i64
      %267 = llvm.inttoptr %266 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %146, %231, %c0, %267, %262) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %268 = llvm.extractvalue %184[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      llvm.call @free(%268) : (!llvm.ptr) -> ()
      scf.yield
    }
    %147 = llvm.extractvalue %37[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.call @free(%147) : (!llvm.ptr) -> ()
    %148 = llvm.extractvalue %105[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    llvm.call @free(%148) : (!llvm.ptr) -> ()
    %149 = llvm.extractvalue %141[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    llvm.call @free(%149) : (!llvm.ptr) -> ()
    %150 = llvm.mlir.constant(1 : index) : i64
    %151 = llvm.alloca %150 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %64, %151 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %152 = llvm.mlir.constant(2 : index) : i64
    %153 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %154 = llvm.insertvalue %152, %153[0] : !llvm.struct<(i64, ptr)> 
    %155 = llvm.insertvalue %151, %154[1] : !llvm.struct<(i64, ptr)> 
    %156 = builtin.unrealized_conversion_cast %155 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%156) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before ExpandStridedMetadata (expand-strided-metadata) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0) -> (d0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %1 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %2 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %3 = llvm.extractvalue %0[1] : !llvm.struct<(i64, ptr)> 
    %4 = llvm.load %3 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %5 = builtin.unrealized_conversion_cast %4 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %6 = llvm.extractvalue %1[1] : !llvm.struct<(i64, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %8 = builtin.unrealized_conversion_cast %7 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %9 = llvm.extractvalue %2[1] : !llvm.struct<(i64, ptr)> 
    %10 = llvm.load %9 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %11 = builtin.unrealized_conversion_cast %10 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %c2_i64 = arith.constant 2 : i64
    %c1_i64 = arith.constant 1 : i64
    %c64_i64 = arith.constant 64 : i64
    %c128_i64 = arith.constant 128 : i64
    %c8_i64 = arith.constant 8 : i64
    %c32_i64 = arith.constant 32 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c0_i64 = arith.constant 0 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c4_i64 = arith.constant 4 : i64
    %c5_i64 = arith.constant 5 : i64
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c8192 = arith.constant 8192 : index
    %c1 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %c8192_i64 = arith.constant 8192 : i64
    %cst = arith.constant 0.000000e+00 : f32
    %collapse_shape = memref.collapse_shape %11 [[0], [1, 2]] : memref<64x512x512xf32> into memref<64x262144xf32>
    %12 = llvm.mlir.constant(262144 : index) : i64
    %13 = llvm.mlir.constant(128 : index) : i64
    %14 = llvm.mlir.constant(1 : index) : i64
    %15 = llvm.mlir.constant(33554432 : index) : i64
    %16 = llvm.mlir.zero : !llvm.ptr
    %17 = llvm.getelementptr %16[%15] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %18 = llvm.ptrtoint %17 : !llvm.ptr to i64
    %19 = llvm.mlir.constant(64 : index) : i64
    %20 = llvm.add %18, %19  : i64
    %21 = llvm.call @malloc(%20) : (i64) -> !llvm.ptr
    %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
    %23 = llvm.mlir.constant(1 : index) : i64
    %24 = llvm.sub %19, %23  : i64
    %25 = llvm.add %22, %24  : i64
    %26 = llvm.urem %25, %19  : i64
    %27 = llvm.sub %25, %26  : i64
    %28 = llvm.inttoptr %27 : i64 to !llvm.ptr
    %29 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %30 = llvm.insertvalue %21, %29[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %31 = llvm.insertvalue %28, %30[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %32 = llvm.mlir.constant(0 : index) : i64
    %33 = llvm.insertvalue %32, %31[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %34 = llvm.insertvalue %12, %33[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %35 = llvm.insertvalue %13, %34[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %13, %35[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %14, %36[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = builtin.unrealized_conversion_cast %37 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%5 : memref<128x262144xf32>) outs(%38 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %39 = llvm.mlir.constant(64 : index) : i64
    %40 = llvm.mlir.constant(128 : index) : i64
    %41 = llvm.mlir.constant(1 : index) : i64
    %42 = llvm.mlir.constant(8192 : index) : i64
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[%42] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.mlir.constant(64 : index) : i64
    %47 = llvm.add %45, %46  : i64
    %48 = llvm.call @malloc(%47) : (i64) -> !llvm.ptr
    %49 = llvm.ptrtoint %48 : !llvm.ptr to i64
    %50 = llvm.mlir.constant(1 : index) : i64
    %51 = llvm.sub %46, %50  : i64
    %52 = llvm.add %49, %51  : i64
    %53 = llvm.urem %52, %46  : i64
    %54 = llvm.sub %52, %53  : i64
    %55 = llvm.inttoptr %54 : i64 to !llvm.ptr
    %56 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %57 = llvm.insertvalue %48, %56[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %55, %57[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.mlir.constant(0 : index) : i64
    %60 = llvm.insertvalue %59, %58[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %39, %60[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = llvm.insertvalue %40, %61[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %63 = llvm.insertvalue %40, %62[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %64 = llvm.insertvalue %41, %63[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %65 = builtin.unrealized_conversion_cast %64 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %66 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %67 = llvm.extractvalue %64[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.ptrtoint %67 : !llvm.ptr to i64
    %69 = builtin.unrealized_conversion_cast %68 : i64 to index
    %70 = arith.index_cast %69 : index to i64
    %71 = llvm.inttoptr %70 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %66, %cst, %71, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %72 = llvm.mlir.constant(2 : index) : i64
    %73 = llvm.mlir.constant(8192 : index) : i64
    %74 = llvm.mlir.constant(32 : index) : i64
    %75 = llvm.mlir.constant(32 : index) : i64
    %76 = llvm.mlir.constant(1 : index) : i64
    %77 = llvm.mlir.constant(1024 : index) : i64
    %78 = llvm.mlir.constant(8388608 : index) : i64
    %79 = llvm.mlir.constant(16777216 : index) : i64
    %80 = llvm.mlir.zero : !llvm.ptr
    %81 = llvm.getelementptr %80[%79] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %82 = llvm.ptrtoint %81 : !llvm.ptr to i64
    %83 = llvm.mlir.constant(64 : index) : i64
    %84 = llvm.add %82, %83  : i64
    %85 = llvm.call @malloc(%84) : (i64) -> !llvm.ptr
    %86 = llvm.ptrtoint %85 : !llvm.ptr to i64
    %87 = llvm.mlir.constant(1 : index) : i64
    %88 = llvm.sub %83, %87  : i64
    %89 = llvm.add %86, %88  : i64
    %90 = llvm.urem %89, %83  : i64
    %91 = llvm.sub %89, %90  : i64
    %92 = llvm.inttoptr %91 : i64 to !llvm.ptr
    %93 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %94 = llvm.insertvalue %85, %93[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %95 = llvm.insertvalue %92, %94[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %96 = llvm.mlir.constant(0 : index) : i64
    %97 = llvm.insertvalue %96, %95[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %98 = llvm.insertvalue %72, %97[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %99 = llvm.insertvalue %73, %98[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %100 = llvm.insertvalue %74, %99[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %101 = llvm.insertvalue %75, %100[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %102 = llvm.insertvalue %78, %101[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %103 = llvm.insertvalue %77, %102[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %104 = llvm.insertvalue %75, %103[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %105 = llvm.insertvalue %76, %104[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = builtin.unrealized_conversion_cast %105 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %107 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %157 = affine.apply #map2(%arg3)
      %158 = affine.apply #map2(%arg4)
      %subview = memref.subview %collapse_shape[%157, %158] [32, 32] [1, 1] : memref<64x262144xf32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %159 = builtin.unrealized_conversion_cast %subview : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %subview_0 = memref.subview %106[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %subview_0 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %161 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %162 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %163 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %164 = llvm.insertvalue %161, %163[0] : !llvm.struct<(ptr, ptr, i64)> 
      %165 = llvm.insertvalue %162, %164[1] : !llvm.struct<(ptr, ptr, i64)> 
      %166 = llvm.mlir.constant(0 : index) : i64
      %167 = llvm.insertvalue %166, %165[2] : !llvm.struct<(ptr, ptr, i64)> 
      %168 = llvm.extractvalue %159[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %169 = llvm.extractvalue %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %170 = llvm.extractvalue %159[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = llvm.extractvalue %159[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %172 = llvm.extractvalue %159[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = builtin.unrealized_conversion_cast %168 : i64 to index
      %174 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
      %176 = builtin.unrealized_conversion_cast %175 : i64 to index
      %177 = arith.index_cast %176 : index to i64
      %178 = llvm.inttoptr %177 : i64 to !llvm.ptr<f32>
      %179 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %180 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %181 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %182 = llvm.insertvalue %179, %181[0] : !llvm.struct<(ptr, ptr, i64)> 
      %183 = llvm.insertvalue %180, %182[1] : !llvm.struct<(ptr, ptr, i64)> 
      %184 = llvm.mlir.constant(0 : index) : i64
      %185 = llvm.insertvalue %184, %183[2] : !llvm.struct<(ptr, ptr, i64)> 
      %186 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %187 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %188 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %189 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %190 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %191 = builtin.unrealized_conversion_cast %186 : i64 to index
      %192 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %193 = llvm.ptrtoint %192 : !llvm.ptr to i64
      %194 = builtin.unrealized_conversion_cast %193 : i64 to index
      %195 = arith.index_cast %194 : index to i64
      %196 = llvm.inttoptr %195 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %107, %178, %173, %196, %191) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %108 = llvm.mlir.constant(4 : index) : i64
    %109 = llvm.mlir.constant(8192 : index) : i64
    %110 = llvm.mlir.constant(32 : index) : i64
    %111 = llvm.mlir.constant(32 : index) : i64
    %112 = llvm.mlir.constant(1 : index) : i64
    %113 = llvm.mlir.constant(1024 : index) : i64
    %114 = llvm.mlir.constant(8388608 : index) : i64
    %115 = llvm.mlir.constant(33554432 : index) : i64
    %116 = llvm.mlir.zero : !llvm.ptr
    %117 = llvm.getelementptr %116[%115] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %118 = llvm.ptrtoint %117 : !llvm.ptr to i64
    %119 = llvm.mlir.constant(64 : index) : i64
    %120 = llvm.add %118, %119  : i64
    %121 = llvm.call @malloc(%120) : (i64) -> !llvm.ptr
    %122 = llvm.ptrtoint %121 : !llvm.ptr to i64
    %123 = llvm.mlir.constant(1 : index) : i64
    %124 = llvm.sub %119, %123  : i64
    %125 = llvm.add %122, %124  : i64
    %126 = llvm.urem %125, %119  : i64
    %127 = llvm.sub %125, %126  : i64
    %128 = llvm.inttoptr %127 : i64 to !llvm.ptr
    %129 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %130 = llvm.insertvalue %121, %129[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %131 = llvm.insertvalue %128, %130[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %132 = llvm.mlir.constant(0 : index) : i64
    %133 = llvm.insertvalue %132, %131[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %134 = llvm.insertvalue %108, %133[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %135 = llvm.insertvalue %109, %134[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %136 = llvm.insertvalue %110, %135[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %137 = llvm.insertvalue %111, %136[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %138 = llvm.insertvalue %114, %137[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %139 = llvm.insertvalue %113, %138[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %140 = llvm.insertvalue %111, %139[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %141 = llvm.insertvalue %112, %140[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %142 = builtin.unrealized_conversion_cast %141 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %143 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %157 = affine.apply #map2(%arg4)
      %158 = affine.apply #map2(%arg3)
      %subview = memref.subview %38[%157, %158] [32, 32] [1, 1] : memref<262144x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %159 = builtin.unrealized_conversion_cast %subview : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %subview_0 = memref.subview %142[%arg3, %arg4, 0, 0] [1, 1, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %subview_0 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %161 = llvm.extractvalue %159[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %162 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %163 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %164 = llvm.insertvalue %161, %163[0] : !llvm.struct<(ptr, ptr, i64)> 
      %165 = llvm.insertvalue %162, %164[1] : !llvm.struct<(ptr, ptr, i64)> 
      %166 = llvm.mlir.constant(0 : index) : i64
      %167 = llvm.insertvalue %166, %165[2] : !llvm.struct<(ptr, ptr, i64)> 
      %168 = llvm.extractvalue %159[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %169 = llvm.extractvalue %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %170 = llvm.extractvalue %159[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = llvm.extractvalue %159[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %172 = llvm.extractvalue %159[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = builtin.unrealized_conversion_cast %168 : i64 to index
      %174 = llvm.extractvalue %159[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
      %176 = builtin.unrealized_conversion_cast %175 : i64 to index
      %177 = arith.index_cast %176 : index to i64
      %178 = llvm.inttoptr %177 : i64 to !llvm.ptr<f32>
      %179 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %180 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %181 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %182 = llvm.insertvalue %179, %181[0] : !llvm.struct<(ptr, ptr, i64)> 
      %183 = llvm.insertvalue %180, %182[1] : !llvm.struct<(ptr, ptr, i64)> 
      %184 = llvm.mlir.constant(0 : index) : i64
      %185 = llvm.insertvalue %184, %183[2] : !llvm.struct<(ptr, ptr, i64)> 
      %186 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %187 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %188 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %189 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %190 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %191 = builtin.unrealized_conversion_cast %186 : i64 to index
      %192 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %193 = llvm.ptrtoint %192 : !llvm.ptr to i64
      %194 = builtin.unrealized_conversion_cast %193 : i64 to index
      %195 = arith.index_cast %194 : index to i64
      %196 = llvm.inttoptr %195 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %143, %178, %173, %196, %191) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %expand_shape = memref.expand_shape %8 [[0, 1]] : memref<128xf32> into memref<4x32xf32>
    %144 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %145 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %146 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %157 = affine.apply #map2(%arg3)
      %158 = affine.apply #map2(%arg4)
      %159 = llvm.mlir.constant(32 : index) : i64
      %160 = llvm.mlir.constant(32 : index) : i64
      %161 = llvm.mlir.constant(1 : index) : i64
      %162 = llvm.mlir.constant(1024 : index) : i64
      %163 = llvm.mlir.zero : !llvm.ptr
      %164 = llvm.getelementptr %163[%162] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      %165 = llvm.ptrtoint %164 : !llvm.ptr to i64
      %166 = llvm.mlir.constant(64 : index) : i64
      %167 = llvm.add %165, %166  : i64
      %168 = llvm.call @malloc(%167) : (i64) -> !llvm.ptr
      %169 = llvm.ptrtoint %168 : !llvm.ptr to i64
      %170 = llvm.mlir.constant(1 : index) : i64
      %171 = llvm.sub %166, %170  : i64
      %172 = llvm.add %169, %171  : i64
      %173 = llvm.urem %172, %166  : i64
      %174 = llvm.sub %172, %173  : i64
      %175 = llvm.inttoptr %174 : i64 to !llvm.ptr
      %176 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %177 = llvm.insertvalue %168, %176[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %178 = llvm.insertvalue %175, %177[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %179 = llvm.mlir.constant(0 : index) : i64
      %180 = llvm.insertvalue %179, %178[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %181 = llvm.insertvalue %159, %180[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %182 = llvm.insertvalue %160, %181[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %183 = llvm.insertvalue %160, %182[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %184 = llvm.insertvalue %161, %183[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %subview = memref.subview %106[%arg3, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<2x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %185 = builtin.unrealized_conversion_cast %subview : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %subview_0 = memref.subview %142[%arg4, 0, 0, 0] [1, 8192, 32, 32] [1, 1, 1, 1] : memref<4x8192x32x32xf32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %186 = builtin.unrealized_conversion_cast %subview_0 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %187 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %188 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %189 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %190 = llvm.insertvalue %187, %189[0] : !llvm.struct<(ptr, ptr, i64)> 
      %191 = llvm.insertvalue %188, %190[1] : !llvm.struct<(ptr, ptr, i64)> 
      %192 = llvm.mlir.constant(0 : index) : i64
      %193 = llvm.insertvalue %192, %191[2] : !llvm.struct<(ptr, ptr, i64)> 
      %194 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %195 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %196 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %197 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %198 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %199 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %200 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %201 = builtin.unrealized_conversion_cast %194 : i64 to index
      %202 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %203 = llvm.ptrtoint %202 : !llvm.ptr to i64
      %204 = builtin.unrealized_conversion_cast %203 : i64 to index
      %205 = arith.index_cast %204 : index to i64
      %206 = llvm.inttoptr %205 : i64 to !llvm.ptr<f32>
      %207 = llvm.extractvalue %186[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %208 = llvm.extractvalue %186[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %209 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %210 = llvm.insertvalue %207, %209[0] : !llvm.struct<(ptr, ptr, i64)> 
      %211 = llvm.insertvalue %208, %210[1] : !llvm.struct<(ptr, ptr, i64)> 
      %212 = llvm.mlir.constant(0 : index) : i64
      %213 = llvm.insertvalue %212, %211[2] : !llvm.struct<(ptr, ptr, i64)> 
      %214 = llvm.extractvalue %186[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %215 = llvm.extractvalue %186[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %216 = llvm.extractvalue %186[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %217 = llvm.extractvalue %186[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %218 = llvm.extractvalue %186[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %219 = llvm.extractvalue %186[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %220 = llvm.extractvalue %186[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %221 = builtin.unrealized_conversion_cast %214 : i64 to index
      %222 = llvm.extractvalue %186[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %223 = llvm.ptrtoint %222 : !llvm.ptr to i64
      %224 = builtin.unrealized_conversion_cast %223 : i64 to index
      %225 = arith.index_cast %224 : index to i64
      %226 = llvm.inttoptr %225 : i64 to !llvm.ptr<f32>
      %227 = llvm.extractvalue %184[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %228 = llvm.ptrtoint %227 : !llvm.ptr to i64
      %229 = builtin.unrealized_conversion_cast %228 : i64 to index
      %230 = arith.index_cast %229 : index to i64
      %231 = llvm.inttoptr %230 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %144, %206, %201, %226, %221, %231, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %subview_1 = memref.subview %expand_shape[%arg4, 0] [1, 32] [1, 1] : memref<4x32xf32> to memref<32xf32, strided<[1], offset: ?>>
      %232 = builtin.unrealized_conversion_cast %subview_1 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %233 = llvm.extractvalue %232[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %234 = llvm.extractvalue %232[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %235 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %236 = llvm.insertvalue %233, %235[0] : !llvm.struct<(ptr, ptr, i64)> 
      %237 = llvm.insertvalue %234, %236[1] : !llvm.struct<(ptr, ptr, i64)> 
      %238 = llvm.mlir.constant(0 : index) : i64
      %239 = llvm.insertvalue %238, %237[2] : !llvm.struct<(ptr, ptr, i64)> 
      %240 = llvm.extractvalue %232[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %241 = llvm.extractvalue %232[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %242 = llvm.extractvalue %232[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %243 = builtin.unrealized_conversion_cast %240 : i64 to index
      %244 = llvm.extractvalue %232[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %245 = llvm.ptrtoint %244 : !llvm.ptr to i64
      %246 = builtin.unrealized_conversion_cast %245 : i64 to index
      %247 = arith.index_cast %246 : index to i64
      %248 = llvm.inttoptr %247 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %145, %248, %243, %231, %c0, %231, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %subview_2 = memref.subview %65[%157, %158] [32, 32] [1, 1] : memref<64x128xf32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %249 = builtin.unrealized_conversion_cast %subview_2 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %250 = llvm.extractvalue %249[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %251 = llvm.extractvalue %249[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %252 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
      %253 = llvm.insertvalue %250, %252[0] : !llvm.struct<(ptr, ptr, i64)> 
      %254 = llvm.insertvalue %251, %253[1] : !llvm.struct<(ptr, ptr, i64)> 
      %255 = llvm.mlir.constant(0 : index) : i64
      %256 = llvm.insertvalue %255, %254[2] : !llvm.struct<(ptr, ptr, i64)> 
      %257 = llvm.extractvalue %249[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %258 = llvm.extractvalue %249[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %259 = llvm.extractvalue %249[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %260 = llvm.extractvalue %249[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %261 = llvm.extractvalue %249[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %262 = builtin.unrealized_conversion_cast %257 : i64 to index
      %263 = llvm.extractvalue %249[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %264 = llvm.ptrtoint %263 : !llvm.ptr to i64
      %265 = builtin.unrealized_conversion_cast %264 : i64 to index
      %266 = arith.index_cast %265 : index to i64
      %267 = llvm.inttoptr %266 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %146, %231, %c0, %267, %262) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %268 = llvm.extractvalue %184[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      llvm.call @free(%268) : (!llvm.ptr) -> ()
      scf.yield
    }
    %147 = llvm.extractvalue %37[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.call @free(%147) : (!llvm.ptr) -> ()
    %148 = llvm.extractvalue %105[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    llvm.call @free(%148) : (!llvm.ptr) -> ()
    %149 = llvm.extractvalue %141[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    llvm.call @free(%149) : (!llvm.ptr) -> ()
    %150 = llvm.mlir.constant(1 : index) : i64
    %151 = llvm.alloca %150 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %64, %151 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %152 = llvm.mlir.constant(2 : index) : i64
    %153 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %154 = llvm.insertvalue %152, %153[0] : !llvm.struct<(i64, ptr)> 
    %155 = llvm.insertvalue %151, %154[1] : !llvm.struct<(i64, ptr)> 
    %156 = builtin.unrealized_conversion_cast %155 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%156) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before ConvertTensorToLinalg (convert-tensor-to-linalg) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @refbackend_consume_func_return_mrf32) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @xsmm_unary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @xsmm_binary_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @xsmm_brgemm_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @xsmm_unary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @xsmm_binary_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @xsmm_brgemm_dispatch) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @xsmm_unary_scalar_invoke) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d1, d0)>
#map2 = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map3 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map4 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map5 = affine_map<()[s0] -> (s0 * 8388608)>
#map6 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c1 = arith.constant 1 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%16 : memref<128x262144xf32>) outs(%42 : memref<262144x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map4(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map3()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map5()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map5()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map6()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map4(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) ('func.func' operation: @MLP) //----- //
#map = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map1 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map2 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map3 = affine_map<()[s0] -> (s0 * 8388608)>
#map4 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map1()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map2(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map1()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map3()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map3()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map4()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before ConvertSCFToOpenMPPass (convert-scf-to-openmp) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map1 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map2 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map3 = affine_map<()[s0] -> (s0 * 8388608)>
#map4 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
      %126 = affine.apply #map(%arg3, %arg4)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map1()[%arg3, %arg4]
      %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
      %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
      %126 = affine.apply #map2(%arg4, %arg3)
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %128 = affine.apply #map1()[%arg3, %arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
      %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = builtin.unrealized_conversion_cast %130 : i64 to index
      %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
      %134 = builtin.unrealized_conversion_cast %133 : i64 to index
      %135 = arith.index_cast %134 : index to i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
      %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %138 = builtin.unrealized_conversion_cast %137 : i64 to index
      %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
      %141 = builtin.unrealized_conversion_cast %140 : i64 to index
      %142 = arith.index_cast %141 : index to i64
      %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      scf.yield
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    scf.parallel (%arg3, %arg4) = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
      %126 = llvm.mlir.zero : !llvm.ptr
      %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
      %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
      %129 = llvm.add %128, %7  : i64
      %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
      %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
      %132 = llvm.sub %7, %8  : i64
      %133 = llvm.add %131, %132  : i64
      %134 = llvm.urem %133, %7  : i64
      %135 = llvm.sub %133, %134  : i64
      %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
      %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %137 = affine.apply #map3()[%arg3]
      %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
      %139 = affine.apply #map3()[%arg4]
      %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
      %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
      %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %142 = builtin.unrealized_conversion_cast %141 : i64 to index
      %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
      %145 = builtin.unrealized_conversion_cast %144 : i64 to index
      %146 = arith.index_cast %145 : index to i64
      %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
      %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %149 = builtin.unrealized_conversion_cast %148 : i64 to index
      %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
      %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
      %152 = builtin.unrealized_conversion_cast %151 : i64 to index
      %153 = arith.index_cast %152 : index to i64
      %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
      %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
      %156 = builtin.unrealized_conversion_cast %155 : i64 to index
      %157 = arith.index_cast %156 : index to i64
      %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
      func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
      %159 = affine.apply #map4()[%arg4]
      %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
      %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
      %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %162 = builtin.unrealized_conversion_cast %161 : i64 to index
      %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
      %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
      %165 = builtin.unrealized_conversion_cast %164 : i64 to index
      %166 = arith.index_cast %165 : index to i64
      %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
      func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
      %168 = affine.apply #map2(%arg3, %arg4)
      %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
      %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
      %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %171 = builtin.unrealized_conversion_cast %170 : i64 to index
      %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
      %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
      %174 = builtin.unrealized_conversion_cast %173 : i64 to index
      %175 = arith.index_cast %174 : index to i64
      %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
      func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
      llvm.call @free(%130) : (!llvm.ptr) -> ()
      scf.yield
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After ConvertSCFToOpenMPPass (convert-scf-to-openmp) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map1 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map2 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map3 = affine_map<()[s0] -> (s0 * 8388608)>
#map4 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %129 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %129, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %93 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %129 = affine.apply #map(%arg3, %arg4)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%129], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %130 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %131 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%131], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %132 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %133 = llvm.extractvalue %130[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = llvm.extractvalue %130[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %136 = llvm.ptrtoint %135 : !llvm.ptr to i64
          %137 = builtin.unrealized_conversion_cast %136 : i64 to index
          %138 = arith.index_cast %137 : index to i64
          %139 = llvm.inttoptr %138 : i64 to !llvm.ptr<f32>
          %140 = llvm.extractvalue %132[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = llvm.extractvalue %132[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
          %144 = builtin.unrealized_conversion_cast %143 : i64 to index
          %145 = arith.index_cast %144 : index to i64
          %146 = llvm.inttoptr %145 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %139, %134, %146, %141) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %94 = llvm.mlir.zero : !llvm.ptr
    %95 = llvm.getelementptr %94[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %96 = llvm.ptrtoint %95 : !llvm.ptr to i64
    %97 = llvm.add %96, %7  : i64
    %98 = llvm.call @malloc(%97) : (i64) -> !llvm.ptr
    %99 = llvm.ptrtoint %98 : !llvm.ptr to i64
    %100 = llvm.sub %7, %8  : i64
    %101 = llvm.add %99, %100  : i64
    %102 = llvm.urem %101, %7  : i64
    %103 = llvm.sub %101, %102  : i64
    %104 = llvm.inttoptr %103 : i64 to !llvm.ptr
    %105 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %106 = llvm.insertvalue %98, %105[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %104, %106[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %6, %107[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %0, %108[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %5, %109[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %3, %111[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %1, %112[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %2, %113[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %3, %114[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = llvm.insertvalue %8, %115[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %117 = builtin.unrealized_conversion_cast %116 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %118 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %129 = affine.apply #map2(%arg4, %arg3)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%129], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %130 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %117 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %131 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%131], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %132 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %133 = llvm.extractvalue %130[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = llvm.extractvalue %130[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %136 = llvm.ptrtoint %135 : !llvm.ptr to i64
          %137 = builtin.unrealized_conversion_cast %136 : i64 to index
          %138 = arith.index_cast %137 : index to i64
          %139 = llvm.inttoptr %138 : i64 to !llvm.ptr<f32>
          %140 = llvm.extractvalue %132[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = llvm.extractvalue %132[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
          %144 = builtin.unrealized_conversion_cast %143 : i64 to index
          %145 = arith.index_cast %144 : index to i64
          %146 = llvm.inttoptr %145 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %118, %139, %134, %146, %141) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %120 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %121 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %122 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %123 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %129 = llvm.mlir.zero : !llvm.ptr
          %130 = llvm.getelementptr %129[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.add %131, %7  : i64
          %133 = llvm.call @malloc(%132) : (i64) -> !llvm.ptr
          %134 = llvm.ptrtoint %133 : !llvm.ptr to i64
          %135 = llvm.sub %7, %8  : i64
          %136 = llvm.add %134, %135  : i64
          %137 = llvm.urem %136, %7  : i64
          %138 = llvm.sub %136, %137  : i64
          %139 = llvm.inttoptr %138 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %140 = affine.apply #map3()[%arg3]
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%140], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %141 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %117 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %142 = affine.apply #map3()[%arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%142], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %143 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %144 = llvm.extractvalue %141[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = llvm.extractvalue %141[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %147 = llvm.ptrtoint %146 : !llvm.ptr to i64
          %148 = builtin.unrealized_conversion_cast %147 : i64 to index
          %149 = arith.index_cast %148 : index to i64
          %150 = llvm.inttoptr %149 : i64 to !llvm.ptr<f32>
          %151 = llvm.extractvalue %143[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = llvm.extractvalue %143[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %154 = llvm.ptrtoint %153 : !llvm.ptr to i64
          %155 = builtin.unrealized_conversion_cast %154 : i64 to index
          %156 = arith.index_cast %155 : index to i64
          %157 = llvm.inttoptr %156 : i64 to !llvm.ptr<f32>
          %158 = llvm.ptrtoint %139 : !llvm.ptr to i64
          %159 = builtin.unrealized_conversion_cast %158 : i64 to index
          %160 = arith.index_cast %159 : index to i64
          %161 = llvm.inttoptr %160 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %120, %150, %145, %157, %152, %161, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %162 = affine.apply #map4()[%arg4]
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%162], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %163 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %164 = llvm.extractvalue %163[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = llvm.extractvalue %163[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %167 = llvm.ptrtoint %166 : !llvm.ptr to i64
          %168 = builtin.unrealized_conversion_cast %167 : i64 to index
          %169 = arith.index_cast %168 : index to i64
          %170 = llvm.inttoptr %169 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %121, %170, %165, %161, %c0, %161, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %171 = affine.apply #map2(%arg3, %arg4)
          %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%171], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %172 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %173 = llvm.extractvalue %172[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %174 = builtin.unrealized_conversion_cast %173 : i64 to index
          %175 = llvm.extractvalue %172[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %176 = llvm.ptrtoint %175 : !llvm.ptr to i64
          %177 = builtin.unrealized_conversion_cast %176 : i64 to index
          %178 = arith.index_cast %177 : index to i64
          %179 = llvm.inttoptr %178 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %122, %161, %c0, %179, %174) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%133) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%98) : (!llvm.ptr) -> ()
    %124 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %124 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %125 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %126 = llvm.insertvalue %4, %125[0] : !llvm.struct<(i64, ptr)> 
    %127 = llvm.insertvalue %124, %126[1] : !llvm.struct<(i64, ptr)> 
    %128 = builtin.unrealized_conversion_cast %127 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%128) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before ConvertVectorToSCF (convert-vector-to-scf) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map1 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map2 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map3 = affine_map<()[s0] -> (s0 * 8388608)>
#map4 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %129 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %129, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %93 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %129 = affine.apply #map(%arg3, %arg4)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%129], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %130 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %131 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%131], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %132 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %133 = llvm.extractvalue %130[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = llvm.extractvalue %130[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %136 = llvm.ptrtoint %135 : !llvm.ptr to i64
          %137 = builtin.unrealized_conversion_cast %136 : i64 to index
          %138 = arith.index_cast %137 : index to i64
          %139 = llvm.inttoptr %138 : i64 to !llvm.ptr<f32>
          %140 = llvm.extractvalue %132[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = llvm.extractvalue %132[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
          %144 = builtin.unrealized_conversion_cast %143 : i64 to index
          %145 = arith.index_cast %144 : index to i64
          %146 = llvm.inttoptr %145 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %139, %134, %146, %141) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %94 = llvm.mlir.zero : !llvm.ptr
    %95 = llvm.getelementptr %94[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %96 = llvm.ptrtoint %95 : !llvm.ptr to i64
    %97 = llvm.add %96, %7  : i64
    %98 = llvm.call @malloc(%97) : (i64) -> !llvm.ptr
    %99 = llvm.ptrtoint %98 : !llvm.ptr to i64
    %100 = llvm.sub %7, %8  : i64
    %101 = llvm.add %99, %100  : i64
    %102 = llvm.urem %101, %7  : i64
    %103 = llvm.sub %101, %102  : i64
    %104 = llvm.inttoptr %103 : i64 to !llvm.ptr
    %105 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %106 = llvm.insertvalue %98, %105[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %104, %106[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %6, %107[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %0, %108[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %5, %109[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %3, %111[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %1, %112[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %2, %113[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %3, %114[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = llvm.insertvalue %8, %115[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %117 = builtin.unrealized_conversion_cast %116 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %118 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %129 = affine.apply #map2(%arg4, %arg3)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%129], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %130 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %117 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %131 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%131], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %132 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %133 = llvm.extractvalue %130[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = llvm.extractvalue %130[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %136 = llvm.ptrtoint %135 : !llvm.ptr to i64
          %137 = builtin.unrealized_conversion_cast %136 : i64 to index
          %138 = arith.index_cast %137 : index to i64
          %139 = llvm.inttoptr %138 : i64 to !llvm.ptr<f32>
          %140 = llvm.extractvalue %132[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = llvm.extractvalue %132[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
          %144 = builtin.unrealized_conversion_cast %143 : i64 to index
          %145 = arith.index_cast %144 : index to i64
          %146 = llvm.inttoptr %145 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %118, %139, %134, %146, %141) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %120 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %121 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %122 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %123 = llvm.mlir.constant(1 : i64) : i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %129 = llvm.mlir.zero : !llvm.ptr
          %130 = llvm.getelementptr %129[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.add %131, %7  : i64
          %133 = llvm.call @malloc(%132) : (i64) -> !llvm.ptr
          %134 = llvm.ptrtoint %133 : !llvm.ptr to i64
          %135 = llvm.sub %7, %8  : i64
          %136 = llvm.add %134, %135  : i64
          %137 = llvm.urem %136, %7  : i64
          %138 = llvm.sub %136, %137  : i64
          %139 = llvm.inttoptr %138 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %140 = affine.apply #map3()[%arg3]
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%140], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %141 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %117 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %142 = affine.apply #map3()[%arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%142], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %143 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %144 = llvm.extractvalue %141[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = llvm.extractvalue %141[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %147 = llvm.ptrtoint %146 : !llvm.ptr to i64
          %148 = builtin.unrealized_conversion_cast %147 : i64 to index
          %149 = arith.index_cast %148 : index to i64
          %150 = llvm.inttoptr %149 : i64 to !llvm.ptr<f32>
          %151 = llvm.extractvalue %143[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = llvm.extractvalue %143[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %154 = llvm.ptrtoint %153 : !llvm.ptr to i64
          %155 = builtin.unrealized_conversion_cast %154 : i64 to index
          %156 = arith.index_cast %155 : index to i64
          %157 = llvm.inttoptr %156 : i64 to !llvm.ptr<f32>
          %158 = llvm.ptrtoint %139 : !llvm.ptr to i64
          %159 = builtin.unrealized_conversion_cast %158 : i64 to index
          %160 = arith.index_cast %159 : index to i64
          %161 = llvm.inttoptr %160 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %120, %150, %145, %157, %152, %161, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %162 = affine.apply #map4()[%arg4]
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%162], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %163 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %164 = llvm.extractvalue %163[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = llvm.extractvalue %163[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %167 = llvm.ptrtoint %166 : !llvm.ptr to i64
          %168 = builtin.unrealized_conversion_cast %167 : i64 to index
          %169 = arith.index_cast %168 : index to i64
          %170 = llvm.inttoptr %169 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %121, %170, %165, %161, %c0, %161, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %171 = affine.apply #map2(%arg3, %arg4)
          %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%171], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %172 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %173 = llvm.extractvalue %172[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %174 = builtin.unrealized_conversion_cast %173 : i64 to index
          %175 = llvm.extractvalue %172[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %176 = llvm.ptrtoint %175 : !llvm.ptr to i64
          %177 = builtin.unrealized_conversion_cast %176 : i64 to index
          %178 = arith.index_cast %177 : index to i64
          %179 = llvm.inttoptr %178 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %122, %161, %c0, %179, %174) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%133) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%98) : (!llvm.ptr) -> ()
    %124 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %124 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %125 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %126 = llvm.insertvalue %4, %125[0] : !llvm.struct<(i64, ptr)> 
    %127 = llvm.insertvalue %124, %126[1] : !llvm.struct<(i64, ptr)> 
    %128 = builtin.unrealized_conversion_cast %127 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%128) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After ConvertVectorToSCF (convert-vector-to-scf) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map1 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map2 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map3 = affine_map<()[s0] -> (s0 * 8388608)>
#map4 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = affine.apply #map(%arg3, %arg4)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %128 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %131 = builtin.unrealized_conversion_cast %130 : i64 to index
          %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = arith.index_cast %134 : index to i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
          %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = arith.index_cast %141 : index to i64
          %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %126 = affine.apply #map2(%arg4, %arg3)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %128 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %131 = builtin.unrealized_conversion_cast %130 : i64 to index
          %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = arith.index_cast %134 : index to i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
          %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = arith.index_cast %141 : index to i64
          %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = llvm.mlir.zero : !llvm.ptr
          %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
          %129 = llvm.add %128, %7  : i64
          %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.sub %7, %8  : i64
          %133 = llvm.add %131, %132  : i64
          %134 = llvm.urem %133, %7  : i64
          %135 = llvm.sub %133, %134  : i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %137 = affine.apply #map3()[%arg3]
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %139 = affine.apply #map3()[%arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %149 = builtin.unrealized_conversion_cast %148 : i64 to index
          %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = arith.index_cast %152 : index to i64
          %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
          %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %156 = builtin.unrealized_conversion_cast %155 : i64 to index
          %157 = arith.index_cast %156 : index to i64
          %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %159 = affine.apply #map4()[%arg4]
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %162 = builtin.unrealized_conversion_cast %161 : i64 to index
          %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = arith.index_cast %165 : index to i64
          %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %168 = affine.apply #map2(%arg3, %arg4)
          %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %171 = builtin.unrealized_conversion_cast %170 : i64 to index
          %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
          %174 = builtin.unrealized_conversion_cast %173 : i64 to index
          %175 = arith.index_cast %174 : index to i64
          %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%130) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before ArithExpandOpsPass (arith-expand) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map1 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map2 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map3 = affine_map<()[s0] -> (s0 * 8388608)>
#map4 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = affine.apply #map(%arg3, %arg4)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %128 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %131 = builtin.unrealized_conversion_cast %130 : i64 to index
          %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = arith.index_cast %134 : index to i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
          %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = arith.index_cast %141 : index to i64
          %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %126 = affine.apply #map2(%arg4, %arg3)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %128 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %131 = builtin.unrealized_conversion_cast %130 : i64 to index
          %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = arith.index_cast %134 : index to i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
          %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = arith.index_cast %141 : index to i64
          %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = llvm.mlir.zero : !llvm.ptr
          %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
          %129 = llvm.add %128, %7  : i64
          %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.sub %7, %8  : i64
          %133 = llvm.add %131, %132  : i64
          %134 = llvm.urem %133, %7  : i64
          %135 = llvm.sub %133, %134  : i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %137 = affine.apply #map3()[%arg3]
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %139 = affine.apply #map3()[%arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %149 = builtin.unrealized_conversion_cast %148 : i64 to index
          %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = arith.index_cast %152 : index to i64
          %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
          %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %156 = builtin.unrealized_conversion_cast %155 : i64 to index
          %157 = arith.index_cast %156 : index to i64
          %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %159 = affine.apply #map4()[%arg4]
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %162 = builtin.unrealized_conversion_cast %161 : i64 to index
          %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = arith.index_cast %165 : index to i64
          %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %168 = affine.apply #map2(%arg3, %arg4)
          %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %171 = builtin.unrealized_conversion_cast %170 : i64 to index
          %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
          %174 = builtin.unrealized_conversion_cast %173 : i64 to index
          %175 = arith.index_cast %174 : index to i64
          %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%130) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before ConvertAffineToStandard (lower-affine) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> (d0 * 8388608 + d1 * 32)>
#map1 = affine_map<()[s0, s1] -> (s0 * 8388608 + s1 * 1024)>
#map2 = affine_map<(d0, d1) -> (d0 * 4096 + d1 * 32)>
#map3 = affine_map<()[s0] -> (s0 * 8388608)>
#map4 = affine_map<()[s0] -> (s0 * 32)>
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = affine.apply #map(%arg3, %arg4)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%126], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %128 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %131 = builtin.unrealized_conversion_cast %130 : i64 to index
          %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = arith.index_cast %134 : index to i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
          %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = arith.index_cast %141 : index to i64
          %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %126 = affine.apply #map2(%arg4, %arg3)
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%126], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %127 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %128 = affine.apply #map1()[%arg3, %arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%128], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %130 = llvm.extractvalue %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %131 = builtin.unrealized_conversion_cast %130 : i64 to index
          %132 = llvm.extractvalue %127[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %133 = llvm.ptrtoint %132 : !llvm.ptr to i64
          %134 = builtin.unrealized_conversion_cast %133 : i64 to index
          %135 = arith.index_cast %134 : index to i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
          %137 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
          %141 = builtin.unrealized_conversion_cast %140 : i64 to index
          %142 = arith.index_cast %141 : index to i64
          %143 = llvm.inttoptr %142 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %117, %136, %131, %143, %138) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = llvm.mlir.zero : !llvm.ptr
          %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
          %129 = llvm.add %128, %7  : i64
          %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.sub %7, %8  : i64
          %133 = llvm.add %131, %132  : i64
          %134 = llvm.urem %133, %7  : i64
          %135 = llvm.sub %133, %134  : i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %137 = affine.apply #map3()[%arg3]
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %139 = affine.apply #map3()[%arg4]
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %149 = builtin.unrealized_conversion_cast %148 : i64 to index
          %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = arith.index_cast %152 : index to i64
          %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
          %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %156 = builtin.unrealized_conversion_cast %155 : i64 to index
          %157 = arith.index_cast %156 : index to i64
          %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %159 = affine.apply #map4()[%arg4]
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %162 = builtin.unrealized_conversion_cast %161 : i64 to index
          %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = arith.index_cast %165 : index to i64
          %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %168 = affine.apply #map2(%arg3, %arg4)
          %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%168], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %169 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %170 = llvm.extractvalue %169[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %171 = builtin.unrealized_conversion_cast %170 : i64 to index
          %172 = llvm.extractvalue %169[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %173 = llvm.ptrtoint %172 : !llvm.ptr to i64
          %174 = builtin.unrealized_conversion_cast %173 : i64 to index
          %175 = arith.index_cast %174 : index to i64
          %176 = llvm.inttoptr %175 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %176, %171) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%130) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After ConvertAffineToStandard (lower-affine) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %c8388608 = arith.constant 8388608 : index
          %126 = arith.muli %arg3, %c8388608 : index
          %c32 = arith.constant 32 : index
          %127 = arith.muli %arg4, %c32 : index
          %128 = arith.addi %126, %127 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%128], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %c8388608_8 = arith.constant 8388608 : index
          %130 = arith.muli %arg3, %c8388608_8 : index
          %c1024 = arith.constant 1024 : index
          %131 = arith.muli %arg4, %c1024 : index
          %132 = arith.addi %130, %131 : index
          %reinterpret_cast_9 = memref.reinterpret_cast %base_buffer_4 to offset: [%132], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %133 = builtin.unrealized_conversion_cast %reinterpret_cast_9 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %134 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = builtin.unrealized_conversion_cast %134 : i64 to index
          %136 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %137 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = arith.index_cast %138 : index to i64
          %140 = llvm.inttoptr %139 : i64 to !llvm.ptr<f32>
          %141 = llvm.extractvalue %133[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %133[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %140, %135, %147, %142) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %c4096 = arith.constant 4096 : index
          %126 = arith.muli %arg4, %c4096 : index
          %c32 = arith.constant 32 : index
          %127 = arith.muli %arg3, %c32 : index
          %128 = arith.addi %126, %127 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %c8388608 = arith.constant 8388608 : index
          %130 = arith.muli %arg3, %c8388608 : index
          %c1024 = arith.constant 1024 : index
          %131 = arith.muli %arg4, %c1024 : index
          %132 = arith.addi %130, %131 : index
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%132], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %133 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %134 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = builtin.unrealized_conversion_cast %134 : i64 to index
          %136 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %137 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = arith.index_cast %138 : index to i64
          %140 = llvm.inttoptr %139 : i64 to !llvm.ptr<f32>
          %141 = llvm.extractvalue %133[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %133[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %117, %140, %135, %147, %142) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = llvm.mlir.zero : !llvm.ptr
          %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
          %129 = llvm.add %128, %7  : i64
          %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.sub %7, %8  : i64
          %133 = llvm.add %131, %132  : i64
          %134 = llvm.urem %133, %7  : i64
          %135 = llvm.sub %133, %134  : i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %c8388608 = arith.constant 8388608 : index
          %137 = arith.muli %arg3, %c8388608 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %c8388608_12 = arith.constant 8388608 : index
          %139 = arith.muli %arg4, %c8388608_12 : index
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %140 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %149 = builtin.unrealized_conversion_cast %148 : i64 to index
          %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = arith.index_cast %152 : index to i64
          %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
          %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %156 = builtin.unrealized_conversion_cast %155 : i64 to index
          %157 = arith.index_cast %156 : index to i64
          %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %c32 = arith.constant 32 : index
          %159 = arith.muli %arg4, %c32 : index
          %reinterpret_cast_14 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %160 = builtin.unrealized_conversion_cast %reinterpret_cast_14 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %162 = builtin.unrealized_conversion_cast %161 : i64 to index
          %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = arith.index_cast %165 : index to i64
          %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_15, %offset_16, %sizes_17:2, %strides_18:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %c4096 = arith.constant 4096 : index
          %168 = arith.muli %arg3, %c4096 : index
          %c32_19 = arith.constant 32 : index
          %169 = arith.muli %arg4, %c32_19 : index
          %170 = arith.addi %168, %169 : index
          %reinterpret_cast_20 = memref.reinterpret_cast %base_buffer_15 to offset: [%170], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %171 = builtin.unrealized_conversion_cast %reinterpret_cast_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %172 = llvm.extractvalue %171[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %173 = builtin.unrealized_conversion_cast %172 : i64 to index
          %174 = llvm.extractvalue %171[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
          %176 = builtin.unrealized_conversion_cast %175 : i64 to index
          %177 = arith.index_cast %176 : index to i64
          %178 = llvm.inttoptr %177 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %178, %173) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%130) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before ConvertVectorToLLVMPass (convert-vector-to-llvm) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %c8388608 = arith.constant 8388608 : index
          %126 = arith.muli %arg3, %c8388608 : index
          %c32 = arith.constant 32 : index
          %127 = arith.muli %arg4, %c32 : index
          %128 = arith.addi %126, %127 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%128], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %c8388608_8 = arith.constant 8388608 : index
          %130 = arith.muli %arg3, %c8388608_8 : index
          %c1024 = arith.constant 1024 : index
          %131 = arith.muli %arg4, %c1024 : index
          %132 = arith.addi %130, %131 : index
          %reinterpret_cast_9 = memref.reinterpret_cast %base_buffer_4 to offset: [%132], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %133 = builtin.unrealized_conversion_cast %reinterpret_cast_9 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %134 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = builtin.unrealized_conversion_cast %134 : i64 to index
          %136 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %137 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = arith.index_cast %138 : index to i64
          %140 = llvm.inttoptr %139 : i64 to !llvm.ptr<f32>
          %141 = llvm.extractvalue %133[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %133[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %140, %135, %147, %142) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %c4096 = arith.constant 4096 : index
          %126 = arith.muli %arg4, %c4096 : index
          %c32 = arith.constant 32 : index
          %127 = arith.muli %arg3, %c32 : index
          %128 = arith.addi %126, %127 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %c8388608 = arith.constant 8388608 : index
          %130 = arith.muli %arg3, %c8388608 : index
          %c1024 = arith.constant 1024 : index
          %131 = arith.muli %arg4, %c1024 : index
          %132 = arith.addi %130, %131 : index
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%132], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %133 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %134 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = builtin.unrealized_conversion_cast %134 : i64 to index
          %136 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %137 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = arith.index_cast %138 : index to i64
          %140 = llvm.inttoptr %139 : i64 to !llvm.ptr<f32>
          %141 = llvm.extractvalue %133[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %133[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %117, %140, %135, %147, %142) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = llvm.mlir.zero : !llvm.ptr
          %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
          %129 = llvm.add %128, %7  : i64
          %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.sub %7, %8  : i64
          %133 = llvm.add %131, %132  : i64
          %134 = llvm.urem %133, %7  : i64
          %135 = llvm.sub %133, %134  : i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %c8388608 = arith.constant 8388608 : index
          %137 = arith.muli %arg3, %c8388608 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %c8388608_12 = arith.constant 8388608 : index
          %139 = arith.muli %arg4, %c8388608_12 : index
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %140 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %149 = builtin.unrealized_conversion_cast %148 : i64 to index
          %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = arith.index_cast %152 : index to i64
          %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
          %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %156 = builtin.unrealized_conversion_cast %155 : i64 to index
          %157 = arith.index_cast %156 : index to i64
          %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %c32 = arith.constant 32 : index
          %159 = arith.muli %arg4, %c32 : index
          %reinterpret_cast_14 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %160 = builtin.unrealized_conversion_cast %reinterpret_cast_14 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %162 = builtin.unrealized_conversion_cast %161 : i64 to index
          %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = arith.index_cast %165 : index to i64
          %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_15, %offset_16, %sizes_17:2, %strides_18:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %c4096 = arith.constant 4096 : index
          %168 = arith.muli %arg3, %c4096 : index
          %c32_19 = arith.constant 32 : index
          %169 = arith.muli %arg4, %c32_19 : index
          %170 = arith.addi %168, %169 : index
          %reinterpret_cast_20 = memref.reinterpret_cast %base_buffer_15 to offset: [%170], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %171 = builtin.unrealized_conversion_cast %reinterpret_cast_20 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %172 = llvm.extractvalue %171[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %173 = builtin.unrealized_conversion_cast %172 : i64 to index
          %174 = llvm.extractvalue %171[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
          %176 = builtin.unrealized_conversion_cast %175 : i64 to index
          %177 = arith.index_cast %176 : index to i64
          %178 = llvm.inttoptr %177 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %178, %173) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%130) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After ConvertVectorToLLVMPass (convert-vector-to-llvm) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c4096 = arith.constant 4096 : index
    %c1024 = arith.constant 1024 : index
    %c32 = arith.constant 32 : index
    %c8388608 = arith.constant 8388608 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = arith.muli %arg3, %c8388608 : index
          %127 = arith.muli %arg4, %c32 : index
          %128 = arith.addi %126, %127 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%128], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %130 = arith.muli %arg3, %c8388608 : index
          %131 = arith.muli %arg4, %c1024 : index
          %132 = arith.addi %130, %131 : index
          %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%132], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %133 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %134 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = builtin.unrealized_conversion_cast %134 : i64 to index
          %136 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %137 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = arith.index_cast %138 : index to i64
          %140 = llvm.inttoptr %139 : i64 to !llvm.ptr<f32>
          %141 = llvm.extractvalue %133[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %133[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %140, %135, %147, %142) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %126 = arith.muli %arg4, %c4096 : index
          %127 = arith.muli %arg3, %c32 : index
          %128 = arith.addi %126, %127 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %130 = arith.muli %arg3, %c8388608 : index
          %131 = arith.muli %arg4, %c1024 : index
          %132 = arith.addi %130, %131 : index
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%132], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %133 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %134 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = builtin.unrealized_conversion_cast %134 : i64 to index
          %136 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %137 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = arith.index_cast %138 : index to i64
          %140 = llvm.inttoptr %139 : i64 to !llvm.ptr<f32>
          %141 = llvm.extractvalue %133[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %133[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %117, %140, %135, %147, %142) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = llvm.mlir.zero : !llvm.ptr
          %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
          %129 = llvm.add %128, %7  : i64
          %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.sub %7, %8  : i64
          %133 = llvm.add %131, %132  : i64
          %134 = llvm.urem %133, %7  : i64
          %135 = llvm.sub %133, %134  : i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %137 = arith.muli %arg3, %c8388608 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %139 = arith.muli %arg4, %c8388608 : index
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %149 = builtin.unrealized_conversion_cast %148 : i64 to index
          %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = arith.index_cast %152 : index to i64
          %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
          %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %156 = builtin.unrealized_conversion_cast %155 : i64 to index
          %157 = arith.index_cast %156 : index to i64
          %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %159 = arith.muli %arg4, %c32 : index
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %162 = builtin.unrealized_conversion_cast %161 : i64 to index
          %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = arith.index_cast %165 : index to i64
          %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %168 = arith.muli %arg3, %c4096 : index
          %169 = arith.muli %arg4, %c32 : index
          %170 = arith.addi %168, %169 : index
          %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%170], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %171 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %172 = llvm.extractvalue %171[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %173 = builtin.unrealized_conversion_cast %172 : i64 to index
          %174 = llvm.extractvalue %171[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
          %176 = builtin.unrealized_conversion_cast %175 : i64 to index
          %177 = arith.index_cast %176 : index to i64
          %178 = llvm.inttoptr %177 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %178, %173) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%130) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c4096 = arith.constant 4096 : index
    %c1024 = arith.constant 1024 : index
    %c32 = arith.constant 32 : index
    %c8388608 = arith.constant 8388608 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %base_buffer, %offset, %sizes:3, %strides:3 = memref.extract_strided_metadata %22 : memref<64x512x512xf32> -> memref<f32>, index, index, index, index, index, index, index
    %23 = llvm.mlir.zero : !llvm.ptr
    %24 = llvm.getelementptr %23[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %25 = llvm.ptrtoint %24 : !llvm.ptr to i64
    %26 = llvm.add %25, %7  : i64
    %27 = llvm.call @malloc(%26) : (i64) -> !llvm.ptr
    %28 = llvm.ptrtoint %27 : !llvm.ptr to i64
    %29 = llvm.sub %7, %8  : i64
    %30 = llvm.add %28, %29  : i64
    %31 = llvm.urem %30, %7  : i64
    %32 = llvm.sub %30, %31  : i64
    %33 = llvm.inttoptr %32 : i64 to !llvm.ptr
    %34 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %35 = llvm.insertvalue %27, %34[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %36 = llvm.insertvalue %33, %35[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %37 = llvm.insertvalue %6, %36[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %38 = llvm.insertvalue %10, %37[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %9, %38[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.insertvalue %9, %39[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %41 = llvm.insertvalue %8, %40[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = builtin.unrealized_conversion_cast %41 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %126 = memref.load %16[%arg3, %arg4] : memref<128x262144xf32>
        memref.store %126, %42[%arg4, %arg3] : memref<262144x128xf32>
      }
    }
    %43 = llvm.mlir.zero : !llvm.ptr
    %44 = llvm.getelementptr %43[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %45 = llvm.ptrtoint %44 : !llvm.ptr to i64
    %46 = llvm.add %45, %7  : i64
    %47 = llvm.call @malloc(%46) : (i64) -> !llvm.ptr
    %48 = llvm.ptrtoint %47 : !llvm.ptr to i64
    %49 = llvm.sub %7, %8  : i64
    %50 = llvm.add %48, %49  : i64
    %51 = llvm.urem %50, %7  : i64
    %52 = llvm.sub %50, %51  : i64
    %53 = llvm.inttoptr %52 : i64 to !llvm.ptr
    %54 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %55 = llvm.insertvalue %47, %54[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = llvm.insertvalue %53, %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %57 = llvm.insertvalue %6, %56[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %58 = llvm.insertvalue %7, %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %59 = llvm.insertvalue %9, %58[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %60 = llvm.insertvalue %9, %59[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %8, %60[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = builtin.unrealized_conversion_cast %61 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %63 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %64 = llvm.ptrtoint %53 : !llvm.ptr to i64
    %65 = builtin.unrealized_conversion_cast %64 : i64 to index
    %66 = arith.index_cast %65 : index to i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %63, %cst, %67, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %68 = llvm.mlir.zero : !llvm.ptr
    %69 = llvm.getelementptr %68[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %70 = llvm.ptrtoint %69 : !llvm.ptr to i64
    %71 = llvm.add %70, %7  : i64
    %72 = llvm.call @malloc(%71) : (i64) -> !llvm.ptr
    %73 = llvm.ptrtoint %72 : !llvm.ptr to i64
    %74 = llvm.sub %7, %8  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.urem %75, %7  : i64
    %77 = llvm.sub %75, %76  : i64
    %78 = llvm.inttoptr %77 : i64 to !llvm.ptr
    %79 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %80 = llvm.insertvalue %72, %79[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %81 = llvm.insertvalue %78, %80[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %82 = llvm.insertvalue %6, %81[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %83 = llvm.insertvalue %4, %82[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %84 = llvm.insertvalue %5, %83[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %85 = llvm.insertvalue %3, %84[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %86 = llvm.insertvalue %3, %85[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %87 = llvm.insertvalue %1, %86[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %88 = llvm.insertvalue %2, %87[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %89 = llvm.insertvalue %3, %88[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %90 = llvm.insertvalue %8, %89[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %91 = builtin.unrealized_conversion_cast %90 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %92 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = arith.muli %arg3, %c8388608 : index
          %127 = arith.muli %arg4, %c32 : index
          %128 = arith.addi %126, %127 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer to offset: [%128], sizes: [32, 32], strides: [262144, 1] : memref<f32> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %130 = arith.muli %arg3, %c8388608 : index
          %131 = arith.muli %arg4, %c1024 : index
          %132 = arith.addi %130, %131 : index
          %reinterpret_cast_8 = memref.reinterpret_cast %base_buffer_4 to offset: [%132], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %133 = builtin.unrealized_conversion_cast %reinterpret_cast_8 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %134 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = builtin.unrealized_conversion_cast %134 : i64 to index
          %136 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %137 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = arith.index_cast %138 : index to i64
          %140 = llvm.inttoptr %139 : i64 to !llvm.ptr<f32>
          %141 = llvm.extractvalue %133[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %133[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %92, %140, %135, %147, %142) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %93 = llvm.mlir.zero : !llvm.ptr
    %94 = llvm.getelementptr %93[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %95 = llvm.ptrtoint %94 : !llvm.ptr to i64
    %96 = llvm.add %95, %7  : i64
    %97 = llvm.call @malloc(%96) : (i64) -> !llvm.ptr
    %98 = llvm.ptrtoint %97 : !llvm.ptr to i64
    %99 = llvm.sub %7, %8  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.urem %100, %7  : i64
    %102 = llvm.sub %100, %101  : i64
    %103 = llvm.inttoptr %102 : i64 to !llvm.ptr
    %104 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %105 = llvm.insertvalue %97, %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %106 = llvm.insertvalue %103, %105[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %107 = llvm.insertvalue %6, %106[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %108 = llvm.insertvalue %0, %107[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %109 = llvm.insertvalue %5, %108[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %110 = llvm.insertvalue %3, %109[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %111 = llvm.insertvalue %3, %110[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %112 = llvm.insertvalue %1, %111[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %113 = llvm.insertvalue %2, %112[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %3, %113[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %8, %114[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = builtin.unrealized_conversion_cast %115 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %117 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        memref.alloca_scope  {
          %base_buffer_4, %offset_5, %sizes_6:2, %strides_7:2 = memref.extract_strided_metadata %42 : memref<262144x128xf32> -> memref<f32>, index, index, index, index, index
          %126 = arith.muli %arg4, %c4096 : index
          %127 = arith.muli %arg3, %c32 : index
          %128 = arith.addi %126, %127 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%128], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %129 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %130 = arith.muli %arg3, %c8388608 : index
          %131 = arith.muli %arg4, %c1024 : index
          %132 = arith.addi %130, %131 : index
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%132], sizes: [32, 32], strides: [32, 1] : memref<f32> to memref<32x32xf32, strided<[32, 1], offset: ?>>
          %133 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %134 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = builtin.unrealized_conversion_cast %134 : i64 to index
          %136 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %137 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %138 = builtin.unrealized_conversion_cast %137 : i64 to index
          %139 = arith.index_cast %138 : index to i64
          %140 = llvm.inttoptr %139 : i64 to !llvm.ptr<f32>
          %141 = llvm.extractvalue %133[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %133[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %117, %140, %135, %147, %142) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    %base_buffer_0, %offset_1, %sizes_2, %strides_3 = memref.extract_strided_metadata %19 : memref<128xf32> -> memref<f32>, index, index, index
    %118 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %120 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        memref.alloca_scope  {
          %126 = llvm.mlir.zero : !llvm.ptr
          %127 = llvm.getelementptr %126[1024] : (!llvm.ptr) -> !llvm.ptr, f32
          %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
          %129 = llvm.add %128, %7  : i64
          %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
          %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
          %132 = llvm.sub %7, %8  : i64
          %133 = llvm.add %131, %132  : i64
          %134 = llvm.urem %133, %7  : i64
          %135 = llvm.sub %133, %134  : i64
          %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
          %base_buffer_4, %offset_5, %sizes_6:4, %strides_7:4 = memref.extract_strided_metadata %91 : memref<2x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %137 = arith.muli %arg3, %c8388608 : index
          %reinterpret_cast = memref.reinterpret_cast %base_buffer_4 to offset: [%137], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %138 = builtin.unrealized_conversion_cast %reinterpret_cast : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %base_buffer_8, %offset_9, %sizes_10:4, %strides_11:4 = memref.extract_strided_metadata %116 : memref<4x8192x32x32xf32> -> memref<f32>, index, index, index, index, index, index, index, index, index
          %139 = arith.muli %arg4, %c8388608 : index
          %reinterpret_cast_12 = memref.reinterpret_cast %base_buffer_8 to offset: [%139], sizes: [8192, 32, 32], strides: [1024, 32, 1] : memref<f32> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
          %140 = builtin.unrealized_conversion_cast %reinterpret_cast_12 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
          %141 = llvm.extractvalue %138[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %142 = builtin.unrealized_conversion_cast %141 : i64 to index
          %143 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %144 = llvm.ptrtoint %143 : !llvm.ptr to i64
          %145 = builtin.unrealized_conversion_cast %144 : i64 to index
          %146 = arith.index_cast %145 : index to i64
          %147 = llvm.inttoptr %146 : i64 to !llvm.ptr<f32>
          %148 = llvm.extractvalue %140[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %149 = builtin.unrealized_conversion_cast %148 : i64 to index
          %150 = llvm.extractvalue %140[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
          %151 = llvm.ptrtoint %150 : !llvm.ptr to i64
          %152 = builtin.unrealized_conversion_cast %151 : i64 to index
          %153 = arith.index_cast %152 : index to i64
          %154 = llvm.inttoptr %153 : i64 to !llvm.ptr<f32>
          %155 = llvm.ptrtoint %136 : !llvm.ptr to i64
          %156 = builtin.unrealized_conversion_cast %155 : i64 to index
          %157 = arith.index_cast %156 : index to i64
          %158 = llvm.inttoptr %157 : i64 to !llvm.ptr<f32>
          func.call @xsmm_brgemm_invoke(%c1_i64, %118, %147, %142, %154, %149, %158, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
          %159 = arith.muli %arg4, %c32 : index
          %reinterpret_cast_13 = memref.reinterpret_cast %base_buffer_0 to offset: [%159], sizes: [32], strides: [1] : memref<f32> to memref<32xf32, strided<[1], offset: ?>>
          %160 = builtin.unrealized_conversion_cast %reinterpret_cast_13 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
          %161 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %162 = builtin.unrealized_conversion_cast %161 : i64 to index
          %163 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
          %164 = llvm.ptrtoint %163 : !llvm.ptr to i64
          %165 = builtin.unrealized_conversion_cast %164 : i64 to index
          %166 = arith.index_cast %165 : index to i64
          %167 = llvm.inttoptr %166 : i64 to !llvm.ptr<f32>
          func.call @xsmm_binary_invoke(%c1_i64, %119, %167, %162, %158, %c0, %158, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          %base_buffer_14, %offset_15, %sizes_16:2, %strides_17:2 = memref.extract_strided_metadata %62 : memref<64x128xf32> -> memref<f32>, index, index, index, index, index
          %168 = arith.muli %arg3, %c4096 : index
          %169 = arith.muli %arg4, %c32 : index
          %170 = arith.addi %168, %169 : index
          %reinterpret_cast_18 = memref.reinterpret_cast %base_buffer_14 to offset: [%170], sizes: [32, 32], strides: [128, 1] : memref<f32> to memref<32x32xf32, strided<[128, 1], offset: ?>>
          %171 = builtin.unrealized_conversion_cast %reinterpret_cast_18 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
          %172 = llvm.extractvalue %171[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %173 = builtin.unrealized_conversion_cast %172 : i64 to index
          %174 = llvm.extractvalue %171[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
          %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
          %176 = builtin.unrealized_conversion_cast %175 : i64 to index
          %177 = arith.index_cast %176 : index to i64
          %178 = llvm.inttoptr %177 : i64 to !llvm.ptr<f32>
          func.call @xsmm_unary_invoke(%c1_i64, %120, %158, %c0, %178, %173) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
          llvm.call @free(%130) : (!llvm.ptr) -> ()
        }
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%27) : (!llvm.ptr) -> ()
    llvm.call @free(%72) : (!llvm.ptr) -> ()
    llvm.call @free(%97) : (!llvm.ptr) -> ()
    %121 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %61, %121 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %122 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %123 = llvm.insertvalue %4, %122[0] : !llvm.struct<(i64, ptr)> 
    %124 = llvm.insertvalue %121, %123[1] : !llvm.struct<(i64, ptr)> 
    %125 = builtin.unrealized_conversion_cast %124 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%125) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After FinalizeMemRefToLLVMConversionPass (finalize-memref-to-llvm) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c4096 = arith.constant 4096 : index
    %c1024 = arith.constant 1024 : index
    %c32 = arith.constant 32 : index
    %c8388608 = arith.constant 8388608 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %23 = llvm.extractvalue %21[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %24 = llvm.extractvalue %21[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %25 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %26 = llvm.insertvalue %23, %25[0] : !llvm.struct<(ptr, ptr, i64)> 
    %27 = llvm.insertvalue %24, %26[1] : !llvm.struct<(ptr, ptr, i64)> 
    %28 = llvm.mlir.constant(0 : index) : i64
    %29 = llvm.insertvalue %28, %27[2] : !llvm.struct<(ptr, ptr, i64)> 
    %30 = llvm.extractvalue %21[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %31 = llvm.extractvalue %21[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %32 = llvm.extractvalue %21[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %33 = llvm.extractvalue %21[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %34 = llvm.extractvalue %21[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %35 = llvm.extractvalue %21[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %36 = llvm.extractvalue %21[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %37 = llvm.mlir.zero : !llvm.ptr
    %38 = llvm.getelementptr %37[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %39 = llvm.ptrtoint %38 : !llvm.ptr to i64
    %40 = llvm.add %39, %7  : i64
    %41 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %42 = llvm.ptrtoint %41 : !llvm.ptr to i64
    %43 = llvm.sub %7, %8  : i64
    %44 = llvm.add %42, %43  : i64
    %45 = llvm.urem %44, %7  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %49 = llvm.insertvalue %41, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %47, %49[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %6, %50[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %10, %51[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %9, %52[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %9, %53[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = llvm.insertvalue %8, %54[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = builtin.unrealized_conversion_cast %55 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      %150 = builtin.unrealized_conversion_cast %arg3 : index to i64
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %151 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %152 = llvm.extractvalue %15[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %153 = llvm.mlir.constant(262144 : index) : i64
        %154 = llvm.mul %150, %153  : i64
        %155 = llvm.add %154, %151  : i64
        %156 = llvm.getelementptr %152[%155] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %157 = llvm.load %156 : !llvm.ptr -> f32
        %158 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %159 = llvm.mlir.constant(128 : index) : i64
        %160 = llvm.mul %151, %159  : i64
        %161 = llvm.add %160, %150  : i64
        %162 = llvm.getelementptr %158[%161] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %157, %162 : f32, !llvm.ptr
      }
    }
    %57 = llvm.mlir.zero : !llvm.ptr
    %58 = llvm.getelementptr %57[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %59 = llvm.ptrtoint %58 : !llvm.ptr to i64
    %60 = llvm.add %59, %7  : i64
    %61 = llvm.call @malloc(%60) : (i64) -> !llvm.ptr
    %62 = llvm.ptrtoint %61 : !llvm.ptr to i64
    %63 = llvm.sub %7, %8  : i64
    %64 = llvm.add %62, %63  : i64
    %65 = llvm.urem %64, %7  : i64
    %66 = llvm.sub %64, %65  : i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr
    %68 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %69 = llvm.insertvalue %61, %68[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.insertvalue %67, %69[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %6, %70[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = llvm.insertvalue %7, %71[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %73 = llvm.insertvalue %9, %72[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %74 = llvm.insertvalue %9, %73[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %75 = llvm.insertvalue %8, %74[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %76 = builtin.unrealized_conversion_cast %75 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %77 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %78 = llvm.ptrtoint %67 : !llvm.ptr to i64
    %79 = builtin.unrealized_conversion_cast %78 : i64 to index
    %80 = arith.index_cast %79 : index to i64
    %81 = llvm.inttoptr %80 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %77, %cst, %81, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %82 = llvm.mlir.zero : !llvm.ptr
    %83 = llvm.getelementptr %82[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %84 = llvm.ptrtoint %83 : !llvm.ptr to i64
    %85 = llvm.add %84, %7  : i64
    %86 = llvm.call @malloc(%85) : (i64) -> !llvm.ptr
    %87 = llvm.ptrtoint %86 : !llvm.ptr to i64
    %88 = llvm.sub %7, %8  : i64
    %89 = llvm.add %87, %88  : i64
    %90 = llvm.urem %89, %7  : i64
    %91 = llvm.sub %89, %90  : i64
    %92 = llvm.inttoptr %91 : i64 to !llvm.ptr
    %93 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %94 = llvm.insertvalue %86, %93[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %95 = llvm.insertvalue %92, %94[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %96 = llvm.insertvalue %6, %95[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %97 = llvm.insertvalue %4, %96[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %98 = llvm.insertvalue %5, %97[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %99 = llvm.insertvalue %3, %98[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %100 = llvm.insertvalue %3, %99[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %101 = llvm.insertvalue %1, %100[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %102 = llvm.insertvalue %2, %101[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %103 = llvm.insertvalue %3, %102[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %104 = llvm.insertvalue %8, %103[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %105 = builtin.unrealized_conversion_cast %104 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %106 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        %150 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %151 = arith.muli %arg3, %c8388608 : index
        %152 = arith.muli %arg4, %c32 : index
        %153 = arith.addi %151, %152 : index
        %154 = builtin.unrealized_conversion_cast %153 : index to i64
        %155 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %156 = llvm.extractvalue %29[0] : !llvm.struct<(ptr, ptr, i64)> 
        %157 = llvm.extractvalue %29[1] : !llvm.struct<(ptr, ptr, i64)> 
        %158 = llvm.insertvalue %156, %155[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %159 = llvm.insertvalue %157, %158[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %160 = llvm.insertvalue %154, %159[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %161 = llvm.mlir.constant(32 : index) : i64
        %162 = llvm.insertvalue %161, %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %163 = llvm.mlir.constant(262144 : index) : i64
        %164 = llvm.insertvalue %163, %162[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %165 = llvm.mlir.constant(32 : index) : i64
        %166 = llvm.insertvalue %165, %164[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %167 = llvm.mlir.constant(1 : index) : i64
        %168 = llvm.insertvalue %167, %166[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %169 = builtin.unrealized_conversion_cast %168 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
        %170 = builtin.unrealized_conversion_cast %169 : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %171 = llvm.extractvalue %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %172 = llvm.extractvalue %104[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %173 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %174 = llvm.insertvalue %171, %173[0] : !llvm.struct<(ptr, ptr, i64)> 
        %175 = llvm.insertvalue %172, %174[1] : !llvm.struct<(ptr, ptr, i64)> 
        %176 = llvm.mlir.constant(0 : index) : i64
        %177 = llvm.insertvalue %176, %175[2] : !llvm.struct<(ptr, ptr, i64)> 
        %178 = llvm.extractvalue %104[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %179 = llvm.extractvalue %104[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %180 = llvm.extractvalue %104[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %181 = llvm.extractvalue %104[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %182 = llvm.extractvalue %104[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %183 = llvm.extractvalue %104[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %184 = llvm.extractvalue %104[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %185 = llvm.extractvalue %104[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %186 = llvm.extractvalue %104[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %187 = arith.muli %arg3, %c8388608 : index
        %188 = arith.muli %arg4, %c1024 : index
        %189 = arith.addi %187, %188 : index
        %190 = builtin.unrealized_conversion_cast %189 : index to i64
        %191 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %192 = llvm.extractvalue %177[0] : !llvm.struct<(ptr, ptr, i64)> 
        %193 = llvm.extractvalue %177[1] : !llvm.struct<(ptr, ptr, i64)> 
        %194 = llvm.insertvalue %192, %191[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %195 = llvm.insertvalue %193, %194[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %196 = llvm.insertvalue %190, %195[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %197 = llvm.mlir.constant(32 : index) : i64
        %198 = llvm.insertvalue %197, %196[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %199 = llvm.mlir.constant(32 : index) : i64
        %200 = llvm.insertvalue %199, %198[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %201 = llvm.mlir.constant(32 : index) : i64
        %202 = llvm.insertvalue %201, %200[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %203 = llvm.mlir.constant(1 : index) : i64
        %204 = llvm.insertvalue %203, %202[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %205 = builtin.unrealized_conversion_cast %204 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %206 = builtin.unrealized_conversion_cast %205 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %207 = llvm.extractvalue %170[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %208 = builtin.unrealized_conversion_cast %207 : i64 to index
        %209 = llvm.extractvalue %170[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %210 = llvm.ptrtoint %209 : !llvm.ptr to i64
        %211 = builtin.unrealized_conversion_cast %210 : i64 to index
        %212 = arith.index_cast %211 : index to i64
        %213 = llvm.inttoptr %212 : i64 to !llvm.ptr<f32>
        %214 = llvm.extractvalue %206[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %215 = builtin.unrealized_conversion_cast %214 : i64 to index
        %216 = llvm.extractvalue %206[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = llvm.ptrtoint %216 : !llvm.ptr to i64
        %218 = builtin.unrealized_conversion_cast %217 : i64 to index
        %219 = arith.index_cast %218 : index to i64
        %220 = llvm.inttoptr %219 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %106, %213, %208, %220, %215) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.intr.stackrestore %150 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %107 = llvm.mlir.zero : !llvm.ptr
    %108 = llvm.getelementptr %107[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %109 = llvm.ptrtoint %108 : !llvm.ptr to i64
    %110 = llvm.add %109, %7  : i64
    %111 = llvm.call @malloc(%110) : (i64) -> !llvm.ptr
    %112 = llvm.ptrtoint %111 : !llvm.ptr to i64
    %113 = llvm.sub %7, %8  : i64
    %114 = llvm.add %112, %113  : i64
    %115 = llvm.urem %114, %7  : i64
    %116 = llvm.sub %114, %115  : i64
    %117 = llvm.inttoptr %116 : i64 to !llvm.ptr
    %118 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %119 = llvm.insertvalue %111, %118[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %120 = llvm.insertvalue %117, %119[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %121 = llvm.insertvalue %6, %120[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %122 = llvm.insertvalue %0, %121[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %123 = llvm.insertvalue %5, %122[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %124 = llvm.insertvalue %3, %123[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %125 = llvm.insertvalue %3, %124[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %126 = llvm.insertvalue %1, %125[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %127 = llvm.insertvalue %2, %126[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %128 = llvm.insertvalue %3, %127[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %129 = llvm.insertvalue %8, %128[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %130 = builtin.unrealized_conversion_cast %129 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %131 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        %150 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %151 = llvm.extractvalue %55[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %152 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %153 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %154 = llvm.insertvalue %151, %153[0] : !llvm.struct<(ptr, ptr, i64)> 
        %155 = llvm.insertvalue %152, %154[1] : !llvm.struct<(ptr, ptr, i64)> 
        %156 = llvm.mlir.constant(0 : index) : i64
        %157 = llvm.insertvalue %156, %155[2] : !llvm.struct<(ptr, ptr, i64)> 
        %158 = llvm.extractvalue %55[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %159 = llvm.extractvalue %55[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %160 = llvm.extractvalue %55[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %161 = llvm.extractvalue %55[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %162 = llvm.extractvalue %55[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %163 = arith.muli %arg4, %c4096 : index
        %164 = arith.muli %arg3, %c32 : index
        %165 = arith.addi %163, %164 : index
        %166 = builtin.unrealized_conversion_cast %165 : index to i64
        %167 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %168 = llvm.extractvalue %157[0] : !llvm.struct<(ptr, ptr, i64)> 
        %169 = llvm.extractvalue %157[1] : !llvm.struct<(ptr, ptr, i64)> 
        %170 = llvm.insertvalue %168, %167[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %171 = llvm.insertvalue %169, %170[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %172 = llvm.insertvalue %166, %171[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %173 = llvm.mlir.constant(32 : index) : i64
        %174 = llvm.insertvalue %173, %172[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %175 = llvm.mlir.constant(128 : index) : i64
        %176 = llvm.insertvalue %175, %174[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %177 = llvm.mlir.constant(32 : index) : i64
        %178 = llvm.insertvalue %177, %176[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %179 = llvm.mlir.constant(1 : index) : i64
        %180 = llvm.insertvalue %179, %178[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %181 = builtin.unrealized_conversion_cast %180 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %182 = builtin.unrealized_conversion_cast %181 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %183 = llvm.extractvalue %129[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %184 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %185 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %186 = llvm.insertvalue %183, %185[0] : !llvm.struct<(ptr, ptr, i64)> 
        %187 = llvm.insertvalue %184, %186[1] : !llvm.struct<(ptr, ptr, i64)> 
        %188 = llvm.mlir.constant(0 : index) : i64
        %189 = llvm.insertvalue %188, %187[2] : !llvm.struct<(ptr, ptr, i64)> 
        %190 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %191 = llvm.extractvalue %129[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %192 = llvm.extractvalue %129[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %193 = llvm.extractvalue %129[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %194 = llvm.extractvalue %129[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %195 = llvm.extractvalue %129[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %196 = llvm.extractvalue %129[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %197 = llvm.extractvalue %129[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %198 = llvm.extractvalue %129[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %199 = arith.muli %arg3, %c8388608 : index
        %200 = arith.muli %arg4, %c1024 : index
        %201 = arith.addi %199, %200 : index
        %202 = builtin.unrealized_conversion_cast %201 : index to i64
        %203 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %204 = llvm.extractvalue %189[0] : !llvm.struct<(ptr, ptr, i64)> 
        %205 = llvm.extractvalue %189[1] : !llvm.struct<(ptr, ptr, i64)> 
        %206 = llvm.insertvalue %204, %203[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %207 = llvm.insertvalue %205, %206[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %208 = llvm.insertvalue %202, %207[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %209 = llvm.mlir.constant(32 : index) : i64
        %210 = llvm.insertvalue %209, %208[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %211 = llvm.mlir.constant(32 : index) : i64
        %212 = llvm.insertvalue %211, %210[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %213 = llvm.mlir.constant(32 : index) : i64
        %214 = llvm.insertvalue %213, %212[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %215 = llvm.mlir.constant(1 : index) : i64
        %216 = llvm.insertvalue %215, %214[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = builtin.unrealized_conversion_cast %216 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %218 = builtin.unrealized_conversion_cast %217 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %219 = llvm.extractvalue %182[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = builtin.unrealized_conversion_cast %219 : i64 to index
        %221 = llvm.extractvalue %182[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.ptrtoint %221 : !llvm.ptr to i64
        %223 = builtin.unrealized_conversion_cast %222 : i64 to index
        %224 = arith.index_cast %223 : index to i64
        %225 = llvm.inttoptr %224 : i64 to !llvm.ptr<f32>
        %226 = llvm.extractvalue %218[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = builtin.unrealized_conversion_cast %226 : i64 to index
        %228 = llvm.extractvalue %218[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %229 = llvm.ptrtoint %228 : !llvm.ptr to i64
        %230 = builtin.unrealized_conversion_cast %229 : i64 to index
        %231 = arith.index_cast %230 : index to i64
        %232 = llvm.inttoptr %231 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %131, %225, %220, %232, %227) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.intr.stackrestore %150 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %132 = llvm.extractvalue %18[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %133 = llvm.extractvalue %18[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %134 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %135 = llvm.insertvalue %132, %134[0] : !llvm.struct<(ptr, ptr, i64)> 
    %136 = llvm.insertvalue %133, %135[1] : !llvm.struct<(ptr, ptr, i64)> 
    %137 = llvm.mlir.constant(0 : index) : i64
    %138 = llvm.insertvalue %137, %136[2] : !llvm.struct<(ptr, ptr, i64)> 
    %139 = llvm.extractvalue %18[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %140 = llvm.extractvalue %18[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %141 = llvm.extractvalue %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %143 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %144 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        %150 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %151 = llvm.mlir.zero : !llvm.ptr
        %152 = llvm.getelementptr %151[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %153 = llvm.ptrtoint %152 : !llvm.ptr to i64
        %154 = llvm.add %153, %7  : i64
        %155 = llvm.call @malloc(%154) : (i64) -> !llvm.ptr
        %156 = llvm.ptrtoint %155 : !llvm.ptr to i64
        %157 = llvm.sub %7, %8  : i64
        %158 = llvm.add %156, %157  : i64
        %159 = llvm.urem %158, %7  : i64
        %160 = llvm.sub %158, %159  : i64
        %161 = llvm.inttoptr %160 : i64 to !llvm.ptr
        %162 = llvm.extractvalue %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %163 = llvm.extractvalue %104[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %164 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %165 = llvm.insertvalue %162, %164[0] : !llvm.struct<(ptr, ptr, i64)> 
        %166 = llvm.insertvalue %163, %165[1] : !llvm.struct<(ptr, ptr, i64)> 
        %167 = llvm.mlir.constant(0 : index) : i64
        %168 = llvm.insertvalue %167, %166[2] : !llvm.struct<(ptr, ptr, i64)> 
        %169 = llvm.extractvalue %104[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %170 = llvm.extractvalue %104[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %171 = llvm.extractvalue %104[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %172 = llvm.extractvalue %104[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %173 = llvm.extractvalue %104[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %174 = llvm.extractvalue %104[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %175 = llvm.extractvalue %104[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %176 = llvm.extractvalue %104[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %177 = llvm.extractvalue %104[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %178 = arith.muli %arg3, %c8388608 : index
        %179 = builtin.unrealized_conversion_cast %178 : index to i64
        %180 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %181 = llvm.extractvalue %168[0] : !llvm.struct<(ptr, ptr, i64)> 
        %182 = llvm.extractvalue %168[1] : !llvm.struct<(ptr, ptr, i64)> 
        %183 = llvm.insertvalue %181, %180[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %184 = llvm.insertvalue %182, %183[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %185 = llvm.insertvalue %179, %184[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %186 = llvm.mlir.constant(8192 : index) : i64
        %187 = llvm.insertvalue %186, %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %188 = llvm.mlir.constant(1024 : index) : i64
        %189 = llvm.insertvalue %188, %187[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %190 = llvm.mlir.constant(32 : index) : i64
        %191 = llvm.insertvalue %190, %189[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %192 = llvm.mlir.constant(32 : index) : i64
        %193 = llvm.insertvalue %192, %191[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %194 = llvm.mlir.constant(32 : index) : i64
        %195 = llvm.insertvalue %194, %193[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %196 = llvm.mlir.constant(1 : index) : i64
        %197 = llvm.insertvalue %196, %195[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %198 = builtin.unrealized_conversion_cast %197 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %199 = builtin.unrealized_conversion_cast %198 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %200 = llvm.extractvalue %129[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %201 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %202 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %203 = llvm.insertvalue %200, %202[0] : !llvm.struct<(ptr, ptr, i64)> 
        %204 = llvm.insertvalue %201, %203[1] : !llvm.struct<(ptr, ptr, i64)> 
        %205 = llvm.mlir.constant(0 : index) : i64
        %206 = llvm.insertvalue %205, %204[2] : !llvm.struct<(ptr, ptr, i64)> 
        %207 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %208 = llvm.extractvalue %129[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %209 = llvm.extractvalue %129[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %210 = llvm.extractvalue %129[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %211 = llvm.extractvalue %129[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %212 = llvm.extractvalue %129[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %213 = llvm.extractvalue %129[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %214 = llvm.extractvalue %129[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %215 = llvm.extractvalue %129[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %216 = arith.muli %arg4, %c8388608 : index
        %217 = builtin.unrealized_conversion_cast %216 : index to i64
        %218 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %219 = llvm.extractvalue %206[0] : !llvm.struct<(ptr, ptr, i64)> 
        %220 = llvm.extractvalue %206[1] : !llvm.struct<(ptr, ptr, i64)> 
        %221 = llvm.insertvalue %219, %218[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %222 = llvm.insertvalue %220, %221[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %223 = llvm.insertvalue %217, %222[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %224 = llvm.mlir.constant(8192 : index) : i64
        %225 = llvm.insertvalue %224, %223[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %226 = llvm.mlir.constant(1024 : index) : i64
        %227 = llvm.insertvalue %226, %225[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %228 = llvm.mlir.constant(32 : index) : i64
        %229 = llvm.insertvalue %228, %227[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %230 = llvm.mlir.constant(32 : index) : i64
        %231 = llvm.insertvalue %230, %229[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %232 = llvm.mlir.constant(32 : index) : i64
        %233 = llvm.insertvalue %232, %231[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %234 = llvm.mlir.constant(1 : index) : i64
        %235 = llvm.insertvalue %234, %233[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %236 = builtin.unrealized_conversion_cast %235 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %237 = builtin.unrealized_conversion_cast %236 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %238 = llvm.extractvalue %199[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %239 = builtin.unrealized_conversion_cast %238 : i64 to index
        %240 = llvm.extractvalue %199[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %241 = llvm.ptrtoint %240 : !llvm.ptr to i64
        %242 = builtin.unrealized_conversion_cast %241 : i64 to index
        %243 = arith.index_cast %242 : index to i64
        %244 = llvm.inttoptr %243 : i64 to !llvm.ptr<f32>
        %245 = llvm.extractvalue %237[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %246 = builtin.unrealized_conversion_cast %245 : i64 to index
        %247 = llvm.extractvalue %237[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %248 = llvm.ptrtoint %247 : !llvm.ptr to i64
        %249 = builtin.unrealized_conversion_cast %248 : i64 to index
        %250 = arith.index_cast %249 : index to i64
        %251 = llvm.inttoptr %250 : i64 to !llvm.ptr<f32>
        %252 = llvm.ptrtoint %161 : !llvm.ptr to i64
        %253 = builtin.unrealized_conversion_cast %252 : i64 to index
        %254 = arith.index_cast %253 : index to i64
        %255 = llvm.inttoptr %254 : i64 to !llvm.ptr<f32>
        func.call @xsmm_brgemm_invoke(%c1_i64, %142, %244, %239, %251, %246, %255, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
        %256 = arith.muli %arg4, %c32 : index
        %257 = builtin.unrealized_conversion_cast %256 : index to i64
        %258 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %259 = llvm.extractvalue %138[0] : !llvm.struct<(ptr, ptr, i64)> 
        %260 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64)> 
        %261 = llvm.insertvalue %259, %258[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %262 = llvm.insertvalue %260, %261[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %263 = llvm.insertvalue %257, %262[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %264 = llvm.mlir.constant(32 : index) : i64
        %265 = llvm.insertvalue %264, %263[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %266 = llvm.mlir.constant(1 : index) : i64
        %267 = llvm.insertvalue %266, %265[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %268 = builtin.unrealized_conversion_cast %267 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<32xf32, strided<[1], offset: ?>>
        %269 = builtin.unrealized_conversion_cast %268 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %270 = llvm.extractvalue %269[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %271 = builtin.unrealized_conversion_cast %270 : i64 to index
        %272 = llvm.extractvalue %269[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %273 = llvm.ptrtoint %272 : !llvm.ptr to i64
        %274 = builtin.unrealized_conversion_cast %273 : i64 to index
        %275 = arith.index_cast %274 : index to i64
        %276 = llvm.inttoptr %275 : i64 to !llvm.ptr<f32>
        func.call @xsmm_binary_invoke(%c1_i64, %143, %276, %271, %255, %c0, %255, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        %277 = llvm.extractvalue %75[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %278 = llvm.extractvalue %75[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %279 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %280 = llvm.insertvalue %277, %279[0] : !llvm.struct<(ptr, ptr, i64)> 
        %281 = llvm.insertvalue %278, %280[1] : !llvm.struct<(ptr, ptr, i64)> 
        %282 = llvm.mlir.constant(0 : index) : i64
        %283 = llvm.insertvalue %282, %281[2] : !llvm.struct<(ptr, ptr, i64)> 
        %284 = llvm.extractvalue %75[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %285 = llvm.extractvalue %75[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %286 = llvm.extractvalue %75[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %287 = llvm.extractvalue %75[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %288 = llvm.extractvalue %75[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %289 = arith.muli %arg3, %c4096 : index
        %290 = arith.muli %arg4, %c32 : index
        %291 = arith.addi %289, %290 : index
        %292 = builtin.unrealized_conversion_cast %291 : index to i64
        %293 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %294 = llvm.extractvalue %283[0] : !llvm.struct<(ptr, ptr, i64)> 
        %295 = llvm.extractvalue %283[1] : !llvm.struct<(ptr, ptr, i64)> 
        %296 = llvm.insertvalue %294, %293[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %297 = llvm.insertvalue %295, %296[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %298 = llvm.insertvalue %292, %297[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %299 = llvm.mlir.constant(32 : index) : i64
        %300 = llvm.insertvalue %299, %298[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %301 = llvm.mlir.constant(128 : index) : i64
        %302 = llvm.insertvalue %301, %300[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %303 = llvm.mlir.constant(32 : index) : i64
        %304 = llvm.insertvalue %303, %302[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %305 = llvm.mlir.constant(1 : index) : i64
        %306 = llvm.insertvalue %305, %304[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %307 = builtin.unrealized_conversion_cast %306 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %308 = builtin.unrealized_conversion_cast %307 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %309 = llvm.extractvalue %308[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %310 = builtin.unrealized_conversion_cast %309 : i64 to index
        %311 = llvm.extractvalue %308[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %312 = llvm.ptrtoint %311 : !llvm.ptr to i64
        %313 = builtin.unrealized_conversion_cast %312 : i64 to index
        %314 = arith.index_cast %313 : index to i64
        %315 = llvm.inttoptr %314 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %144, %255, %c0, %315, %310) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.call @free(%155) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %150 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%41) : (!llvm.ptr) -> ()
    llvm.call @free(%86) : (!llvm.ptr) -> ()
    llvm.call @free(%111) : (!llvm.ptr) -> ()
    %145 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %75, %145 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %146 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %147 = llvm.insertvalue %4, %146[0] : !llvm.struct<(i64, ptr)> 
    %148 = llvm.insertvalue %145, %147[1] : !llvm.struct<(i64, ptr)> 
    %149 = builtin.unrealized_conversion_cast %148 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%149) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before SCFToControlFlow (convert-scf-to-cf) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c4096 = arith.constant 4096 : index
    %c1024 = arith.constant 1024 : index
    %c32 = arith.constant 32 : index
    %c8388608 = arith.constant 8388608 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %23 = llvm.extractvalue %21[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %24 = llvm.extractvalue %21[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %25 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %26 = llvm.insertvalue %23, %25[0] : !llvm.struct<(ptr, ptr, i64)> 
    %27 = llvm.insertvalue %24, %26[1] : !llvm.struct<(ptr, ptr, i64)> 
    %28 = llvm.mlir.constant(0 : index) : i64
    %29 = llvm.insertvalue %28, %27[2] : !llvm.struct<(ptr, ptr, i64)> 
    %30 = llvm.extractvalue %21[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %31 = llvm.extractvalue %21[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %32 = llvm.extractvalue %21[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %33 = llvm.extractvalue %21[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %34 = llvm.extractvalue %21[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %35 = llvm.extractvalue %21[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %36 = llvm.extractvalue %21[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %37 = llvm.mlir.zero : !llvm.ptr
    %38 = llvm.getelementptr %37[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %39 = llvm.ptrtoint %38 : !llvm.ptr to i64
    %40 = llvm.add %39, %7  : i64
    %41 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %42 = llvm.ptrtoint %41 : !llvm.ptr to i64
    %43 = llvm.sub %7, %8  : i64
    %44 = llvm.add %42, %43  : i64
    %45 = llvm.urem %44, %7  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %49 = llvm.insertvalue %41, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %47, %49[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %6, %50[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %10, %51[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %9, %52[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %9, %53[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = llvm.insertvalue %8, %54[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = builtin.unrealized_conversion_cast %55 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    scf.for %arg3 = %c0 to %c128 step %c1 {
      %150 = builtin.unrealized_conversion_cast %arg3 : index to i64
      scf.for %arg4 = %c0 to %c262144 step %c1 {
        %151 = builtin.unrealized_conversion_cast %arg4 : index to i64
        %152 = llvm.extractvalue %15[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %153 = llvm.mlir.constant(262144 : index) : i64
        %154 = llvm.mul %150, %153  : i64
        %155 = llvm.add %154, %151  : i64
        %156 = llvm.getelementptr %152[%155] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %157 = llvm.load %156 : !llvm.ptr -> f32
        %158 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %159 = llvm.mlir.constant(128 : index) : i64
        %160 = llvm.mul %151, %159  : i64
        %161 = llvm.add %160, %150  : i64
        %162 = llvm.getelementptr %158[%161] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        llvm.store %157, %162 : f32, !llvm.ptr
      }
    }
    %57 = llvm.mlir.zero : !llvm.ptr
    %58 = llvm.getelementptr %57[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %59 = llvm.ptrtoint %58 : !llvm.ptr to i64
    %60 = llvm.add %59, %7  : i64
    %61 = llvm.call @malloc(%60) : (i64) -> !llvm.ptr
    %62 = llvm.ptrtoint %61 : !llvm.ptr to i64
    %63 = llvm.sub %7, %8  : i64
    %64 = llvm.add %62, %63  : i64
    %65 = llvm.urem %64, %7  : i64
    %66 = llvm.sub %64, %65  : i64
    %67 = llvm.inttoptr %66 : i64 to !llvm.ptr
    %68 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %69 = llvm.insertvalue %61, %68[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.insertvalue %67, %69[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %71 = llvm.insertvalue %6, %70[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = llvm.insertvalue %7, %71[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %73 = llvm.insertvalue %9, %72[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %74 = llvm.insertvalue %9, %73[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %75 = llvm.insertvalue %8, %74[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %76 = builtin.unrealized_conversion_cast %75 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %77 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %78 = llvm.ptrtoint %67 : !llvm.ptr to i64
    %79 = builtin.unrealized_conversion_cast %78 : i64 to index
    %80 = arith.index_cast %79 : index to i64
    %81 = llvm.inttoptr %80 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %77, %cst, %81, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %82 = llvm.mlir.zero : !llvm.ptr
    %83 = llvm.getelementptr %82[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %84 = llvm.ptrtoint %83 : !llvm.ptr to i64
    %85 = llvm.add %84, %7  : i64
    %86 = llvm.call @malloc(%85) : (i64) -> !llvm.ptr
    %87 = llvm.ptrtoint %86 : !llvm.ptr to i64
    %88 = llvm.sub %7, %8  : i64
    %89 = llvm.add %87, %88  : i64
    %90 = llvm.urem %89, %7  : i64
    %91 = llvm.sub %89, %90  : i64
    %92 = llvm.inttoptr %91 : i64 to !llvm.ptr
    %93 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %94 = llvm.insertvalue %86, %93[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %95 = llvm.insertvalue %92, %94[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %96 = llvm.insertvalue %6, %95[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %97 = llvm.insertvalue %4, %96[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %98 = llvm.insertvalue %5, %97[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %99 = llvm.insertvalue %3, %98[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %100 = llvm.insertvalue %3, %99[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %101 = llvm.insertvalue %1, %100[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %102 = llvm.insertvalue %2, %101[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %103 = llvm.insertvalue %3, %102[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %104 = llvm.insertvalue %8, %103[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %105 = builtin.unrealized_conversion_cast %104 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %106 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        %150 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %151 = arith.muli %arg3, %c8388608 : index
        %152 = arith.muli %arg4, %c32 : index
        %153 = arith.addi %151, %152 : index
        %154 = builtin.unrealized_conversion_cast %153 : index to i64
        %155 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %156 = llvm.extractvalue %29[0] : !llvm.struct<(ptr, ptr, i64)> 
        %157 = llvm.extractvalue %29[1] : !llvm.struct<(ptr, ptr, i64)> 
        %158 = llvm.insertvalue %156, %155[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %159 = llvm.insertvalue %157, %158[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %160 = llvm.insertvalue %154, %159[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %161 = llvm.mlir.constant(32 : index) : i64
        %162 = llvm.insertvalue %161, %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %163 = llvm.mlir.constant(262144 : index) : i64
        %164 = llvm.insertvalue %163, %162[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %165 = llvm.mlir.constant(32 : index) : i64
        %166 = llvm.insertvalue %165, %164[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %167 = llvm.mlir.constant(1 : index) : i64
        %168 = llvm.insertvalue %167, %166[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %169 = builtin.unrealized_conversion_cast %168 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
        %170 = builtin.unrealized_conversion_cast %169 : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %171 = llvm.extractvalue %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %172 = llvm.extractvalue %104[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %173 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %174 = llvm.insertvalue %171, %173[0] : !llvm.struct<(ptr, ptr, i64)> 
        %175 = llvm.insertvalue %172, %174[1] : !llvm.struct<(ptr, ptr, i64)> 
        %176 = llvm.mlir.constant(0 : index) : i64
        %177 = llvm.insertvalue %176, %175[2] : !llvm.struct<(ptr, ptr, i64)> 
        %178 = llvm.extractvalue %104[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %179 = llvm.extractvalue %104[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %180 = llvm.extractvalue %104[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %181 = llvm.extractvalue %104[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %182 = llvm.extractvalue %104[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %183 = llvm.extractvalue %104[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %184 = llvm.extractvalue %104[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %185 = llvm.extractvalue %104[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %186 = llvm.extractvalue %104[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %187 = arith.muli %arg3, %c8388608 : index
        %188 = arith.muli %arg4, %c1024 : index
        %189 = arith.addi %187, %188 : index
        %190 = builtin.unrealized_conversion_cast %189 : index to i64
        %191 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %192 = llvm.extractvalue %177[0] : !llvm.struct<(ptr, ptr, i64)> 
        %193 = llvm.extractvalue %177[1] : !llvm.struct<(ptr, ptr, i64)> 
        %194 = llvm.insertvalue %192, %191[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %195 = llvm.insertvalue %193, %194[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %196 = llvm.insertvalue %190, %195[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %197 = llvm.mlir.constant(32 : index) : i64
        %198 = llvm.insertvalue %197, %196[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %199 = llvm.mlir.constant(32 : index) : i64
        %200 = llvm.insertvalue %199, %198[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %201 = llvm.mlir.constant(32 : index) : i64
        %202 = llvm.insertvalue %201, %200[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %203 = llvm.mlir.constant(1 : index) : i64
        %204 = llvm.insertvalue %203, %202[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %205 = builtin.unrealized_conversion_cast %204 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %206 = builtin.unrealized_conversion_cast %205 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %207 = llvm.extractvalue %170[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %208 = builtin.unrealized_conversion_cast %207 : i64 to index
        %209 = llvm.extractvalue %170[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %210 = llvm.ptrtoint %209 : !llvm.ptr to i64
        %211 = builtin.unrealized_conversion_cast %210 : i64 to index
        %212 = arith.index_cast %211 : index to i64
        %213 = llvm.inttoptr %212 : i64 to !llvm.ptr<f32>
        %214 = llvm.extractvalue %206[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %215 = builtin.unrealized_conversion_cast %214 : i64 to index
        %216 = llvm.extractvalue %206[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = llvm.ptrtoint %216 : !llvm.ptr to i64
        %218 = builtin.unrealized_conversion_cast %217 : i64 to index
        %219 = arith.index_cast %218 : index to i64
        %220 = llvm.inttoptr %219 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %106, %213, %208, %220, %215) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.intr.stackrestore %150 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %107 = llvm.mlir.zero : !llvm.ptr
    %108 = llvm.getelementptr %107[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %109 = llvm.ptrtoint %108 : !llvm.ptr to i64
    %110 = llvm.add %109, %7  : i64
    %111 = llvm.call @malloc(%110) : (i64) -> !llvm.ptr
    %112 = llvm.ptrtoint %111 : !llvm.ptr to i64
    %113 = llvm.sub %7, %8  : i64
    %114 = llvm.add %112, %113  : i64
    %115 = llvm.urem %114, %7  : i64
    %116 = llvm.sub %114, %115  : i64
    %117 = llvm.inttoptr %116 : i64 to !llvm.ptr
    %118 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %119 = llvm.insertvalue %111, %118[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %120 = llvm.insertvalue %117, %119[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %121 = llvm.insertvalue %6, %120[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %122 = llvm.insertvalue %0, %121[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %123 = llvm.insertvalue %5, %122[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %124 = llvm.insertvalue %3, %123[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %125 = llvm.insertvalue %3, %124[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %126 = llvm.insertvalue %1, %125[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %127 = llvm.insertvalue %2, %126[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %128 = llvm.insertvalue %3, %127[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %129 = llvm.insertvalue %8, %128[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %130 = builtin.unrealized_conversion_cast %129 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %131 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        %150 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %151 = llvm.extractvalue %55[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %152 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %153 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %154 = llvm.insertvalue %151, %153[0] : !llvm.struct<(ptr, ptr, i64)> 
        %155 = llvm.insertvalue %152, %154[1] : !llvm.struct<(ptr, ptr, i64)> 
        %156 = llvm.mlir.constant(0 : index) : i64
        %157 = llvm.insertvalue %156, %155[2] : !llvm.struct<(ptr, ptr, i64)> 
        %158 = llvm.extractvalue %55[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %159 = llvm.extractvalue %55[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %160 = llvm.extractvalue %55[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %161 = llvm.extractvalue %55[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %162 = llvm.extractvalue %55[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %163 = arith.muli %arg4, %c4096 : index
        %164 = arith.muli %arg3, %c32 : index
        %165 = arith.addi %163, %164 : index
        %166 = builtin.unrealized_conversion_cast %165 : index to i64
        %167 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %168 = llvm.extractvalue %157[0] : !llvm.struct<(ptr, ptr, i64)> 
        %169 = llvm.extractvalue %157[1] : !llvm.struct<(ptr, ptr, i64)> 
        %170 = llvm.insertvalue %168, %167[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %171 = llvm.insertvalue %169, %170[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %172 = llvm.insertvalue %166, %171[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %173 = llvm.mlir.constant(32 : index) : i64
        %174 = llvm.insertvalue %173, %172[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %175 = llvm.mlir.constant(128 : index) : i64
        %176 = llvm.insertvalue %175, %174[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %177 = llvm.mlir.constant(32 : index) : i64
        %178 = llvm.insertvalue %177, %176[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %179 = llvm.mlir.constant(1 : index) : i64
        %180 = llvm.insertvalue %179, %178[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %181 = builtin.unrealized_conversion_cast %180 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %182 = builtin.unrealized_conversion_cast %181 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %183 = llvm.extractvalue %129[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %184 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %185 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %186 = llvm.insertvalue %183, %185[0] : !llvm.struct<(ptr, ptr, i64)> 
        %187 = llvm.insertvalue %184, %186[1] : !llvm.struct<(ptr, ptr, i64)> 
        %188 = llvm.mlir.constant(0 : index) : i64
        %189 = llvm.insertvalue %188, %187[2] : !llvm.struct<(ptr, ptr, i64)> 
        %190 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %191 = llvm.extractvalue %129[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %192 = llvm.extractvalue %129[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %193 = llvm.extractvalue %129[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %194 = llvm.extractvalue %129[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %195 = llvm.extractvalue %129[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %196 = llvm.extractvalue %129[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %197 = llvm.extractvalue %129[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %198 = llvm.extractvalue %129[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %199 = arith.muli %arg3, %c8388608 : index
        %200 = arith.muli %arg4, %c1024 : index
        %201 = arith.addi %199, %200 : index
        %202 = builtin.unrealized_conversion_cast %201 : index to i64
        %203 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %204 = llvm.extractvalue %189[0] : !llvm.struct<(ptr, ptr, i64)> 
        %205 = llvm.extractvalue %189[1] : !llvm.struct<(ptr, ptr, i64)> 
        %206 = llvm.insertvalue %204, %203[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %207 = llvm.insertvalue %205, %206[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %208 = llvm.insertvalue %202, %207[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %209 = llvm.mlir.constant(32 : index) : i64
        %210 = llvm.insertvalue %209, %208[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %211 = llvm.mlir.constant(32 : index) : i64
        %212 = llvm.insertvalue %211, %210[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %213 = llvm.mlir.constant(32 : index) : i64
        %214 = llvm.insertvalue %213, %212[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %215 = llvm.mlir.constant(1 : index) : i64
        %216 = llvm.insertvalue %215, %214[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = builtin.unrealized_conversion_cast %216 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %218 = builtin.unrealized_conversion_cast %217 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %219 = llvm.extractvalue %182[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = builtin.unrealized_conversion_cast %219 : i64 to index
        %221 = llvm.extractvalue %182[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.ptrtoint %221 : !llvm.ptr to i64
        %223 = builtin.unrealized_conversion_cast %222 : i64 to index
        %224 = arith.index_cast %223 : index to i64
        %225 = llvm.inttoptr %224 : i64 to !llvm.ptr<f32>
        %226 = llvm.extractvalue %218[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = builtin.unrealized_conversion_cast %226 : i64 to index
        %228 = llvm.extractvalue %218[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %229 = llvm.ptrtoint %228 : !llvm.ptr to i64
        %230 = builtin.unrealized_conversion_cast %229 : i64 to index
        %231 = arith.index_cast %230 : index to i64
        %232 = llvm.inttoptr %231 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %131, %225, %220, %232, %227) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.intr.stackrestore %150 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %132 = llvm.extractvalue %18[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %133 = llvm.extractvalue %18[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %134 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %135 = llvm.insertvalue %132, %134[0] : !llvm.struct<(ptr, ptr, i64)> 
    %136 = llvm.insertvalue %133, %135[1] : !llvm.struct<(ptr, ptr, i64)> 
    %137 = llvm.mlir.constant(0 : index) : i64
    %138 = llvm.insertvalue %137, %136[2] : !llvm.struct<(ptr, ptr, i64)> 
    %139 = llvm.extractvalue %18[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %140 = llvm.extractvalue %18[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %141 = llvm.extractvalue %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %142 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %143 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %144 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        %150 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %151 = llvm.mlir.zero : !llvm.ptr
        %152 = llvm.getelementptr %151[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %153 = llvm.ptrtoint %152 : !llvm.ptr to i64
        %154 = llvm.add %153, %7  : i64
        %155 = llvm.call @malloc(%154) : (i64) -> !llvm.ptr
        %156 = llvm.ptrtoint %155 : !llvm.ptr to i64
        %157 = llvm.sub %7, %8  : i64
        %158 = llvm.add %156, %157  : i64
        %159 = llvm.urem %158, %7  : i64
        %160 = llvm.sub %158, %159  : i64
        %161 = llvm.inttoptr %160 : i64 to !llvm.ptr
        %162 = llvm.extractvalue %104[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %163 = llvm.extractvalue %104[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %164 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %165 = llvm.insertvalue %162, %164[0] : !llvm.struct<(ptr, ptr, i64)> 
        %166 = llvm.insertvalue %163, %165[1] : !llvm.struct<(ptr, ptr, i64)> 
        %167 = llvm.mlir.constant(0 : index) : i64
        %168 = llvm.insertvalue %167, %166[2] : !llvm.struct<(ptr, ptr, i64)> 
        %169 = llvm.extractvalue %104[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %170 = llvm.extractvalue %104[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %171 = llvm.extractvalue %104[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %172 = llvm.extractvalue %104[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %173 = llvm.extractvalue %104[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %174 = llvm.extractvalue %104[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %175 = llvm.extractvalue %104[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %176 = llvm.extractvalue %104[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %177 = llvm.extractvalue %104[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %178 = arith.muli %arg3, %c8388608 : index
        %179 = builtin.unrealized_conversion_cast %178 : index to i64
        %180 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %181 = llvm.extractvalue %168[0] : !llvm.struct<(ptr, ptr, i64)> 
        %182 = llvm.extractvalue %168[1] : !llvm.struct<(ptr, ptr, i64)> 
        %183 = llvm.insertvalue %181, %180[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %184 = llvm.insertvalue %182, %183[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %185 = llvm.insertvalue %179, %184[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %186 = llvm.mlir.constant(8192 : index) : i64
        %187 = llvm.insertvalue %186, %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %188 = llvm.mlir.constant(1024 : index) : i64
        %189 = llvm.insertvalue %188, %187[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %190 = llvm.mlir.constant(32 : index) : i64
        %191 = llvm.insertvalue %190, %189[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %192 = llvm.mlir.constant(32 : index) : i64
        %193 = llvm.insertvalue %192, %191[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %194 = llvm.mlir.constant(32 : index) : i64
        %195 = llvm.insertvalue %194, %193[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %196 = llvm.mlir.constant(1 : index) : i64
        %197 = llvm.insertvalue %196, %195[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %198 = builtin.unrealized_conversion_cast %197 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %199 = builtin.unrealized_conversion_cast %198 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %200 = llvm.extractvalue %129[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %201 = llvm.extractvalue %129[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %202 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %203 = llvm.insertvalue %200, %202[0] : !llvm.struct<(ptr, ptr, i64)> 
        %204 = llvm.insertvalue %201, %203[1] : !llvm.struct<(ptr, ptr, i64)> 
        %205 = llvm.mlir.constant(0 : index) : i64
        %206 = llvm.insertvalue %205, %204[2] : !llvm.struct<(ptr, ptr, i64)> 
        %207 = llvm.extractvalue %129[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %208 = llvm.extractvalue %129[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %209 = llvm.extractvalue %129[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %210 = llvm.extractvalue %129[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %211 = llvm.extractvalue %129[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %212 = llvm.extractvalue %129[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %213 = llvm.extractvalue %129[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %214 = llvm.extractvalue %129[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %215 = llvm.extractvalue %129[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %216 = arith.muli %arg4, %c8388608 : index
        %217 = builtin.unrealized_conversion_cast %216 : index to i64
        %218 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %219 = llvm.extractvalue %206[0] : !llvm.struct<(ptr, ptr, i64)> 
        %220 = llvm.extractvalue %206[1] : !llvm.struct<(ptr, ptr, i64)> 
        %221 = llvm.insertvalue %219, %218[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %222 = llvm.insertvalue %220, %221[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %223 = llvm.insertvalue %217, %222[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %224 = llvm.mlir.constant(8192 : index) : i64
        %225 = llvm.insertvalue %224, %223[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %226 = llvm.mlir.constant(1024 : index) : i64
        %227 = llvm.insertvalue %226, %225[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %228 = llvm.mlir.constant(32 : index) : i64
        %229 = llvm.insertvalue %228, %227[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %230 = llvm.mlir.constant(32 : index) : i64
        %231 = llvm.insertvalue %230, %229[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %232 = llvm.mlir.constant(32 : index) : i64
        %233 = llvm.insertvalue %232, %231[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %234 = llvm.mlir.constant(1 : index) : i64
        %235 = llvm.insertvalue %234, %233[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %236 = builtin.unrealized_conversion_cast %235 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %237 = builtin.unrealized_conversion_cast %236 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %238 = llvm.extractvalue %199[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %239 = builtin.unrealized_conversion_cast %238 : i64 to index
        %240 = llvm.extractvalue %199[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %241 = llvm.ptrtoint %240 : !llvm.ptr to i64
        %242 = builtin.unrealized_conversion_cast %241 : i64 to index
        %243 = arith.index_cast %242 : index to i64
        %244 = llvm.inttoptr %243 : i64 to !llvm.ptr<f32>
        %245 = llvm.extractvalue %237[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %246 = builtin.unrealized_conversion_cast %245 : i64 to index
        %247 = llvm.extractvalue %237[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %248 = llvm.ptrtoint %247 : !llvm.ptr to i64
        %249 = builtin.unrealized_conversion_cast %248 : i64 to index
        %250 = arith.index_cast %249 : index to i64
        %251 = llvm.inttoptr %250 : i64 to !llvm.ptr<f32>
        %252 = llvm.ptrtoint %161 : !llvm.ptr to i64
        %253 = builtin.unrealized_conversion_cast %252 : i64 to index
        %254 = arith.index_cast %253 : index to i64
        %255 = llvm.inttoptr %254 : i64 to !llvm.ptr<f32>
        func.call @xsmm_brgemm_invoke(%c1_i64, %142, %244, %239, %251, %246, %255, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
        %256 = arith.muli %arg4, %c32 : index
        %257 = builtin.unrealized_conversion_cast %256 : index to i64
        %258 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %259 = llvm.extractvalue %138[0] : !llvm.struct<(ptr, ptr, i64)> 
        %260 = llvm.extractvalue %138[1] : !llvm.struct<(ptr, ptr, i64)> 
        %261 = llvm.insertvalue %259, %258[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %262 = llvm.insertvalue %260, %261[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %263 = llvm.insertvalue %257, %262[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %264 = llvm.mlir.constant(32 : index) : i64
        %265 = llvm.insertvalue %264, %263[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %266 = llvm.mlir.constant(1 : index) : i64
        %267 = llvm.insertvalue %266, %265[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %268 = builtin.unrealized_conversion_cast %267 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<32xf32, strided<[1], offset: ?>>
        %269 = builtin.unrealized_conversion_cast %268 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %270 = llvm.extractvalue %269[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %271 = builtin.unrealized_conversion_cast %270 : i64 to index
        %272 = llvm.extractvalue %269[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %273 = llvm.ptrtoint %272 : !llvm.ptr to i64
        %274 = builtin.unrealized_conversion_cast %273 : i64 to index
        %275 = arith.index_cast %274 : index to i64
        %276 = llvm.inttoptr %275 : i64 to !llvm.ptr<f32>
        func.call @xsmm_binary_invoke(%c1_i64, %143, %276, %271, %255, %c0, %255, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        %277 = llvm.extractvalue %75[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %278 = llvm.extractvalue %75[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %279 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %280 = llvm.insertvalue %277, %279[0] : !llvm.struct<(ptr, ptr, i64)> 
        %281 = llvm.insertvalue %278, %280[1] : !llvm.struct<(ptr, ptr, i64)> 
        %282 = llvm.mlir.constant(0 : index) : i64
        %283 = llvm.insertvalue %282, %281[2] : !llvm.struct<(ptr, ptr, i64)> 
        %284 = llvm.extractvalue %75[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %285 = llvm.extractvalue %75[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %286 = llvm.extractvalue %75[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %287 = llvm.extractvalue %75[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %288 = llvm.extractvalue %75[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %289 = arith.muli %arg3, %c4096 : index
        %290 = arith.muli %arg4, %c32 : index
        %291 = arith.addi %289, %290 : index
        %292 = builtin.unrealized_conversion_cast %291 : index to i64
        %293 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %294 = llvm.extractvalue %283[0] : !llvm.struct<(ptr, ptr, i64)> 
        %295 = llvm.extractvalue %283[1] : !llvm.struct<(ptr, ptr, i64)> 
        %296 = llvm.insertvalue %294, %293[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %297 = llvm.insertvalue %295, %296[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %298 = llvm.insertvalue %292, %297[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %299 = llvm.mlir.constant(32 : index) : i64
        %300 = llvm.insertvalue %299, %298[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %301 = llvm.mlir.constant(128 : index) : i64
        %302 = llvm.insertvalue %301, %300[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %303 = llvm.mlir.constant(32 : index) : i64
        %304 = llvm.insertvalue %303, %302[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %305 = llvm.mlir.constant(1 : index) : i64
        %306 = llvm.insertvalue %305, %304[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %307 = builtin.unrealized_conversion_cast %306 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %308 = builtin.unrealized_conversion_cast %307 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %309 = llvm.extractvalue %308[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %310 = builtin.unrealized_conversion_cast %309 : i64 to index
        %311 = llvm.extractvalue %308[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %312 = llvm.ptrtoint %311 : !llvm.ptr to i64
        %313 = builtin.unrealized_conversion_cast %312 : i64 to index
        %314 = arith.index_cast %313 : index to i64
        %315 = llvm.inttoptr %314 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %144, %255, %c0, %315, %310) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.call @free(%155) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %150 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%41) : (!llvm.ptr) -> ()
    llvm.call @free(%86) : (!llvm.ptr) -> ()
    llvm.call @free(%111) : (!llvm.ptr) -> ()
    %145 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %75, %145 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %146 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %147 = llvm.insertvalue %4, %146[0] : !llvm.struct<(i64, ptr)> 
    %148 = llvm.insertvalue %145, %147[1] : !llvm.struct<(i64, ptr)> 
    %149 = builtin.unrealized_conversion_cast %148 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%149) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c4096 = arith.constant 4096 : index
    %c1024 = arith.constant 1024 : index
    %c32 = arith.constant 32 : index
    %c8388608 = arith.constant 8388608 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %23 = llvm.extractvalue %21[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %24 = llvm.extractvalue %21[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %25 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %26 = llvm.insertvalue %23, %25[0] : !llvm.struct<(ptr, ptr, i64)> 
    %27 = llvm.insertvalue %24, %26[1] : !llvm.struct<(ptr, ptr, i64)> 
    %28 = llvm.mlir.constant(0 : index) : i64
    %29 = llvm.insertvalue %28, %27[2] : !llvm.struct<(ptr, ptr, i64)> 
    %30 = llvm.extractvalue %21[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %31 = llvm.extractvalue %21[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %32 = llvm.extractvalue %21[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %33 = llvm.extractvalue %21[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %34 = llvm.extractvalue %21[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %35 = llvm.extractvalue %21[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %36 = llvm.extractvalue %21[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %37 = llvm.mlir.zero : !llvm.ptr
    %38 = llvm.getelementptr %37[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %39 = llvm.ptrtoint %38 : !llvm.ptr to i64
    %40 = llvm.add %39, %7  : i64
    %41 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %42 = llvm.ptrtoint %41 : !llvm.ptr to i64
    %43 = llvm.sub %7, %8  : i64
    %44 = llvm.add %42, %43  : i64
    %45 = llvm.urem %44, %7  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %49 = llvm.insertvalue %41, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %47, %49[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %6, %50[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %10, %51[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %9, %52[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %9, %53[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = llvm.insertvalue %8, %54[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = builtin.unrealized_conversion_cast %55 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    cf.br ^bb1(%c0 : index)
  ^bb1(%57: index):  // 2 preds: ^bb0, ^bb5
    %58 = arith.cmpi slt, %57, %c128 : index
    cf.cond_br %58, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    %59 = builtin.unrealized_conversion_cast %57 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%60: index):  // 2 preds: ^bb2, ^bb4
    %61 = arith.cmpi slt, %60, %c262144 : index
    cf.cond_br %61, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %62 = builtin.unrealized_conversion_cast %60 : index to i64
    %63 = llvm.extractvalue %15[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %64 = llvm.mlir.constant(262144 : index) : i64
    %65 = llvm.mul %59, %64  : i64
    %66 = llvm.add %65, %62  : i64
    %67 = llvm.getelementptr %63[%66] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %68 = llvm.load %67 : !llvm.ptr -> f32
    %69 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.mlir.constant(128 : index) : i64
    %71 = llvm.mul %62, %70  : i64
    %72 = llvm.add %71, %59  : i64
    %73 = llvm.getelementptr %69[%72] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %68, %73 : f32, !llvm.ptr
    %74 = arith.addi %60, %c1 : index
    cf.br ^bb3(%74 : index)
  ^bb5:  // pred: ^bb3
    %75 = arith.addi %57, %c1 : index
    cf.br ^bb1(%75 : index)
  ^bb6:  // pred: ^bb1
    %76 = llvm.mlir.zero : !llvm.ptr
    %77 = llvm.getelementptr %76[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.add %78, %7  : i64
    %80 = llvm.call @malloc(%79) : (i64) -> !llvm.ptr
    %81 = llvm.ptrtoint %80 : !llvm.ptr to i64
    %82 = llvm.sub %7, %8  : i64
    %83 = llvm.add %81, %82  : i64
    %84 = llvm.urem %83, %7  : i64
    %85 = llvm.sub %83, %84  : i64
    %86 = llvm.inttoptr %85 : i64 to !llvm.ptr
    %87 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %88 = llvm.insertvalue %80, %87[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %89 = llvm.insertvalue %86, %88[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.insertvalue %6, %89[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.insertvalue %7, %90[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = llvm.insertvalue %9, %91[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %93 = llvm.insertvalue %9, %92[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %94 = llvm.insertvalue %8, %93[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %95 = builtin.unrealized_conversion_cast %94 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %96 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %97 = llvm.ptrtoint %86 : !llvm.ptr to i64
    %98 = builtin.unrealized_conversion_cast %97 : i64 to index
    %99 = arith.index_cast %98 : index to i64
    %100 = llvm.inttoptr %99 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %96, %cst, %100, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %101 = llvm.mlir.zero : !llvm.ptr
    %102 = llvm.getelementptr %101[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %103 = llvm.ptrtoint %102 : !llvm.ptr to i64
    %104 = llvm.add %103, %7  : i64
    %105 = llvm.call @malloc(%104) : (i64) -> !llvm.ptr
    %106 = llvm.ptrtoint %105 : !llvm.ptr to i64
    %107 = llvm.sub %7, %8  : i64
    %108 = llvm.add %106, %107  : i64
    %109 = llvm.urem %108, %7  : i64
    %110 = llvm.sub %108, %109  : i64
    %111 = llvm.inttoptr %110 : i64 to !llvm.ptr
    %112 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %113 = llvm.insertvalue %105, %112[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %111, %113[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %6, %114[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = llvm.insertvalue %4, %115[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %117 = llvm.insertvalue %5, %116[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %118 = llvm.insertvalue %3, %117[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %119 = llvm.insertvalue %3, %118[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %120 = llvm.insertvalue %1, %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %121 = llvm.insertvalue %2, %120[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %122 = llvm.insertvalue %3, %121[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %123 = llvm.insertvalue %8, %122[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %124 = builtin.unrealized_conversion_cast %123 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %125 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        %169 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %170 = arith.muli %arg3, %c8388608 : index
        %171 = arith.muli %arg4, %c32 : index
        %172 = arith.addi %170, %171 : index
        %173 = builtin.unrealized_conversion_cast %172 : index to i64
        %174 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %175 = llvm.extractvalue %29[0] : !llvm.struct<(ptr, ptr, i64)> 
        %176 = llvm.extractvalue %29[1] : !llvm.struct<(ptr, ptr, i64)> 
        %177 = llvm.insertvalue %175, %174[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %178 = llvm.insertvalue %176, %177[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %179 = llvm.insertvalue %173, %178[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %180 = llvm.mlir.constant(32 : index) : i64
        %181 = llvm.insertvalue %180, %179[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %182 = llvm.mlir.constant(262144 : index) : i64
        %183 = llvm.insertvalue %182, %181[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %184 = llvm.mlir.constant(32 : index) : i64
        %185 = llvm.insertvalue %184, %183[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %186 = llvm.mlir.constant(1 : index) : i64
        %187 = llvm.insertvalue %186, %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %188 = builtin.unrealized_conversion_cast %187 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
        %189 = builtin.unrealized_conversion_cast %188 : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %190 = llvm.extractvalue %123[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %191 = llvm.extractvalue %123[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %192 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %193 = llvm.insertvalue %190, %192[0] : !llvm.struct<(ptr, ptr, i64)> 
        %194 = llvm.insertvalue %191, %193[1] : !llvm.struct<(ptr, ptr, i64)> 
        %195 = llvm.mlir.constant(0 : index) : i64
        %196 = llvm.insertvalue %195, %194[2] : !llvm.struct<(ptr, ptr, i64)> 
        %197 = llvm.extractvalue %123[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %198 = llvm.extractvalue %123[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %199 = llvm.extractvalue %123[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %200 = llvm.extractvalue %123[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %201 = llvm.extractvalue %123[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %202 = llvm.extractvalue %123[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %203 = llvm.extractvalue %123[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %204 = llvm.extractvalue %123[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %205 = llvm.extractvalue %123[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %206 = arith.muli %arg3, %c8388608 : index
        %207 = arith.muli %arg4, %c1024 : index
        %208 = arith.addi %206, %207 : index
        %209 = builtin.unrealized_conversion_cast %208 : index to i64
        %210 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %211 = llvm.extractvalue %196[0] : !llvm.struct<(ptr, ptr, i64)> 
        %212 = llvm.extractvalue %196[1] : !llvm.struct<(ptr, ptr, i64)> 
        %213 = llvm.insertvalue %211, %210[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %214 = llvm.insertvalue %212, %213[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %215 = llvm.insertvalue %209, %214[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %216 = llvm.mlir.constant(32 : index) : i64
        %217 = llvm.insertvalue %216, %215[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.mlir.constant(32 : index) : i64
        %219 = llvm.insertvalue %218, %217[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.mlir.constant(32 : index) : i64
        %221 = llvm.insertvalue %220, %219[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.mlir.constant(1 : index) : i64
        %223 = llvm.insertvalue %222, %221[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %224 = builtin.unrealized_conversion_cast %223 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %225 = builtin.unrealized_conversion_cast %224 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %226 = llvm.extractvalue %189[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = builtin.unrealized_conversion_cast %226 : i64 to index
        %228 = llvm.extractvalue %189[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %229 = llvm.ptrtoint %228 : !llvm.ptr to i64
        %230 = builtin.unrealized_conversion_cast %229 : i64 to index
        %231 = arith.index_cast %230 : index to i64
        %232 = llvm.inttoptr %231 : i64 to !llvm.ptr<f32>
        %233 = llvm.extractvalue %225[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %234 = builtin.unrealized_conversion_cast %233 : i64 to index
        %235 = llvm.extractvalue %225[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %236 = llvm.ptrtoint %235 : !llvm.ptr to i64
        %237 = builtin.unrealized_conversion_cast %236 : i64 to index
        %238 = arith.index_cast %237 : index to i64
        %239 = llvm.inttoptr %238 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %125, %232, %227, %239, %234) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.intr.stackrestore %169 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %126 = llvm.mlir.zero : !llvm.ptr
    %127 = llvm.getelementptr %126[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
    %129 = llvm.add %128, %7  : i64
    %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
    %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
    %132 = llvm.sub %7, %8  : i64
    %133 = llvm.add %131, %132  : i64
    %134 = llvm.urem %133, %7  : i64
    %135 = llvm.sub %133, %134  : i64
    %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
    %137 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %138 = llvm.insertvalue %130, %137[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %139 = llvm.insertvalue %136, %138[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %140 = llvm.insertvalue %6, %139[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %141 = llvm.insertvalue %0, %140[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %142 = llvm.insertvalue %5, %141[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %143 = llvm.insertvalue %3, %142[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %144 = llvm.insertvalue %3, %143[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %145 = llvm.insertvalue %1, %144[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %146 = llvm.insertvalue %2, %145[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %147 = llvm.insertvalue %3, %146[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %148 = llvm.insertvalue %8, %147[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %149 = builtin.unrealized_conversion_cast %148 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %150 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        %169 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %170 = llvm.extractvalue %55[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %171 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %172 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %173 = llvm.insertvalue %170, %172[0] : !llvm.struct<(ptr, ptr, i64)> 
        %174 = llvm.insertvalue %171, %173[1] : !llvm.struct<(ptr, ptr, i64)> 
        %175 = llvm.mlir.constant(0 : index) : i64
        %176 = llvm.insertvalue %175, %174[2] : !llvm.struct<(ptr, ptr, i64)> 
        %177 = llvm.extractvalue %55[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %178 = llvm.extractvalue %55[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %179 = llvm.extractvalue %55[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %180 = llvm.extractvalue %55[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %181 = llvm.extractvalue %55[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %182 = arith.muli %arg4, %c4096 : index
        %183 = arith.muli %arg3, %c32 : index
        %184 = arith.addi %182, %183 : index
        %185 = builtin.unrealized_conversion_cast %184 : index to i64
        %186 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %187 = llvm.extractvalue %176[0] : !llvm.struct<(ptr, ptr, i64)> 
        %188 = llvm.extractvalue %176[1] : !llvm.struct<(ptr, ptr, i64)> 
        %189 = llvm.insertvalue %187, %186[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %190 = llvm.insertvalue %188, %189[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %191 = llvm.insertvalue %185, %190[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %192 = llvm.mlir.constant(32 : index) : i64
        %193 = llvm.insertvalue %192, %191[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %194 = llvm.mlir.constant(128 : index) : i64
        %195 = llvm.insertvalue %194, %193[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %196 = llvm.mlir.constant(32 : index) : i64
        %197 = llvm.insertvalue %196, %195[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %198 = llvm.mlir.constant(1 : index) : i64
        %199 = llvm.insertvalue %198, %197[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %200 = builtin.unrealized_conversion_cast %199 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %201 = builtin.unrealized_conversion_cast %200 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %202 = llvm.extractvalue %148[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %203 = llvm.extractvalue %148[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %204 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %205 = llvm.insertvalue %202, %204[0] : !llvm.struct<(ptr, ptr, i64)> 
        %206 = llvm.insertvalue %203, %205[1] : !llvm.struct<(ptr, ptr, i64)> 
        %207 = llvm.mlir.constant(0 : index) : i64
        %208 = llvm.insertvalue %207, %206[2] : !llvm.struct<(ptr, ptr, i64)> 
        %209 = llvm.extractvalue %148[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %210 = llvm.extractvalue %148[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %211 = llvm.extractvalue %148[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %212 = llvm.extractvalue %148[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %213 = llvm.extractvalue %148[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %214 = llvm.extractvalue %148[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %215 = llvm.extractvalue %148[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %216 = llvm.extractvalue %148[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %217 = llvm.extractvalue %148[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %218 = arith.muli %arg3, %c8388608 : index
        %219 = arith.muli %arg4, %c1024 : index
        %220 = arith.addi %218, %219 : index
        %221 = builtin.unrealized_conversion_cast %220 : index to i64
        %222 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %223 = llvm.extractvalue %208[0] : !llvm.struct<(ptr, ptr, i64)> 
        %224 = llvm.extractvalue %208[1] : !llvm.struct<(ptr, ptr, i64)> 
        %225 = llvm.insertvalue %223, %222[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %226 = llvm.insertvalue %224, %225[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = llvm.insertvalue %221, %226[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = llvm.mlir.constant(32 : index) : i64
        %229 = llvm.insertvalue %228, %227[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %230 = llvm.mlir.constant(32 : index) : i64
        %231 = llvm.insertvalue %230, %229[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %232 = llvm.mlir.constant(32 : index) : i64
        %233 = llvm.insertvalue %232, %231[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %234 = llvm.mlir.constant(1 : index) : i64
        %235 = llvm.insertvalue %234, %233[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %236 = builtin.unrealized_conversion_cast %235 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %237 = builtin.unrealized_conversion_cast %236 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %238 = llvm.extractvalue %201[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %239 = builtin.unrealized_conversion_cast %238 : i64 to index
        %240 = llvm.extractvalue %201[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %241 = llvm.ptrtoint %240 : !llvm.ptr to i64
        %242 = builtin.unrealized_conversion_cast %241 : i64 to index
        %243 = arith.index_cast %242 : index to i64
        %244 = llvm.inttoptr %243 : i64 to !llvm.ptr<f32>
        %245 = llvm.extractvalue %237[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %246 = builtin.unrealized_conversion_cast %245 : i64 to index
        %247 = llvm.extractvalue %237[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %248 = llvm.ptrtoint %247 : !llvm.ptr to i64
        %249 = builtin.unrealized_conversion_cast %248 : i64 to index
        %250 = arith.index_cast %249 : index to i64
        %251 = llvm.inttoptr %250 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %150, %244, %239, %251, %246) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.intr.stackrestore %169 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %151 = llvm.extractvalue %18[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %152 = llvm.extractvalue %18[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %153 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %154 = llvm.insertvalue %151, %153[0] : !llvm.struct<(ptr, ptr, i64)> 
    %155 = llvm.insertvalue %152, %154[1] : !llvm.struct<(ptr, ptr, i64)> 
    %156 = llvm.mlir.constant(0 : index) : i64
    %157 = llvm.insertvalue %156, %155[2] : !llvm.struct<(ptr, ptr, i64)> 
    %158 = llvm.extractvalue %18[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %159 = llvm.extractvalue %18[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %160 = llvm.extractvalue %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %161 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %162 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %163 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        %169 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %170 = llvm.mlir.zero : !llvm.ptr
        %171 = llvm.getelementptr %170[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %172 = llvm.ptrtoint %171 : !llvm.ptr to i64
        %173 = llvm.add %172, %7  : i64
        %174 = llvm.call @malloc(%173) : (i64) -> !llvm.ptr
        %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
        %176 = llvm.sub %7, %8  : i64
        %177 = llvm.add %175, %176  : i64
        %178 = llvm.urem %177, %7  : i64
        %179 = llvm.sub %177, %178  : i64
        %180 = llvm.inttoptr %179 : i64 to !llvm.ptr
        %181 = llvm.extractvalue %123[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %182 = llvm.extractvalue %123[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %183 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %184 = llvm.insertvalue %181, %183[0] : !llvm.struct<(ptr, ptr, i64)> 
        %185 = llvm.insertvalue %182, %184[1] : !llvm.struct<(ptr, ptr, i64)> 
        %186 = llvm.mlir.constant(0 : index) : i64
        %187 = llvm.insertvalue %186, %185[2] : !llvm.struct<(ptr, ptr, i64)> 
        %188 = llvm.extractvalue %123[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %189 = llvm.extractvalue %123[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %190 = llvm.extractvalue %123[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %191 = llvm.extractvalue %123[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %192 = llvm.extractvalue %123[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %193 = llvm.extractvalue %123[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %194 = llvm.extractvalue %123[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %195 = llvm.extractvalue %123[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %196 = llvm.extractvalue %123[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %197 = arith.muli %arg3, %c8388608 : index
        %198 = builtin.unrealized_conversion_cast %197 : index to i64
        %199 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %200 = llvm.extractvalue %187[0] : !llvm.struct<(ptr, ptr, i64)> 
        %201 = llvm.extractvalue %187[1] : !llvm.struct<(ptr, ptr, i64)> 
        %202 = llvm.insertvalue %200, %199[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %203 = llvm.insertvalue %201, %202[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %204 = llvm.insertvalue %198, %203[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %205 = llvm.mlir.constant(8192 : index) : i64
        %206 = llvm.insertvalue %205, %204[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %207 = llvm.mlir.constant(1024 : index) : i64
        %208 = llvm.insertvalue %207, %206[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %209 = llvm.mlir.constant(32 : index) : i64
        %210 = llvm.insertvalue %209, %208[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %211 = llvm.mlir.constant(32 : index) : i64
        %212 = llvm.insertvalue %211, %210[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %213 = llvm.mlir.constant(32 : index) : i64
        %214 = llvm.insertvalue %213, %212[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %215 = llvm.mlir.constant(1 : index) : i64
        %216 = llvm.insertvalue %215, %214[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %217 = builtin.unrealized_conversion_cast %216 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %218 = builtin.unrealized_conversion_cast %217 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %219 = llvm.extractvalue %148[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %220 = llvm.extractvalue %148[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %221 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %222 = llvm.insertvalue %219, %221[0] : !llvm.struct<(ptr, ptr, i64)> 
        %223 = llvm.insertvalue %220, %222[1] : !llvm.struct<(ptr, ptr, i64)> 
        %224 = llvm.mlir.constant(0 : index) : i64
        %225 = llvm.insertvalue %224, %223[2] : !llvm.struct<(ptr, ptr, i64)> 
        %226 = llvm.extractvalue %148[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %227 = llvm.extractvalue %148[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %228 = llvm.extractvalue %148[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %229 = llvm.extractvalue %148[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %230 = llvm.extractvalue %148[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %148[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.extractvalue %148[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %233 = llvm.extractvalue %148[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %234 = llvm.extractvalue %148[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %235 = arith.muli %arg4, %c8388608 : index
        %236 = builtin.unrealized_conversion_cast %235 : index to i64
        %237 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %238 = llvm.extractvalue %225[0] : !llvm.struct<(ptr, ptr, i64)> 
        %239 = llvm.extractvalue %225[1] : !llvm.struct<(ptr, ptr, i64)> 
        %240 = llvm.insertvalue %238, %237[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %241 = llvm.insertvalue %239, %240[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %242 = llvm.insertvalue %236, %241[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %243 = llvm.mlir.constant(8192 : index) : i64
        %244 = llvm.insertvalue %243, %242[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %245 = llvm.mlir.constant(1024 : index) : i64
        %246 = llvm.insertvalue %245, %244[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %247 = llvm.mlir.constant(32 : index) : i64
        %248 = llvm.insertvalue %247, %246[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %249 = llvm.mlir.constant(32 : index) : i64
        %250 = llvm.insertvalue %249, %248[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %251 = llvm.mlir.constant(32 : index) : i64
        %252 = llvm.insertvalue %251, %250[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %253 = llvm.mlir.constant(1 : index) : i64
        %254 = llvm.insertvalue %253, %252[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %255 = builtin.unrealized_conversion_cast %254 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %256 = builtin.unrealized_conversion_cast %255 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %257 = llvm.extractvalue %218[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %258 = builtin.unrealized_conversion_cast %257 : i64 to index
        %259 = llvm.extractvalue %218[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %260 = llvm.ptrtoint %259 : !llvm.ptr to i64
        %261 = builtin.unrealized_conversion_cast %260 : i64 to index
        %262 = arith.index_cast %261 : index to i64
        %263 = llvm.inttoptr %262 : i64 to !llvm.ptr<f32>
        %264 = llvm.extractvalue %256[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %265 = builtin.unrealized_conversion_cast %264 : i64 to index
        %266 = llvm.extractvalue %256[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %267 = llvm.ptrtoint %266 : !llvm.ptr to i64
        %268 = builtin.unrealized_conversion_cast %267 : i64 to index
        %269 = arith.index_cast %268 : index to i64
        %270 = llvm.inttoptr %269 : i64 to !llvm.ptr<f32>
        %271 = llvm.ptrtoint %180 : !llvm.ptr to i64
        %272 = builtin.unrealized_conversion_cast %271 : i64 to index
        %273 = arith.index_cast %272 : index to i64
        %274 = llvm.inttoptr %273 : i64 to !llvm.ptr<f32>
        func.call @xsmm_brgemm_invoke(%c1_i64, %161, %263, %258, %270, %265, %274, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
        %275 = arith.muli %arg4, %c32 : index
        %276 = builtin.unrealized_conversion_cast %275 : index to i64
        %277 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %278 = llvm.extractvalue %157[0] : !llvm.struct<(ptr, ptr, i64)> 
        %279 = llvm.extractvalue %157[1] : !llvm.struct<(ptr, ptr, i64)> 
        %280 = llvm.insertvalue %278, %277[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %281 = llvm.insertvalue %279, %280[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %282 = llvm.insertvalue %276, %281[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %283 = llvm.mlir.constant(32 : index) : i64
        %284 = llvm.insertvalue %283, %282[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %285 = llvm.mlir.constant(1 : index) : i64
        %286 = llvm.insertvalue %285, %284[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %287 = builtin.unrealized_conversion_cast %286 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<32xf32, strided<[1], offset: ?>>
        %288 = builtin.unrealized_conversion_cast %287 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %289 = llvm.extractvalue %288[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %290 = builtin.unrealized_conversion_cast %289 : i64 to index
        %291 = llvm.extractvalue %288[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %292 = llvm.ptrtoint %291 : !llvm.ptr to i64
        %293 = builtin.unrealized_conversion_cast %292 : i64 to index
        %294 = arith.index_cast %293 : index to i64
        %295 = llvm.inttoptr %294 : i64 to !llvm.ptr<f32>
        func.call @xsmm_binary_invoke(%c1_i64, %162, %295, %290, %274, %c0, %274, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        %296 = llvm.extractvalue %94[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %297 = llvm.extractvalue %94[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %298 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %299 = llvm.insertvalue %296, %298[0] : !llvm.struct<(ptr, ptr, i64)> 
        %300 = llvm.insertvalue %297, %299[1] : !llvm.struct<(ptr, ptr, i64)> 
        %301 = llvm.mlir.constant(0 : index) : i64
        %302 = llvm.insertvalue %301, %300[2] : !llvm.struct<(ptr, ptr, i64)> 
        %303 = llvm.extractvalue %94[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %304 = llvm.extractvalue %94[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %305 = llvm.extractvalue %94[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %306 = llvm.extractvalue %94[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %307 = llvm.extractvalue %94[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %308 = arith.muli %arg3, %c4096 : index
        %309 = arith.muli %arg4, %c32 : index
        %310 = arith.addi %308, %309 : index
        %311 = builtin.unrealized_conversion_cast %310 : index to i64
        %312 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %313 = llvm.extractvalue %302[0] : !llvm.struct<(ptr, ptr, i64)> 
        %314 = llvm.extractvalue %302[1] : !llvm.struct<(ptr, ptr, i64)> 
        %315 = llvm.insertvalue %313, %312[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %316 = llvm.insertvalue %314, %315[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %317 = llvm.insertvalue %311, %316[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %318 = llvm.mlir.constant(32 : index) : i64
        %319 = llvm.insertvalue %318, %317[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %320 = llvm.mlir.constant(128 : index) : i64
        %321 = llvm.insertvalue %320, %319[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %322 = llvm.mlir.constant(32 : index) : i64
        %323 = llvm.insertvalue %322, %321[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %324 = llvm.mlir.constant(1 : index) : i64
        %325 = llvm.insertvalue %324, %323[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %326 = builtin.unrealized_conversion_cast %325 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %327 = builtin.unrealized_conversion_cast %326 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %328 = llvm.extractvalue %327[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %329 = builtin.unrealized_conversion_cast %328 : i64 to index
        %330 = llvm.extractvalue %327[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %331 = llvm.ptrtoint %330 : !llvm.ptr to i64
        %332 = builtin.unrealized_conversion_cast %331 : i64 to index
        %333 = arith.index_cast %332 : index to i64
        %334 = llvm.inttoptr %333 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %163, %274, %c0, %334, %329) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.call @free(%174) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %169 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%41) : (!llvm.ptr) -> ()
    llvm.call @free(%105) : (!llvm.ptr) -> ()
    llvm.call @free(%130) : (!llvm.ptr) -> ()
    %164 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %94, %164 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %165 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %166 = llvm.insertvalue %4, %165[0] : !llvm.struct<(i64, ptr)> 
    %167 = llvm.insertvalue %164, %166[1] : !llvm.struct<(i64, ptr)> 
    %168 = builtin.unrealized_conversion_cast %167 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%168) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump Before ConvertOpenMPToLLVMPass (convert-openmp-to-llvm) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  func.func private @refbackend_consume_func_return_mrf32(memref<*xf32>) attributes {llvm.emit_c_interface}
  func.func private @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index)
  func.func private @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64)
  func.func private @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
  func.func private @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, index)
  func.func @MLP(%arg0: memref<*xf32>, %arg1: memref<*xf32>, %arg2: memref<*xf32>) attributes {llvm.emit_c_interface} {
    %c4096 = arith.constant 4096 : index
    %c1024 = arith.constant 1024 : index
    %c32 = arith.constant 32 : index
    %c8388608 = arith.constant 8388608 : index
    %c0 = arith.constant 0 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c262144 = arith.constant 262144 : index
    %0 = llvm.mlir.constant(4 : index) : i64
    %1 = llvm.mlir.constant(8388608 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(2 : index) : i64
    %5 = llvm.mlir.constant(8192 : index) : i64
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(64 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(128 : index) : i64
    %10 = llvm.mlir.constant(262144 : index) : i64
    %cst = arith.constant 0.000000e+00 : f32
    %c8192_i64 = arith.constant 8192 : i64
    %c4 = arith.constant 4 : index
    %c8192 = arith.constant 8192 : index
    %c2 = arith.constant 2 : index
    %c5_i64 = arith.constant 5 : i64
    %c4_i64 = arith.constant 4 : i64
    %c1024_i64 = arith.constant 1024 : i64
    %c0_i64 = arith.constant 0 : i64
    %c262144_i64 = arith.constant 262144 : i64
    %c32_i64 = arith.constant 32 : i64
    %c8_i64 = arith.constant 8 : i64
    %c128_i64 = arith.constant 128 : i64
    %c64_i64 = arith.constant 64 : i64
    %c1_i64 = arith.constant 1 : i64
    %c2_i64 = arith.constant 2 : i64
    %11 = builtin.unrealized_conversion_cast %arg0 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %12 = builtin.unrealized_conversion_cast %arg1 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %13 = builtin.unrealized_conversion_cast %arg2 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %14 = llvm.extractvalue %11[1] : !llvm.struct<(i64, ptr)> 
    %15 = llvm.load %14 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %16 = builtin.unrealized_conversion_cast %15 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %17 = llvm.extractvalue %12[1] : !llvm.struct<(i64, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %19 = builtin.unrealized_conversion_cast %18 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %20 = llvm.extractvalue %13[1] : !llvm.struct<(i64, ptr)> 
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %22 = builtin.unrealized_conversion_cast %21 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %23 = llvm.extractvalue %21[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %24 = llvm.extractvalue %21[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %25 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %26 = llvm.insertvalue %23, %25[0] : !llvm.struct<(ptr, ptr, i64)> 
    %27 = llvm.insertvalue %24, %26[1] : !llvm.struct<(ptr, ptr, i64)> 
    %28 = llvm.mlir.constant(0 : index) : i64
    %29 = llvm.insertvalue %28, %27[2] : !llvm.struct<(ptr, ptr, i64)> 
    %30 = llvm.extractvalue %21[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %31 = llvm.extractvalue %21[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %32 = llvm.extractvalue %21[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %33 = llvm.extractvalue %21[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %34 = llvm.extractvalue %21[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %35 = llvm.extractvalue %21[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %36 = llvm.extractvalue %21[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %37 = llvm.mlir.zero : !llvm.ptr
    %38 = llvm.getelementptr %37[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %39 = llvm.ptrtoint %38 : !llvm.ptr to i64
    %40 = llvm.add %39, %7  : i64
    %41 = llvm.call @malloc(%40) : (i64) -> !llvm.ptr
    %42 = llvm.ptrtoint %41 : !llvm.ptr to i64
    %43 = llvm.sub %7, %8  : i64
    %44 = llvm.add %42, %43  : i64
    %45 = llvm.urem %44, %7  : i64
    %46 = llvm.sub %44, %45  : i64
    %47 = llvm.inttoptr %46 : i64 to !llvm.ptr
    %48 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %49 = llvm.insertvalue %41, %48[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.insertvalue %47, %49[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.insertvalue %6, %50[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %52 = llvm.insertvalue %10, %51[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %53 = llvm.insertvalue %9, %52[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %54 = llvm.insertvalue %9, %53[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %55 = llvm.insertvalue %8, %54[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %56 = builtin.unrealized_conversion_cast %55 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    cf.br ^bb1(%c0 : index)
  ^bb1(%57: index):  // 2 preds: ^bb0, ^bb5
    %58 = arith.cmpi slt, %57, %c128 : index
    cf.cond_br %58, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    %59 = builtin.unrealized_conversion_cast %57 : index to i64
    cf.br ^bb3(%c0 : index)
  ^bb3(%60: index):  // 2 preds: ^bb2, ^bb4
    %61 = arith.cmpi slt, %60, %c262144 : index
    cf.cond_br %61, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %62 = builtin.unrealized_conversion_cast %60 : index to i64
    %63 = llvm.extractvalue %15[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %64 = llvm.mlir.constant(262144 : index) : i64
    %65 = llvm.mul %59, %64  : i64
    %66 = llvm.add %65, %62  : i64
    %67 = llvm.getelementptr %63[%66] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %68 = llvm.load %67 : !llvm.ptr -> f32
    %69 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.mlir.constant(128 : index) : i64
    %71 = llvm.mul %62, %70  : i64
    %72 = llvm.add %71, %59  : i64
    %73 = llvm.getelementptr %69[%72] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %68, %73 : f32, !llvm.ptr
    %74 = arith.addi %60, %c1 : index
    cf.br ^bb3(%74 : index)
  ^bb5:  // pred: ^bb3
    %75 = arith.addi %57, %c1 : index
    cf.br ^bb1(%75 : index)
  ^bb6:  // pred: ^bb1
    %76 = llvm.mlir.zero : !llvm.ptr
    %77 = llvm.getelementptr %76[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.add %78, %7  : i64
    %80 = llvm.call @malloc(%79) : (i64) -> !llvm.ptr
    %81 = llvm.ptrtoint %80 : !llvm.ptr to i64
    %82 = llvm.sub %7, %8  : i64
    %83 = llvm.add %81, %82  : i64
    %84 = llvm.urem %83, %7  : i64
    %85 = llvm.sub %83, %84  : i64
    %86 = llvm.inttoptr %85 : i64 to !llvm.ptr
    %87 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %88 = llvm.insertvalue %80, %87[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %89 = llvm.insertvalue %86, %88[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.insertvalue %6, %89[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.insertvalue %7, %90[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = llvm.insertvalue %9, %91[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %93 = llvm.insertvalue %9, %92[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %94 = llvm.insertvalue %8, %93[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %95 = builtin.unrealized_conversion_cast %94 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %96 = call @xsmm_unary_dispatch(%c2_i64, %c1_i64, %c64_i64, %c128_i64, %c1_i64, %c128_i64, %c8_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %97 = llvm.ptrtoint %86 : !llvm.ptr to i64
    %98 = builtin.unrealized_conversion_cast %97 : i64 to index
    %99 = arith.index_cast %98 : index to i64
    %100 = llvm.inttoptr %99 : i64 to !llvm.ptr<f32>
    call @xsmm_unary_scalar_invoke(%c1_i64, %96, %cst, %100, %c0) : (i64, i64, f32, !llvm.ptr<f32>, index) -> ()
    %101 = llvm.mlir.zero : !llvm.ptr
    %102 = llvm.getelementptr %101[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %103 = llvm.ptrtoint %102 : !llvm.ptr to i64
    %104 = llvm.add %103, %7  : i64
    %105 = llvm.call @malloc(%104) : (i64) -> !llvm.ptr
    %106 = llvm.ptrtoint %105 : !llvm.ptr to i64
    %107 = llvm.sub %7, %8  : i64
    %108 = llvm.add %106, %107  : i64
    %109 = llvm.urem %108, %7  : i64
    %110 = llvm.sub %108, %109  : i64
    %111 = llvm.inttoptr %110 : i64 to !llvm.ptr
    %112 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %113 = llvm.insertvalue %105, %112[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %114 = llvm.insertvalue %111, %113[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %115 = llvm.insertvalue %6, %114[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %116 = llvm.insertvalue %4, %115[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %117 = llvm.insertvalue %5, %116[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %118 = llvm.insertvalue %3, %117[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %119 = llvm.insertvalue %3, %118[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %120 = llvm.insertvalue %1, %119[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %121 = llvm.insertvalue %2, %120[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %122 = llvm.insertvalue %3, %121[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %123 = llvm.insertvalue %8, %122[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %124 = builtin.unrealized_conversion_cast %123 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %125 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c262144_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c8192) step (%c1, %c1) {
        %169 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %170 = arith.muli %arg3, %c8388608 : index
        %171 = arith.muli %arg4, %c32 : index
        %172 = arith.addi %170, %171 : index
        %173 = builtin.unrealized_conversion_cast %172 : index to i64
        %174 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %175 = llvm.extractvalue %29[0] : !llvm.struct<(ptr, ptr, i64)> 
        %176 = llvm.extractvalue %29[1] : !llvm.struct<(ptr, ptr, i64)> 
        %177 = llvm.insertvalue %175, %174[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %178 = llvm.insertvalue %176, %177[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %179 = llvm.insertvalue %173, %178[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %180 = llvm.mlir.constant(32 : index) : i64
        %181 = llvm.insertvalue %180, %179[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %182 = llvm.mlir.constant(262144 : index) : i64
        %183 = llvm.insertvalue %182, %181[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %184 = llvm.mlir.constant(32 : index) : i64
        %185 = llvm.insertvalue %184, %183[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %186 = llvm.mlir.constant(1 : index) : i64
        %187 = llvm.insertvalue %186, %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %188 = builtin.unrealized_conversion_cast %187 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
        %189 = builtin.unrealized_conversion_cast %188 : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %190 = llvm.extractvalue %123[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %191 = llvm.extractvalue %123[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %192 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %193 = llvm.insertvalue %190, %192[0] : !llvm.struct<(ptr, ptr, i64)> 
        %194 = llvm.insertvalue %191, %193[1] : !llvm.struct<(ptr, ptr, i64)> 
        %195 = llvm.mlir.constant(0 : index) : i64
        %196 = llvm.insertvalue %195, %194[2] : !llvm.struct<(ptr, ptr, i64)> 
        %197 = llvm.extractvalue %123[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %198 = llvm.extractvalue %123[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %199 = llvm.extractvalue %123[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %200 = llvm.extractvalue %123[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %201 = llvm.extractvalue %123[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %202 = llvm.extractvalue %123[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %203 = llvm.extractvalue %123[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %204 = llvm.extractvalue %123[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %205 = llvm.extractvalue %123[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %206 = arith.muli %arg3, %c8388608 : index
        %207 = arith.muli %arg4, %c1024 : index
        %208 = arith.addi %206, %207 : index
        %209 = builtin.unrealized_conversion_cast %208 : index to i64
        %210 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %211 = llvm.extractvalue %196[0] : !llvm.struct<(ptr, ptr, i64)> 
        %212 = llvm.extractvalue %196[1] : !llvm.struct<(ptr, ptr, i64)> 
        %213 = llvm.insertvalue %211, %210[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %214 = llvm.insertvalue %212, %213[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %215 = llvm.insertvalue %209, %214[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %216 = llvm.mlir.constant(32 : index) : i64
        %217 = llvm.insertvalue %216, %215[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.mlir.constant(32 : index) : i64
        %219 = llvm.insertvalue %218, %217[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.mlir.constant(32 : index) : i64
        %221 = llvm.insertvalue %220, %219[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.mlir.constant(1 : index) : i64
        %223 = llvm.insertvalue %222, %221[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %224 = builtin.unrealized_conversion_cast %223 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %225 = builtin.unrealized_conversion_cast %224 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %226 = llvm.extractvalue %189[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = builtin.unrealized_conversion_cast %226 : i64 to index
        %228 = llvm.extractvalue %189[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %229 = llvm.ptrtoint %228 : !llvm.ptr to i64
        %230 = builtin.unrealized_conversion_cast %229 : i64 to index
        %231 = arith.index_cast %230 : index to i64
        %232 = llvm.inttoptr %231 : i64 to !llvm.ptr<f32>
        %233 = llvm.extractvalue %225[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %234 = builtin.unrealized_conversion_cast %233 : i64 to index
        %235 = llvm.extractvalue %225[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %236 = llvm.ptrtoint %235 : !llvm.ptr to i64
        %237 = builtin.unrealized_conversion_cast %236 : i64 to index
        %238 = arith.index_cast %237 : index to i64
        %239 = llvm.inttoptr %238 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %125, %232, %227, %239, %234) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.intr.stackrestore %169 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %126 = llvm.mlir.zero : !llvm.ptr
    %127 = llvm.getelementptr %126[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %128 = llvm.ptrtoint %127 : !llvm.ptr to i64
    %129 = llvm.add %128, %7  : i64
    %130 = llvm.call @malloc(%129) : (i64) -> !llvm.ptr
    %131 = llvm.ptrtoint %130 : !llvm.ptr to i64
    %132 = llvm.sub %7, %8  : i64
    %133 = llvm.add %131, %132  : i64
    %134 = llvm.urem %133, %7  : i64
    %135 = llvm.sub %133, %134  : i64
    %136 = llvm.inttoptr %135 : i64 to !llvm.ptr
    %137 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %138 = llvm.insertvalue %130, %137[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %139 = llvm.insertvalue %136, %138[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %140 = llvm.insertvalue %6, %139[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %141 = llvm.insertvalue %0, %140[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %142 = llvm.insertvalue %5, %141[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %143 = llvm.insertvalue %3, %142[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %144 = llvm.insertvalue %3, %143[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %145 = llvm.insertvalue %1, %144[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %146 = llvm.insertvalue %2, %145[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %147 = llvm.insertvalue %3, %146[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %148 = llvm.insertvalue %8, %147[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %149 = builtin.unrealized_conversion_cast %148 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %150 = call @xsmm_unary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c128_i64, %c32_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c4, %c8192) step (%c1, %c1) {
        %169 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %170 = llvm.extractvalue %55[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %171 = llvm.extractvalue %55[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %172 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %173 = llvm.insertvalue %170, %172[0] : !llvm.struct<(ptr, ptr, i64)> 
        %174 = llvm.insertvalue %171, %173[1] : !llvm.struct<(ptr, ptr, i64)> 
        %175 = llvm.mlir.constant(0 : index) : i64
        %176 = llvm.insertvalue %175, %174[2] : !llvm.struct<(ptr, ptr, i64)> 
        %177 = llvm.extractvalue %55[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %178 = llvm.extractvalue %55[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %179 = llvm.extractvalue %55[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %180 = llvm.extractvalue %55[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %181 = llvm.extractvalue %55[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %182 = arith.muli %arg4, %c4096 : index
        %183 = arith.muli %arg3, %c32 : index
        %184 = arith.addi %182, %183 : index
        %185 = builtin.unrealized_conversion_cast %184 : index to i64
        %186 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %187 = llvm.extractvalue %176[0] : !llvm.struct<(ptr, ptr, i64)> 
        %188 = llvm.extractvalue %176[1] : !llvm.struct<(ptr, ptr, i64)> 
        %189 = llvm.insertvalue %187, %186[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %190 = llvm.insertvalue %188, %189[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %191 = llvm.insertvalue %185, %190[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %192 = llvm.mlir.constant(32 : index) : i64
        %193 = llvm.insertvalue %192, %191[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %194 = llvm.mlir.constant(128 : index) : i64
        %195 = llvm.insertvalue %194, %193[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %196 = llvm.mlir.constant(32 : index) : i64
        %197 = llvm.insertvalue %196, %195[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %198 = llvm.mlir.constant(1 : index) : i64
        %199 = llvm.insertvalue %198, %197[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %200 = builtin.unrealized_conversion_cast %199 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %201 = builtin.unrealized_conversion_cast %200 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %202 = llvm.extractvalue %148[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %203 = llvm.extractvalue %148[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %204 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %205 = llvm.insertvalue %202, %204[0] : !llvm.struct<(ptr, ptr, i64)> 
        %206 = llvm.insertvalue %203, %205[1] : !llvm.struct<(ptr, ptr, i64)> 
        %207 = llvm.mlir.constant(0 : index) : i64
        %208 = llvm.insertvalue %207, %206[2] : !llvm.struct<(ptr, ptr, i64)> 
        %209 = llvm.extractvalue %148[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %210 = llvm.extractvalue %148[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %211 = llvm.extractvalue %148[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %212 = llvm.extractvalue %148[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %213 = llvm.extractvalue %148[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %214 = llvm.extractvalue %148[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %215 = llvm.extractvalue %148[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %216 = llvm.extractvalue %148[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %217 = llvm.extractvalue %148[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %218 = arith.muli %arg3, %c8388608 : index
        %219 = arith.muli %arg4, %c1024 : index
        %220 = arith.addi %218, %219 : index
        %221 = builtin.unrealized_conversion_cast %220 : index to i64
        %222 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %223 = llvm.extractvalue %208[0] : !llvm.struct<(ptr, ptr, i64)> 
        %224 = llvm.extractvalue %208[1] : !llvm.struct<(ptr, ptr, i64)> 
        %225 = llvm.insertvalue %223, %222[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %226 = llvm.insertvalue %224, %225[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = llvm.insertvalue %221, %226[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = llvm.mlir.constant(32 : index) : i64
        %229 = llvm.insertvalue %228, %227[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %230 = llvm.mlir.constant(32 : index) : i64
        %231 = llvm.insertvalue %230, %229[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %232 = llvm.mlir.constant(32 : index) : i64
        %233 = llvm.insertvalue %232, %231[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %234 = llvm.mlir.constant(1 : index) : i64
        %235 = llvm.insertvalue %234, %233[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %236 = builtin.unrealized_conversion_cast %235 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %237 = builtin.unrealized_conversion_cast %236 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %238 = llvm.extractvalue %201[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %239 = builtin.unrealized_conversion_cast %238 : i64 to index
        %240 = llvm.extractvalue %201[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %241 = llvm.ptrtoint %240 : !llvm.ptr to i64
        %242 = builtin.unrealized_conversion_cast %241 : i64 to index
        %243 = arith.index_cast %242 : index to i64
        %244 = llvm.inttoptr %243 : i64 to !llvm.ptr<f32>
        %245 = llvm.extractvalue %237[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %246 = builtin.unrealized_conversion_cast %245 : i64 to index
        %247 = llvm.extractvalue %237[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %248 = llvm.ptrtoint %247 : !llvm.ptr to i64
        %249 = builtin.unrealized_conversion_cast %248 : i64 to index
        %250 = arith.index_cast %249 : index to i64
        %251 = llvm.inttoptr %250 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %150, %244, %239, %251, %246) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.intr.stackrestore %169 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %151 = llvm.extractvalue %18[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %152 = llvm.extractvalue %18[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %153 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %154 = llvm.insertvalue %151, %153[0] : !llvm.struct<(ptr, ptr, i64)> 
    %155 = llvm.insertvalue %152, %154[1] : !llvm.struct<(ptr, ptr, i64)> 
    %156 = llvm.mlir.constant(0 : index) : i64
    %157 = llvm.insertvalue %156, %155[2] : !llvm.struct<(ptr, ptr, i64)> 
    %158 = llvm.extractvalue %18[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %159 = llvm.extractvalue %18[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %160 = llvm.extractvalue %18[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %161 = call @xsmm_brgemm_dispatch(%c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c1024_i64, %c1024_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %162 = call @xsmm_binary_dispatch(%c1_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c32_i64, %c4_i64) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %163 = call @xsmm_unary_dispatch(%c5_i64, %c1_i64, %c32_i64, %c32_i64, %c32_i64, %c128_i64, %c0_i64) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg3, %arg4) : index = (%c0, %c0) to (%c2, %c4) step (%c1, %c1) {
        %169 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %170 = llvm.mlir.zero : !llvm.ptr
        %171 = llvm.getelementptr %170[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %172 = llvm.ptrtoint %171 : !llvm.ptr to i64
        %173 = llvm.add %172, %7  : i64
        %174 = llvm.call @malloc(%173) : (i64) -> !llvm.ptr
        %175 = llvm.ptrtoint %174 : !llvm.ptr to i64
        %176 = llvm.sub %7, %8  : i64
        %177 = llvm.add %175, %176  : i64
        %178 = llvm.urem %177, %7  : i64
        %179 = llvm.sub %177, %178  : i64
        %180 = llvm.inttoptr %179 : i64 to !llvm.ptr
        %181 = llvm.extractvalue %123[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %182 = llvm.extractvalue %123[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %183 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %184 = llvm.insertvalue %181, %183[0] : !llvm.struct<(ptr, ptr, i64)> 
        %185 = llvm.insertvalue %182, %184[1] : !llvm.struct<(ptr, ptr, i64)> 
        %186 = llvm.mlir.constant(0 : index) : i64
        %187 = llvm.insertvalue %186, %185[2] : !llvm.struct<(ptr, ptr, i64)> 
        %188 = llvm.extractvalue %123[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %189 = llvm.extractvalue %123[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %190 = llvm.extractvalue %123[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %191 = llvm.extractvalue %123[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %192 = llvm.extractvalue %123[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %193 = llvm.extractvalue %123[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %194 = llvm.extractvalue %123[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %195 = llvm.extractvalue %123[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %196 = llvm.extractvalue %123[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %197 = arith.muli %arg3, %c8388608 : index
        %198 = builtin.unrealized_conversion_cast %197 : index to i64
        %199 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %200 = llvm.extractvalue %187[0] : !llvm.struct<(ptr, ptr, i64)> 
        %201 = llvm.extractvalue %187[1] : !llvm.struct<(ptr, ptr, i64)> 
        %202 = llvm.insertvalue %200, %199[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %203 = llvm.insertvalue %201, %202[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %204 = llvm.insertvalue %198, %203[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %205 = llvm.mlir.constant(8192 : index) : i64
        %206 = llvm.insertvalue %205, %204[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %207 = llvm.mlir.constant(1024 : index) : i64
        %208 = llvm.insertvalue %207, %206[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %209 = llvm.mlir.constant(32 : index) : i64
        %210 = llvm.insertvalue %209, %208[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %211 = llvm.mlir.constant(32 : index) : i64
        %212 = llvm.insertvalue %211, %210[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %213 = llvm.mlir.constant(32 : index) : i64
        %214 = llvm.insertvalue %213, %212[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %215 = llvm.mlir.constant(1 : index) : i64
        %216 = llvm.insertvalue %215, %214[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %217 = builtin.unrealized_conversion_cast %216 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %218 = builtin.unrealized_conversion_cast %217 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %219 = llvm.extractvalue %148[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %220 = llvm.extractvalue %148[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %221 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %222 = llvm.insertvalue %219, %221[0] : !llvm.struct<(ptr, ptr, i64)> 
        %223 = llvm.insertvalue %220, %222[1] : !llvm.struct<(ptr, ptr, i64)> 
        %224 = llvm.mlir.constant(0 : index) : i64
        %225 = llvm.insertvalue %224, %223[2] : !llvm.struct<(ptr, ptr, i64)> 
        %226 = llvm.extractvalue %148[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %227 = llvm.extractvalue %148[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %228 = llvm.extractvalue %148[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %229 = llvm.extractvalue %148[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %230 = llvm.extractvalue %148[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %148[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.extractvalue %148[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %233 = llvm.extractvalue %148[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %234 = llvm.extractvalue %148[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %235 = arith.muli %arg4, %c8388608 : index
        %236 = builtin.unrealized_conversion_cast %235 : index to i64
        %237 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %238 = llvm.extractvalue %225[0] : !llvm.struct<(ptr, ptr, i64)> 
        %239 = llvm.extractvalue %225[1] : !llvm.struct<(ptr, ptr, i64)> 
        %240 = llvm.insertvalue %238, %237[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %241 = llvm.insertvalue %239, %240[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %242 = llvm.insertvalue %236, %241[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %243 = llvm.mlir.constant(8192 : index) : i64
        %244 = llvm.insertvalue %243, %242[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %245 = llvm.mlir.constant(1024 : index) : i64
        %246 = llvm.insertvalue %245, %244[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %247 = llvm.mlir.constant(32 : index) : i64
        %248 = llvm.insertvalue %247, %246[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %249 = llvm.mlir.constant(32 : index) : i64
        %250 = llvm.insertvalue %249, %248[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %251 = llvm.mlir.constant(32 : index) : i64
        %252 = llvm.insertvalue %251, %250[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %253 = llvm.mlir.constant(1 : index) : i64
        %254 = llvm.insertvalue %253, %252[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %255 = builtin.unrealized_conversion_cast %254 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %256 = builtin.unrealized_conversion_cast %255 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %257 = llvm.extractvalue %218[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %258 = builtin.unrealized_conversion_cast %257 : i64 to index
        %259 = llvm.extractvalue %218[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %260 = llvm.ptrtoint %259 : !llvm.ptr to i64
        %261 = builtin.unrealized_conversion_cast %260 : i64 to index
        %262 = arith.index_cast %261 : index to i64
        %263 = llvm.inttoptr %262 : i64 to !llvm.ptr<f32>
        %264 = llvm.extractvalue %256[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %265 = builtin.unrealized_conversion_cast %264 : i64 to index
        %266 = llvm.extractvalue %256[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %267 = llvm.ptrtoint %266 : !llvm.ptr to i64
        %268 = builtin.unrealized_conversion_cast %267 : i64 to index
        %269 = arith.index_cast %268 : index to i64
        %270 = llvm.inttoptr %269 : i64 to !llvm.ptr<f32>
        %271 = llvm.ptrtoint %180 : !llvm.ptr to i64
        %272 = builtin.unrealized_conversion_cast %271 : i64 to index
        %273 = arith.index_cast %272 : index to i64
        %274 = llvm.inttoptr %273 : i64 to !llvm.ptr<f32>
        func.call @xsmm_brgemm_invoke(%c1_i64, %161, %263, %258, %270, %265, %274, %c0, %c8192_i64) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, i64) -> ()
        %275 = arith.muli %arg4, %c32 : index
        %276 = builtin.unrealized_conversion_cast %275 : index to i64
        %277 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %278 = llvm.extractvalue %157[0] : !llvm.struct<(ptr, ptr, i64)> 
        %279 = llvm.extractvalue %157[1] : !llvm.struct<(ptr, ptr, i64)> 
        %280 = llvm.insertvalue %278, %277[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %281 = llvm.insertvalue %279, %280[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %282 = llvm.insertvalue %276, %281[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %283 = llvm.mlir.constant(32 : index) : i64
        %284 = llvm.insertvalue %283, %282[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %285 = llvm.mlir.constant(1 : index) : i64
        %286 = llvm.insertvalue %285, %284[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %287 = builtin.unrealized_conversion_cast %286 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<32xf32, strided<[1], offset: ?>>
        %288 = builtin.unrealized_conversion_cast %287 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %289 = llvm.extractvalue %288[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %290 = builtin.unrealized_conversion_cast %289 : i64 to index
        %291 = llvm.extractvalue %288[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %292 = llvm.ptrtoint %291 : !llvm.ptr to i64
        %293 = builtin.unrealized_conversion_cast %292 : i64 to index
        %294 = arith.index_cast %293 : index to i64
        %295 = llvm.inttoptr %294 : i64 to !llvm.ptr<f32>
        func.call @xsmm_binary_invoke(%c1_i64, %162, %295, %290, %274, %c0, %274, %c0) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        %296 = llvm.extractvalue %94[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %297 = llvm.extractvalue %94[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %298 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %299 = llvm.insertvalue %296, %298[0] : !llvm.struct<(ptr, ptr, i64)> 
        %300 = llvm.insertvalue %297, %299[1] : !llvm.struct<(ptr, ptr, i64)> 
        %301 = llvm.mlir.constant(0 : index) : i64
        %302 = llvm.insertvalue %301, %300[2] : !llvm.struct<(ptr, ptr, i64)> 
        %303 = llvm.extractvalue %94[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %304 = llvm.extractvalue %94[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %305 = llvm.extractvalue %94[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %306 = llvm.extractvalue %94[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %307 = llvm.extractvalue %94[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %308 = arith.muli %arg3, %c4096 : index
        %309 = arith.muli %arg4, %c32 : index
        %310 = arith.addi %308, %309 : index
        %311 = builtin.unrealized_conversion_cast %310 : index to i64
        %312 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %313 = llvm.extractvalue %302[0] : !llvm.struct<(ptr, ptr, i64)> 
        %314 = llvm.extractvalue %302[1] : !llvm.struct<(ptr, ptr, i64)> 
        %315 = llvm.insertvalue %313, %312[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %316 = llvm.insertvalue %314, %315[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %317 = llvm.insertvalue %311, %316[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %318 = llvm.mlir.constant(32 : index) : i64
        %319 = llvm.insertvalue %318, %317[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %320 = llvm.mlir.constant(128 : index) : i64
        %321 = llvm.insertvalue %320, %319[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %322 = llvm.mlir.constant(32 : index) : i64
        %323 = llvm.insertvalue %322, %321[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %324 = llvm.mlir.constant(1 : index) : i64
        %325 = llvm.insertvalue %324, %323[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %326 = builtin.unrealized_conversion_cast %325 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %327 = builtin.unrealized_conversion_cast %326 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %328 = llvm.extractvalue %327[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %329 = builtin.unrealized_conversion_cast %328 : i64 to index
        %330 = llvm.extractvalue %327[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %331 = llvm.ptrtoint %330 : !llvm.ptr to i64
        %332 = builtin.unrealized_conversion_cast %331 : i64 to index
        %333 = arith.index_cast %332 : index to i64
        %334 = llvm.inttoptr %333 : i64 to !llvm.ptr<f32>
        func.call @xsmm_unary_invoke(%c1_i64, %163, %274, %c0, %334, %329) : (i64, i64, !llvm.ptr<f32>, index, !llvm.ptr<f32>, index) -> ()
        llvm.call @free(%174) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %169 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%41) : (!llvm.ptr) -> ()
    llvm.call @free(%105) : (!llvm.ptr) -> ()
    llvm.call @free(%130) : (!llvm.ptr) -> ()
    %164 = llvm.alloca %8 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %94, %164 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %165 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %166 = llvm.insertvalue %4, %165[0] : !llvm.struct<(i64, ptr)> 
    %167 = llvm.insertvalue %164, %166[1] : !llvm.struct<(i64, ptr)> 
    %168 = builtin.unrealized_conversion_cast %167 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    call @refbackend_consume_func_return_mrf32(%168) : (memref<*xf32>) -> ()
    return
  }
}


// -----// IR Dump After ConvertOpenMPToLLVMPass (convert-openmp-to-llvm) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func private @refbackend_consume_func_return_mrf32(%arg0: i64, %arg1: !llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.mlir.constant(1 : index) : i64
    %4 = llvm.alloca %3 x !llvm.struct<(i64, ptr)> : (i64) -> !llvm.ptr
    llvm.store %2, %4 : !llvm.struct<(i64, ptr)>, !llvm.ptr
    llvm.call @_mlir_ciface_refbackend_consume_func_return_mrf32(%4) : (!llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_refbackend_consume_func_return_mrf32(!llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"}
  llvm.func @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @MLP(%arg0: i64, %arg1: !llvm.ptr, %arg2: i64, %arg3: !llvm.ptr, %arg4: i64, %arg5: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = builtin.unrealized_conversion_cast %2 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %4 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %5 = llvm.insertvalue %arg2, %4[0] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.insertvalue %arg3, %5[1] : !llvm.struct<(i64, ptr)> 
    %7 = builtin.unrealized_conversion_cast %6 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %8 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %9 = llvm.insertvalue %arg4, %8[0] : !llvm.struct<(i64, ptr)> 
    %10 = llvm.insertvalue %arg5, %9[1] : !llvm.struct<(i64, ptr)> 
    %11 = builtin.unrealized_conversion_cast %10 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %12 = llvm.mlir.constant(4096 : index) : i64
    %13 = llvm.mlir.constant(1024 : index) : i64
    %14 = llvm.mlir.constant(32 : index) : i64
    %15 = llvm.mlir.constant(8388608 : index) : i64
    %16 = llvm.mlir.constant(0 : index) : i64
    %17 = llvm.mlir.constant(128 : index) : i64
    %18 = llvm.mlir.constant(1 : index) : i64
    %19 = llvm.mlir.constant(262144 : index) : i64
    %20 = llvm.mlir.constant(4 : index) : i64
    %21 = llvm.mlir.constant(8388608 : index) : i64
    %22 = llvm.mlir.constant(1024 : index) : i64
    %23 = llvm.mlir.constant(32 : index) : i64
    %24 = llvm.mlir.constant(2 : index) : i64
    %25 = llvm.mlir.constant(8192 : index) : i64
    %26 = llvm.mlir.constant(0 : index) : i64
    %27 = llvm.mlir.constant(64 : index) : i64
    %28 = llvm.mlir.constant(1 : index) : i64
    %29 = llvm.mlir.constant(128 : index) : i64
    %30 = llvm.mlir.constant(262144 : index) : i64
    %31 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %32 = llvm.mlir.constant(8192 : i64) : i64
    %33 = llvm.mlir.constant(4 : index) : i64
    %34 = llvm.mlir.constant(8192 : index) : i64
    %35 = llvm.mlir.constant(2 : index) : i64
    %36 = llvm.mlir.constant(5 : i64) : i64
    %37 = llvm.mlir.constant(4 : i64) : i64
    %38 = llvm.mlir.constant(1024 : i64) : i64
    %39 = llvm.mlir.constant(0 : i64) : i64
    %40 = llvm.mlir.constant(262144 : i64) : i64
    %41 = llvm.mlir.constant(32 : i64) : i64
    %42 = llvm.mlir.constant(8 : i64) : i64
    %43 = llvm.mlir.constant(128 : i64) : i64
    %44 = llvm.mlir.constant(64 : i64) : i64
    %45 = llvm.mlir.constant(1 : i64) : i64
    %46 = llvm.mlir.constant(2 : i64) : i64
    %47 = builtin.unrealized_conversion_cast %3 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %48 = builtin.unrealized_conversion_cast %7 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %49 = builtin.unrealized_conversion_cast %11 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %50 = llvm.extractvalue %47[1] : !llvm.struct<(i64, ptr)> 
    %51 = llvm.load %50 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %52 = builtin.unrealized_conversion_cast %51 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %53 = llvm.extractvalue %48[1] : !llvm.struct<(i64, ptr)> 
    %54 = llvm.load %53 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %56 = llvm.extractvalue %49[1] : !llvm.struct<(i64, ptr)> 
    %57 = llvm.load %56 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %58 = builtin.unrealized_conversion_cast %57 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %59 = llvm.extractvalue %57[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %60 = llvm.extractvalue %57[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %61 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %62 = llvm.insertvalue %59, %61[0] : !llvm.struct<(ptr, ptr, i64)> 
    %63 = llvm.insertvalue %60, %62[1] : !llvm.struct<(ptr, ptr, i64)> 
    %64 = llvm.mlir.constant(0 : index) : i64
    %65 = llvm.insertvalue %64, %63[2] : !llvm.struct<(ptr, ptr, i64)> 
    %66 = llvm.extractvalue %57[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %67 = llvm.extractvalue %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %68 = llvm.extractvalue %57[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %69 = llvm.extractvalue %57[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %70 = llvm.extractvalue %57[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %71 = llvm.extractvalue %57[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %72 = llvm.extractvalue %57[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %73 = llvm.mlir.zero : !llvm.ptr
    %74 = llvm.getelementptr %73[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %75 = llvm.ptrtoint %74 : !llvm.ptr to i64
    %76 = llvm.add %75, %27  : i64
    %77 = llvm.call @malloc(%76) : (i64) -> !llvm.ptr
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.sub %27, %28  : i64
    %80 = llvm.add %78, %79  : i64
    %81 = llvm.urem %80, %27  : i64
    %82 = llvm.sub %80, %81  : i64
    %83 = llvm.inttoptr %82 : i64 to !llvm.ptr
    %84 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %85 = llvm.insertvalue %77, %84[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %86 = llvm.insertvalue %83, %85[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %87 = llvm.insertvalue %26, %86[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %88 = llvm.insertvalue %30, %87[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %89 = llvm.insertvalue %29, %88[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.insertvalue %29, %89[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.insertvalue %28, %90[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = builtin.unrealized_conversion_cast %91 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    llvm.br ^bb1(%16 : i64)
  ^bb1(%93: i64):  // 2 preds: ^bb0, ^bb5
    %94 = builtin.unrealized_conversion_cast %93 : i64 to index
    %95 = llvm.icmp "slt" %93, %17 : i64
    llvm.cond_br %95, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    %96 = builtin.unrealized_conversion_cast %94 : index to i64
    llvm.br ^bb3(%16 : i64)
  ^bb3(%97: i64):  // 2 preds: ^bb2, ^bb4
    %98 = builtin.unrealized_conversion_cast %97 : i64 to index
    %99 = llvm.icmp "slt" %97, %19 : i64
    llvm.cond_br %99, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %100 = builtin.unrealized_conversion_cast %98 : index to i64
    %101 = llvm.extractvalue %51[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %102 = llvm.mlir.constant(262144 : index) : i64
    %103 = llvm.mul %96, %102  : i64
    %104 = llvm.add %103, %100  : i64
    %105 = llvm.getelementptr %101[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %106 = llvm.load %105 : !llvm.ptr -> f32
    %107 = llvm.extractvalue %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %108 = llvm.mlir.constant(128 : index) : i64
    %109 = llvm.mul %100, %108  : i64
    %110 = llvm.add %109, %96  : i64
    %111 = llvm.getelementptr %107[%110] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %106, %111 : f32, !llvm.ptr
    %112 = llvm.add %97, %18  : i64
    llvm.br ^bb3(%112 : i64)
  ^bb5:  // pred: ^bb3
    %113 = llvm.add %93, %18  : i64
    llvm.br ^bb1(%113 : i64)
  ^bb6:  // pred: ^bb1
    %114 = llvm.mlir.zero : !llvm.ptr
    %115 = llvm.getelementptr %114[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %116 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %117 = llvm.add %116, %27  : i64
    %118 = llvm.call @malloc(%117) : (i64) -> !llvm.ptr
    %119 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %120 = llvm.sub %27, %28  : i64
    %121 = llvm.add %119, %120  : i64
    %122 = llvm.urem %121, %27  : i64
    %123 = llvm.sub %121, %122  : i64
    %124 = llvm.inttoptr %123 : i64 to !llvm.ptr
    %125 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %126 = llvm.insertvalue %118, %125[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %127 = llvm.insertvalue %124, %126[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %128 = llvm.insertvalue %26, %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %129 = llvm.insertvalue %27, %128[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %130 = llvm.insertvalue %29, %129[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.insertvalue %29, %130[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %132 = llvm.insertvalue %28, %131[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = builtin.unrealized_conversion_cast %132 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %134 = llvm.call @xsmm_unary_dispatch(%46, %45, %44, %43, %45, %43, %42) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %135 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %136 = builtin.unrealized_conversion_cast %135 : i64 to index
    %137 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
    llvm.call @xsmm_unary_scalar_invoke(%45, %134, %31, %137, %16) : (i64, i64, f32, !llvm.ptr<f32>, i64) -> ()
    %138 = llvm.mlir.zero : !llvm.ptr
    %139 = llvm.getelementptr %138[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
    %141 = llvm.add %140, %27  : i64
    %142 = llvm.call @malloc(%141) : (i64) -> !llvm.ptr
    %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
    %144 = llvm.sub %27, %28  : i64
    %145 = llvm.add %143, %144  : i64
    %146 = llvm.urem %145, %27  : i64
    %147 = llvm.sub %145, %146  : i64
    %148 = llvm.inttoptr %147 : i64 to !llvm.ptr
    %149 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %150 = llvm.insertvalue %142, %149[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %151 = llvm.insertvalue %148, %150[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %152 = llvm.insertvalue %26, %151[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %153 = llvm.insertvalue %24, %152[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %154 = llvm.insertvalue %25, %153[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %155 = llvm.insertvalue %23, %154[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %156 = llvm.insertvalue %23, %155[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %157 = llvm.insertvalue %21, %156[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %158 = llvm.insertvalue %22, %157[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %159 = llvm.insertvalue %23, %158[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %160 = llvm.insertvalue %28, %159[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %161 = builtin.unrealized_conversion_cast %160 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %162 = llvm.call @xsmm_unary_dispatch(%45, %45, %41, %41, %40, %41, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%35, %34) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.mul %arg6, %15  : i64
        %210 = llvm.mul %arg7, %14  : i64
        %211 = llvm.add %209, %210  : i64
        %212 = builtin.unrealized_conversion_cast %211 : i64 to index
        %213 = builtin.unrealized_conversion_cast %212 : index to i64
        %214 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %215 = llvm.extractvalue %65[0] : !llvm.struct<(ptr, ptr, i64)> 
        %216 = llvm.extractvalue %65[1] : !llvm.struct<(ptr, ptr, i64)> 
        %217 = llvm.insertvalue %215, %214[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.insertvalue %216, %217[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.insertvalue %213, %218[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.mlir.constant(32 : index) : i64
        %221 = llvm.insertvalue %220, %219[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.mlir.constant(262144 : index) : i64
        %223 = llvm.insertvalue %222, %221[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %224 = llvm.mlir.constant(32 : index) : i64
        %225 = llvm.insertvalue %224, %223[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %226 = llvm.mlir.constant(1 : index) : i64
        %227 = llvm.insertvalue %226, %225[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = builtin.unrealized_conversion_cast %227 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
        %229 = builtin.unrealized_conversion_cast %228 : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %230 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %233 = llvm.insertvalue %230, %232[0] : !llvm.struct<(ptr, ptr, i64)> 
        %234 = llvm.insertvalue %231, %233[1] : !llvm.struct<(ptr, ptr, i64)> 
        %235 = llvm.mlir.constant(0 : index) : i64
        %236 = llvm.insertvalue %235, %234[2] : !llvm.struct<(ptr, ptr, i64)> 
        %237 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %238 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %239 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %240 = llvm.extractvalue %160[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %241 = llvm.extractvalue %160[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %242 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %243 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %244 = llvm.extractvalue %160[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %245 = llvm.extractvalue %160[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %246 = llvm.mul %arg6, %15  : i64
        %247 = llvm.mul %arg7, %13  : i64
        %248 = llvm.add %246, %247  : i64
        %249 = builtin.unrealized_conversion_cast %248 : i64 to index
        %250 = builtin.unrealized_conversion_cast %249 : index to i64
        %251 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %252 = llvm.extractvalue %236[0] : !llvm.struct<(ptr, ptr, i64)> 
        %253 = llvm.extractvalue %236[1] : !llvm.struct<(ptr, ptr, i64)> 
        %254 = llvm.insertvalue %252, %251[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %255 = llvm.insertvalue %253, %254[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %256 = llvm.insertvalue %250, %255[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %257 = llvm.mlir.constant(32 : index) : i64
        %258 = llvm.insertvalue %257, %256[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %259 = llvm.mlir.constant(32 : index) : i64
        %260 = llvm.insertvalue %259, %258[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %261 = llvm.mlir.constant(32 : index) : i64
        %262 = llvm.insertvalue %261, %260[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %263 = llvm.mlir.constant(1 : index) : i64
        %264 = llvm.insertvalue %263, %262[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %265 = builtin.unrealized_conversion_cast %264 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %266 = builtin.unrealized_conversion_cast %265 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %267 = llvm.extractvalue %229[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %268 = builtin.unrealized_conversion_cast %267 : i64 to index
        %269 = llvm.extractvalue %229[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %270 = llvm.ptrtoint %269 : !llvm.ptr to i64
        %271 = builtin.unrealized_conversion_cast %270 : i64 to index
        %272 = llvm.inttoptr %270 : i64 to !llvm.ptr<f32>
        %273 = llvm.extractvalue %266[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %274 = builtin.unrealized_conversion_cast %273 : i64 to index
        %275 = llvm.extractvalue %266[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %276 = llvm.ptrtoint %275 : !llvm.ptr to i64
        %277 = builtin.unrealized_conversion_cast %276 : i64 to index
        %278 = llvm.inttoptr %276 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %162, %272, %267, %278, %273) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %163 = llvm.mlir.zero : !llvm.ptr
    %164 = llvm.getelementptr %163[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %165 = llvm.ptrtoint %164 : !llvm.ptr to i64
    %166 = llvm.add %165, %27  : i64
    %167 = llvm.call @malloc(%166) : (i64) -> !llvm.ptr
    %168 = llvm.ptrtoint %167 : !llvm.ptr to i64
    %169 = llvm.sub %27, %28  : i64
    %170 = llvm.add %168, %169  : i64
    %171 = llvm.urem %170, %27  : i64
    %172 = llvm.sub %170, %171  : i64
    %173 = llvm.inttoptr %172 : i64 to !llvm.ptr
    %174 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %175 = llvm.insertvalue %167, %174[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %176 = llvm.insertvalue %173, %175[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %177 = llvm.insertvalue %26, %176[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %178 = llvm.insertvalue %20, %177[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %179 = llvm.insertvalue %25, %178[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %180 = llvm.insertvalue %23, %179[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %181 = llvm.insertvalue %23, %180[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %182 = llvm.insertvalue %21, %181[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %183 = llvm.insertvalue %22, %182[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %184 = llvm.insertvalue %23, %183[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %185 = llvm.insertvalue %28, %184[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %186 = builtin.unrealized_conversion_cast %185 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %187 = llvm.call @xsmm_unary_dispatch(%45, %45, %41, %41, %43, %41, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%33, %34) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.extractvalue %91[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %210 = llvm.extractvalue %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %211 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %212 = llvm.insertvalue %209, %211[0] : !llvm.struct<(ptr, ptr, i64)> 
        %213 = llvm.insertvalue %210, %212[1] : !llvm.struct<(ptr, ptr, i64)> 
        %214 = llvm.mlir.constant(0 : index) : i64
        %215 = llvm.insertvalue %214, %213[2] : !llvm.struct<(ptr, ptr, i64)> 
        %216 = llvm.extractvalue %91[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = llvm.extractvalue %91[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.extractvalue %91[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.extractvalue %91[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.extractvalue %91[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %221 = llvm.mul %arg7, %12  : i64
        %222 = llvm.mul %arg6, %14  : i64
        %223 = llvm.add %221, %222  : i64
        %224 = builtin.unrealized_conversion_cast %223 : i64 to index
        %225 = builtin.unrealized_conversion_cast %224 : index to i64
        %226 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %227 = llvm.extractvalue %215[0] : !llvm.struct<(ptr, ptr, i64)> 
        %228 = llvm.extractvalue %215[1] : !llvm.struct<(ptr, ptr, i64)> 
        %229 = llvm.insertvalue %227, %226[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %230 = llvm.insertvalue %228, %229[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %231 = llvm.insertvalue %225, %230[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %232 = llvm.mlir.constant(32 : index) : i64
        %233 = llvm.insertvalue %232, %231[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %234 = llvm.mlir.constant(128 : index) : i64
        %235 = llvm.insertvalue %234, %233[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %236 = llvm.mlir.constant(32 : index) : i64
        %237 = llvm.insertvalue %236, %235[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %238 = llvm.mlir.constant(1 : index) : i64
        %239 = llvm.insertvalue %238, %237[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %240 = builtin.unrealized_conversion_cast %239 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %241 = builtin.unrealized_conversion_cast %240 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %242 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %243 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %244 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %245 = llvm.insertvalue %242, %244[0] : !llvm.struct<(ptr, ptr, i64)> 
        %246 = llvm.insertvalue %243, %245[1] : !llvm.struct<(ptr, ptr, i64)> 
        %247 = llvm.mlir.constant(0 : index) : i64
        %248 = llvm.insertvalue %247, %246[2] : !llvm.struct<(ptr, ptr, i64)> 
        %249 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %250 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %251 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %252 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %253 = llvm.extractvalue %185[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %254 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %255 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %256 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %257 = llvm.extractvalue %185[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %258 = llvm.mul %arg6, %15  : i64
        %259 = llvm.mul %arg7, %13  : i64
        %260 = llvm.add %258, %259  : i64
        %261 = builtin.unrealized_conversion_cast %260 : i64 to index
        %262 = builtin.unrealized_conversion_cast %261 : index to i64
        %263 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %264 = llvm.extractvalue %248[0] : !llvm.struct<(ptr, ptr, i64)> 
        %265 = llvm.extractvalue %248[1] : !llvm.struct<(ptr, ptr, i64)> 
        %266 = llvm.insertvalue %264, %263[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %267 = llvm.insertvalue %265, %266[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %268 = llvm.insertvalue %262, %267[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %269 = llvm.mlir.constant(32 : index) : i64
        %270 = llvm.insertvalue %269, %268[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %271 = llvm.mlir.constant(32 : index) : i64
        %272 = llvm.insertvalue %271, %270[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %273 = llvm.mlir.constant(32 : index) : i64
        %274 = llvm.insertvalue %273, %272[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %275 = llvm.mlir.constant(1 : index) : i64
        %276 = llvm.insertvalue %275, %274[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %277 = builtin.unrealized_conversion_cast %276 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %278 = builtin.unrealized_conversion_cast %277 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %279 = llvm.extractvalue %241[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %280 = builtin.unrealized_conversion_cast %279 : i64 to index
        %281 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %282 = llvm.ptrtoint %281 : !llvm.ptr to i64
        %283 = builtin.unrealized_conversion_cast %282 : i64 to index
        %284 = llvm.inttoptr %282 : i64 to !llvm.ptr<f32>
        %285 = llvm.extractvalue %278[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %286 = builtin.unrealized_conversion_cast %285 : i64 to index
        %287 = llvm.extractvalue %278[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %288 = llvm.ptrtoint %287 : !llvm.ptr to i64
        %289 = builtin.unrealized_conversion_cast %288 : i64 to index
        %290 = llvm.inttoptr %288 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %187, %284, %279, %290, %285) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %188 = llvm.extractvalue %54[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %189 = llvm.extractvalue %54[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %190 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %191 = llvm.insertvalue %188, %190[0] : !llvm.struct<(ptr, ptr, i64)> 
    %192 = llvm.insertvalue %189, %191[1] : !llvm.struct<(ptr, ptr, i64)> 
    %193 = llvm.mlir.constant(0 : index) : i64
    %194 = llvm.insertvalue %193, %192[2] : !llvm.struct<(ptr, ptr, i64)> 
    %195 = llvm.extractvalue %54[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %196 = llvm.extractvalue %54[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %197 = llvm.extractvalue %54[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %198 = llvm.call @xsmm_brgemm_dispatch(%45, %41, %41, %41, %41, %41, %41, %38, %38, %37) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %199 = llvm.call @xsmm_binary_dispatch(%45, %45, %41, %41, %41, %41, %41, %37) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %200 = llvm.call @xsmm_unary_dispatch(%36, %45, %41, %41, %41, %43, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%35, %33) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.mlir.zero : !llvm.ptr
        %210 = llvm.getelementptr %209[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %211 = llvm.ptrtoint %210 : !llvm.ptr to i64
        %212 = llvm.add %211, %27  : i64
        %213 = llvm.call @malloc(%212) : (i64) -> !llvm.ptr
        %214 = llvm.ptrtoint %213 : !llvm.ptr to i64
        %215 = llvm.sub %27, %28  : i64
        %216 = llvm.add %214, %215  : i64
        %217 = llvm.urem %216, %27  : i64
        %218 = llvm.sub %216, %217  : i64
        %219 = llvm.inttoptr %218 : i64 to !llvm.ptr
        %220 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %221 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %222 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %223 = llvm.insertvalue %220, %222[0] : !llvm.struct<(ptr, ptr, i64)> 
        %224 = llvm.insertvalue %221, %223[1] : !llvm.struct<(ptr, ptr, i64)> 
        %225 = llvm.mlir.constant(0 : index) : i64
        %226 = llvm.insertvalue %225, %224[2] : !llvm.struct<(ptr, ptr, i64)> 
        %227 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %228 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %229 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %230 = llvm.extractvalue %160[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %160[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %233 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %234 = llvm.extractvalue %160[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %235 = llvm.extractvalue %160[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %236 = llvm.mul %arg6, %15  : i64
        %237 = builtin.unrealized_conversion_cast %236 : i64 to index
        %238 = builtin.unrealized_conversion_cast %237 : index to i64
        %239 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %240 = llvm.extractvalue %226[0] : !llvm.struct<(ptr, ptr, i64)> 
        %241 = llvm.extractvalue %226[1] : !llvm.struct<(ptr, ptr, i64)> 
        %242 = llvm.insertvalue %240, %239[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %243 = llvm.insertvalue %241, %242[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %244 = llvm.insertvalue %238, %243[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %245 = llvm.mlir.constant(8192 : index) : i64
        %246 = llvm.insertvalue %245, %244[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %247 = llvm.mlir.constant(1024 : index) : i64
        %248 = llvm.insertvalue %247, %246[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %249 = llvm.mlir.constant(32 : index) : i64
        %250 = llvm.insertvalue %249, %248[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %251 = llvm.mlir.constant(32 : index) : i64
        %252 = llvm.insertvalue %251, %250[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %253 = llvm.mlir.constant(32 : index) : i64
        %254 = llvm.insertvalue %253, %252[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %255 = llvm.mlir.constant(1 : index) : i64
        %256 = llvm.insertvalue %255, %254[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %257 = builtin.unrealized_conversion_cast %256 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %258 = builtin.unrealized_conversion_cast %257 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %259 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %260 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %261 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %262 = llvm.insertvalue %259, %261[0] : !llvm.struct<(ptr, ptr, i64)> 
        %263 = llvm.insertvalue %260, %262[1] : !llvm.struct<(ptr, ptr, i64)> 
        %264 = llvm.mlir.constant(0 : index) : i64
        %265 = llvm.insertvalue %264, %263[2] : !llvm.struct<(ptr, ptr, i64)> 
        %266 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %267 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %268 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %269 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %270 = llvm.extractvalue %185[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %271 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %272 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %273 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %274 = llvm.extractvalue %185[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %275 = llvm.mul %arg7, %15  : i64
        %276 = builtin.unrealized_conversion_cast %275 : i64 to index
        %277 = builtin.unrealized_conversion_cast %276 : index to i64
        %278 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %279 = llvm.extractvalue %265[0] : !llvm.struct<(ptr, ptr, i64)> 
        %280 = llvm.extractvalue %265[1] : !llvm.struct<(ptr, ptr, i64)> 
        %281 = llvm.insertvalue %279, %278[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %282 = llvm.insertvalue %280, %281[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %283 = llvm.insertvalue %277, %282[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %284 = llvm.mlir.constant(8192 : index) : i64
        %285 = llvm.insertvalue %284, %283[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %286 = llvm.mlir.constant(1024 : index) : i64
        %287 = llvm.insertvalue %286, %285[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %288 = llvm.mlir.constant(32 : index) : i64
        %289 = llvm.insertvalue %288, %287[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %290 = llvm.mlir.constant(32 : index) : i64
        %291 = llvm.insertvalue %290, %289[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %292 = llvm.mlir.constant(32 : index) : i64
        %293 = llvm.insertvalue %292, %291[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %294 = llvm.mlir.constant(1 : index) : i64
        %295 = llvm.insertvalue %294, %293[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %296 = builtin.unrealized_conversion_cast %295 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %297 = builtin.unrealized_conversion_cast %296 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %298 = llvm.extractvalue %258[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %299 = builtin.unrealized_conversion_cast %298 : i64 to index
        %300 = llvm.extractvalue %258[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %301 = llvm.ptrtoint %300 : !llvm.ptr to i64
        %302 = builtin.unrealized_conversion_cast %301 : i64 to index
        %303 = llvm.inttoptr %301 : i64 to !llvm.ptr<f32>
        %304 = llvm.extractvalue %297[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %305 = builtin.unrealized_conversion_cast %304 : i64 to index
        %306 = llvm.extractvalue %297[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %307 = llvm.ptrtoint %306 : !llvm.ptr to i64
        %308 = builtin.unrealized_conversion_cast %307 : i64 to index
        %309 = llvm.inttoptr %307 : i64 to !llvm.ptr<f32>
        %310 = llvm.ptrtoint %219 : !llvm.ptr to i64
        %311 = builtin.unrealized_conversion_cast %310 : i64 to index
        %312 = llvm.inttoptr %310 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_brgemm_invoke(%45, %198, %303, %298, %309, %304, %312, %16, %32) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) -> ()
        %313 = llvm.mul %arg7, %14  : i64
        %314 = builtin.unrealized_conversion_cast %313 : i64 to index
        %315 = builtin.unrealized_conversion_cast %314 : index to i64
        %316 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %317 = llvm.extractvalue %194[0] : !llvm.struct<(ptr, ptr, i64)> 
        %318 = llvm.extractvalue %194[1] : !llvm.struct<(ptr, ptr, i64)> 
        %319 = llvm.insertvalue %317, %316[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %320 = llvm.insertvalue %318, %319[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %321 = llvm.insertvalue %315, %320[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %322 = llvm.mlir.constant(32 : index) : i64
        %323 = llvm.insertvalue %322, %321[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %324 = llvm.mlir.constant(1 : index) : i64
        %325 = llvm.insertvalue %324, %323[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %326 = builtin.unrealized_conversion_cast %325 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<32xf32, strided<[1], offset: ?>>
        %327 = builtin.unrealized_conversion_cast %326 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %328 = llvm.extractvalue %327[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %329 = builtin.unrealized_conversion_cast %328 : i64 to index
        %330 = llvm.extractvalue %327[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %331 = llvm.ptrtoint %330 : !llvm.ptr to i64
        %332 = builtin.unrealized_conversion_cast %331 : i64 to index
        %333 = llvm.inttoptr %331 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_binary_invoke(%45, %199, %333, %328, %312, %16, %312, %16) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        %334 = llvm.extractvalue %132[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %335 = llvm.extractvalue %132[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %336 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %337 = llvm.insertvalue %334, %336[0] : !llvm.struct<(ptr, ptr, i64)> 
        %338 = llvm.insertvalue %335, %337[1] : !llvm.struct<(ptr, ptr, i64)> 
        %339 = llvm.mlir.constant(0 : index) : i64
        %340 = llvm.insertvalue %339, %338[2] : !llvm.struct<(ptr, ptr, i64)> 
        %341 = llvm.extractvalue %132[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %342 = llvm.extractvalue %132[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %343 = llvm.extractvalue %132[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %344 = llvm.extractvalue %132[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %345 = llvm.extractvalue %132[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %346 = llvm.mul %arg6, %12  : i64
        %347 = llvm.mul %arg7, %14  : i64
        %348 = llvm.add %346, %347  : i64
        %349 = builtin.unrealized_conversion_cast %348 : i64 to index
        %350 = builtin.unrealized_conversion_cast %349 : index to i64
        %351 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %352 = llvm.extractvalue %340[0] : !llvm.struct<(ptr, ptr, i64)> 
        %353 = llvm.extractvalue %340[1] : !llvm.struct<(ptr, ptr, i64)> 
        %354 = llvm.insertvalue %352, %351[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %355 = llvm.insertvalue %353, %354[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %356 = llvm.insertvalue %350, %355[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %357 = llvm.mlir.constant(32 : index) : i64
        %358 = llvm.insertvalue %357, %356[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %359 = llvm.mlir.constant(128 : index) : i64
        %360 = llvm.insertvalue %359, %358[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %361 = llvm.mlir.constant(32 : index) : i64
        %362 = llvm.insertvalue %361, %360[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %363 = llvm.mlir.constant(1 : index) : i64
        %364 = llvm.insertvalue %363, %362[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %365 = builtin.unrealized_conversion_cast %364 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %366 = builtin.unrealized_conversion_cast %365 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %367 = llvm.extractvalue %366[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %368 = builtin.unrealized_conversion_cast %367 : i64 to index
        %369 = llvm.extractvalue %366[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %370 = llvm.ptrtoint %369 : !llvm.ptr to i64
        %371 = builtin.unrealized_conversion_cast %370 : i64 to index
        %372 = llvm.inttoptr %370 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %200, %312, %16, %372, %367) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.call @free(%213) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%77) : (!llvm.ptr) -> ()
    llvm.call @free(%142) : (!llvm.ptr) -> ()
    llvm.call @free(%167) : (!llvm.ptr) -> ()
    %201 = llvm.alloca %28 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %132, %201 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %202 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %203 = llvm.insertvalue %24, %202[0] : !llvm.struct<(i64, ptr)> 
    %204 = llvm.insertvalue %201, %203[1] : !llvm.struct<(i64, ptr)> 
    %205 = builtin.unrealized_conversion_cast %204 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %206 = llvm.extractvalue %204[0] : !llvm.struct<(i64, ptr)> 
    %207 = llvm.extractvalue %204[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @refbackend_consume_func_return_mrf32(%206, %207) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_MLP(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg0 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %4 = llvm.extractvalue %3[0] : !llvm.struct<(i64, ptr)> 
    %5 = llvm.extractvalue %3[1] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(i64, ptr)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @MLP(%1, %2, %4, %5, %7, %8) : (i64, !llvm.ptr, i64, !llvm.ptr, i64, !llvm.ptr) -> ()
    llvm.return
  }
}


// -----// IR Dump Before ConvertMathToLLVMPass (convert-math-to-llvm) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func private @refbackend_consume_func_return_mrf32(%arg0: i64, %arg1: !llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.mlir.constant(1 : index) : i64
    %4 = llvm.alloca %3 x !llvm.struct<(i64, ptr)> : (i64) -> !llvm.ptr
    llvm.store %2, %4 : !llvm.struct<(i64, ptr)>, !llvm.ptr
    llvm.call @_mlir_ciface_refbackend_consume_func_return_mrf32(%4) : (!llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_refbackend_consume_func_return_mrf32(!llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"}
  llvm.func @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @MLP(%arg0: i64, %arg1: !llvm.ptr, %arg2: i64, %arg3: !llvm.ptr, %arg4: i64, %arg5: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = builtin.unrealized_conversion_cast %2 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %4 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %5 = llvm.insertvalue %arg2, %4[0] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.insertvalue %arg3, %5[1] : !llvm.struct<(i64, ptr)> 
    %7 = builtin.unrealized_conversion_cast %6 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %8 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %9 = llvm.insertvalue %arg4, %8[0] : !llvm.struct<(i64, ptr)> 
    %10 = llvm.insertvalue %arg5, %9[1] : !llvm.struct<(i64, ptr)> 
    %11 = builtin.unrealized_conversion_cast %10 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %12 = llvm.mlir.constant(4096 : index) : i64
    %13 = llvm.mlir.constant(1024 : index) : i64
    %14 = llvm.mlir.constant(32 : index) : i64
    %15 = llvm.mlir.constant(8388608 : index) : i64
    %16 = llvm.mlir.constant(0 : index) : i64
    %17 = llvm.mlir.constant(128 : index) : i64
    %18 = llvm.mlir.constant(1 : index) : i64
    %19 = llvm.mlir.constant(262144 : index) : i64
    %20 = llvm.mlir.constant(4 : index) : i64
    %21 = llvm.mlir.constant(8388608 : index) : i64
    %22 = llvm.mlir.constant(1024 : index) : i64
    %23 = llvm.mlir.constant(32 : index) : i64
    %24 = llvm.mlir.constant(2 : index) : i64
    %25 = llvm.mlir.constant(8192 : index) : i64
    %26 = llvm.mlir.constant(0 : index) : i64
    %27 = llvm.mlir.constant(64 : index) : i64
    %28 = llvm.mlir.constant(1 : index) : i64
    %29 = llvm.mlir.constant(128 : index) : i64
    %30 = llvm.mlir.constant(262144 : index) : i64
    %31 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %32 = llvm.mlir.constant(8192 : i64) : i64
    %33 = llvm.mlir.constant(4 : index) : i64
    %34 = llvm.mlir.constant(8192 : index) : i64
    %35 = llvm.mlir.constant(2 : index) : i64
    %36 = llvm.mlir.constant(5 : i64) : i64
    %37 = llvm.mlir.constant(4 : i64) : i64
    %38 = llvm.mlir.constant(1024 : i64) : i64
    %39 = llvm.mlir.constant(0 : i64) : i64
    %40 = llvm.mlir.constant(262144 : i64) : i64
    %41 = llvm.mlir.constant(32 : i64) : i64
    %42 = llvm.mlir.constant(8 : i64) : i64
    %43 = llvm.mlir.constant(128 : i64) : i64
    %44 = llvm.mlir.constant(64 : i64) : i64
    %45 = llvm.mlir.constant(1 : i64) : i64
    %46 = llvm.mlir.constant(2 : i64) : i64
    %47 = builtin.unrealized_conversion_cast %3 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %48 = builtin.unrealized_conversion_cast %7 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %49 = builtin.unrealized_conversion_cast %11 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %50 = llvm.extractvalue %47[1] : !llvm.struct<(i64, ptr)> 
    %51 = llvm.load %50 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %52 = builtin.unrealized_conversion_cast %51 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %53 = llvm.extractvalue %48[1] : !llvm.struct<(i64, ptr)> 
    %54 = llvm.load %53 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %56 = llvm.extractvalue %49[1] : !llvm.struct<(i64, ptr)> 
    %57 = llvm.load %56 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %58 = builtin.unrealized_conversion_cast %57 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %59 = llvm.extractvalue %57[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %60 = llvm.extractvalue %57[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %61 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %62 = llvm.insertvalue %59, %61[0] : !llvm.struct<(ptr, ptr, i64)> 
    %63 = llvm.insertvalue %60, %62[1] : !llvm.struct<(ptr, ptr, i64)> 
    %64 = llvm.mlir.constant(0 : index) : i64
    %65 = llvm.insertvalue %64, %63[2] : !llvm.struct<(ptr, ptr, i64)> 
    %66 = llvm.extractvalue %57[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %67 = llvm.extractvalue %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %68 = llvm.extractvalue %57[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %69 = llvm.extractvalue %57[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %70 = llvm.extractvalue %57[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %71 = llvm.extractvalue %57[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %72 = llvm.extractvalue %57[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %73 = llvm.mlir.zero : !llvm.ptr
    %74 = llvm.getelementptr %73[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %75 = llvm.ptrtoint %74 : !llvm.ptr to i64
    %76 = llvm.add %75, %27  : i64
    %77 = llvm.call @malloc(%76) : (i64) -> !llvm.ptr
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.sub %27, %28  : i64
    %80 = llvm.add %78, %79  : i64
    %81 = llvm.urem %80, %27  : i64
    %82 = llvm.sub %80, %81  : i64
    %83 = llvm.inttoptr %82 : i64 to !llvm.ptr
    %84 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %85 = llvm.insertvalue %77, %84[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %86 = llvm.insertvalue %83, %85[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %87 = llvm.insertvalue %26, %86[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %88 = llvm.insertvalue %30, %87[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %89 = llvm.insertvalue %29, %88[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.insertvalue %29, %89[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.insertvalue %28, %90[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = builtin.unrealized_conversion_cast %91 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    llvm.br ^bb1(%16 : i64)
  ^bb1(%93: i64):  // 2 preds: ^bb0, ^bb5
    %94 = builtin.unrealized_conversion_cast %93 : i64 to index
    %95 = llvm.icmp "slt" %93, %17 : i64
    llvm.cond_br %95, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    %96 = builtin.unrealized_conversion_cast %94 : index to i64
    llvm.br ^bb3(%16 : i64)
  ^bb3(%97: i64):  // 2 preds: ^bb2, ^bb4
    %98 = builtin.unrealized_conversion_cast %97 : i64 to index
    %99 = llvm.icmp "slt" %97, %19 : i64
    llvm.cond_br %99, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %100 = builtin.unrealized_conversion_cast %98 : index to i64
    %101 = llvm.extractvalue %51[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %102 = llvm.mlir.constant(262144 : index) : i64
    %103 = llvm.mul %96, %102  : i64
    %104 = llvm.add %103, %100  : i64
    %105 = llvm.getelementptr %101[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %106 = llvm.load %105 : !llvm.ptr -> f32
    %107 = llvm.extractvalue %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %108 = llvm.mlir.constant(128 : index) : i64
    %109 = llvm.mul %100, %108  : i64
    %110 = llvm.add %109, %96  : i64
    %111 = llvm.getelementptr %107[%110] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %106, %111 : f32, !llvm.ptr
    %112 = llvm.add %97, %18  : i64
    llvm.br ^bb3(%112 : i64)
  ^bb5:  // pred: ^bb3
    %113 = llvm.add %93, %18  : i64
    llvm.br ^bb1(%113 : i64)
  ^bb6:  // pred: ^bb1
    %114 = llvm.mlir.zero : !llvm.ptr
    %115 = llvm.getelementptr %114[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %116 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %117 = llvm.add %116, %27  : i64
    %118 = llvm.call @malloc(%117) : (i64) -> !llvm.ptr
    %119 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %120 = llvm.sub %27, %28  : i64
    %121 = llvm.add %119, %120  : i64
    %122 = llvm.urem %121, %27  : i64
    %123 = llvm.sub %121, %122  : i64
    %124 = llvm.inttoptr %123 : i64 to !llvm.ptr
    %125 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %126 = llvm.insertvalue %118, %125[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %127 = llvm.insertvalue %124, %126[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %128 = llvm.insertvalue %26, %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %129 = llvm.insertvalue %27, %128[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %130 = llvm.insertvalue %29, %129[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.insertvalue %29, %130[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %132 = llvm.insertvalue %28, %131[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = builtin.unrealized_conversion_cast %132 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %134 = llvm.call @xsmm_unary_dispatch(%46, %45, %44, %43, %45, %43, %42) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %135 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %136 = builtin.unrealized_conversion_cast %135 : i64 to index
    %137 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
    llvm.call @xsmm_unary_scalar_invoke(%45, %134, %31, %137, %16) : (i64, i64, f32, !llvm.ptr<f32>, i64) -> ()
    %138 = llvm.mlir.zero : !llvm.ptr
    %139 = llvm.getelementptr %138[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
    %141 = llvm.add %140, %27  : i64
    %142 = llvm.call @malloc(%141) : (i64) -> !llvm.ptr
    %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
    %144 = llvm.sub %27, %28  : i64
    %145 = llvm.add %143, %144  : i64
    %146 = llvm.urem %145, %27  : i64
    %147 = llvm.sub %145, %146  : i64
    %148 = llvm.inttoptr %147 : i64 to !llvm.ptr
    %149 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %150 = llvm.insertvalue %142, %149[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %151 = llvm.insertvalue %148, %150[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %152 = llvm.insertvalue %26, %151[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %153 = llvm.insertvalue %24, %152[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %154 = llvm.insertvalue %25, %153[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %155 = llvm.insertvalue %23, %154[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %156 = llvm.insertvalue %23, %155[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %157 = llvm.insertvalue %21, %156[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %158 = llvm.insertvalue %22, %157[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %159 = llvm.insertvalue %23, %158[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %160 = llvm.insertvalue %28, %159[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %161 = builtin.unrealized_conversion_cast %160 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %162 = llvm.call @xsmm_unary_dispatch(%45, %45, %41, %41, %40, %41, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%35, %34) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.mul %arg6, %15  : i64
        %210 = llvm.mul %arg7, %14  : i64
        %211 = llvm.add %209, %210  : i64
        %212 = builtin.unrealized_conversion_cast %211 : i64 to index
        %213 = builtin.unrealized_conversion_cast %212 : index to i64
        %214 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %215 = llvm.extractvalue %65[0] : !llvm.struct<(ptr, ptr, i64)> 
        %216 = llvm.extractvalue %65[1] : !llvm.struct<(ptr, ptr, i64)> 
        %217 = llvm.insertvalue %215, %214[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.insertvalue %216, %217[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.insertvalue %213, %218[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.mlir.constant(32 : index) : i64
        %221 = llvm.insertvalue %220, %219[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.mlir.constant(262144 : index) : i64
        %223 = llvm.insertvalue %222, %221[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %224 = llvm.mlir.constant(32 : index) : i64
        %225 = llvm.insertvalue %224, %223[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %226 = llvm.mlir.constant(1 : index) : i64
        %227 = llvm.insertvalue %226, %225[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = builtin.unrealized_conversion_cast %227 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
        %229 = builtin.unrealized_conversion_cast %228 : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %230 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %233 = llvm.insertvalue %230, %232[0] : !llvm.struct<(ptr, ptr, i64)> 
        %234 = llvm.insertvalue %231, %233[1] : !llvm.struct<(ptr, ptr, i64)> 
        %235 = llvm.mlir.constant(0 : index) : i64
        %236 = llvm.insertvalue %235, %234[2] : !llvm.struct<(ptr, ptr, i64)> 
        %237 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %238 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %239 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %240 = llvm.extractvalue %160[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %241 = llvm.extractvalue %160[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %242 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %243 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %244 = llvm.extractvalue %160[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %245 = llvm.extractvalue %160[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %246 = llvm.mul %arg6, %15  : i64
        %247 = llvm.mul %arg7, %13  : i64
        %248 = llvm.add %246, %247  : i64
        %249 = builtin.unrealized_conversion_cast %248 : i64 to index
        %250 = builtin.unrealized_conversion_cast %249 : index to i64
        %251 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %252 = llvm.extractvalue %236[0] : !llvm.struct<(ptr, ptr, i64)> 
        %253 = llvm.extractvalue %236[1] : !llvm.struct<(ptr, ptr, i64)> 
        %254 = llvm.insertvalue %252, %251[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %255 = llvm.insertvalue %253, %254[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %256 = llvm.insertvalue %250, %255[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %257 = llvm.mlir.constant(32 : index) : i64
        %258 = llvm.insertvalue %257, %256[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %259 = llvm.mlir.constant(32 : index) : i64
        %260 = llvm.insertvalue %259, %258[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %261 = llvm.mlir.constant(32 : index) : i64
        %262 = llvm.insertvalue %261, %260[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %263 = llvm.mlir.constant(1 : index) : i64
        %264 = llvm.insertvalue %263, %262[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %265 = builtin.unrealized_conversion_cast %264 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %266 = builtin.unrealized_conversion_cast %265 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %267 = llvm.extractvalue %229[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %268 = builtin.unrealized_conversion_cast %267 : i64 to index
        %269 = llvm.extractvalue %229[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %270 = llvm.ptrtoint %269 : !llvm.ptr to i64
        %271 = builtin.unrealized_conversion_cast %270 : i64 to index
        %272 = llvm.inttoptr %270 : i64 to !llvm.ptr<f32>
        %273 = llvm.extractvalue %266[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %274 = builtin.unrealized_conversion_cast %273 : i64 to index
        %275 = llvm.extractvalue %266[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %276 = llvm.ptrtoint %275 : !llvm.ptr to i64
        %277 = builtin.unrealized_conversion_cast %276 : i64 to index
        %278 = llvm.inttoptr %276 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %162, %272, %267, %278, %273) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %163 = llvm.mlir.zero : !llvm.ptr
    %164 = llvm.getelementptr %163[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %165 = llvm.ptrtoint %164 : !llvm.ptr to i64
    %166 = llvm.add %165, %27  : i64
    %167 = llvm.call @malloc(%166) : (i64) -> !llvm.ptr
    %168 = llvm.ptrtoint %167 : !llvm.ptr to i64
    %169 = llvm.sub %27, %28  : i64
    %170 = llvm.add %168, %169  : i64
    %171 = llvm.urem %170, %27  : i64
    %172 = llvm.sub %170, %171  : i64
    %173 = llvm.inttoptr %172 : i64 to !llvm.ptr
    %174 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %175 = llvm.insertvalue %167, %174[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %176 = llvm.insertvalue %173, %175[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %177 = llvm.insertvalue %26, %176[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %178 = llvm.insertvalue %20, %177[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %179 = llvm.insertvalue %25, %178[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %180 = llvm.insertvalue %23, %179[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %181 = llvm.insertvalue %23, %180[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %182 = llvm.insertvalue %21, %181[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %183 = llvm.insertvalue %22, %182[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %184 = llvm.insertvalue %23, %183[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %185 = llvm.insertvalue %28, %184[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %186 = builtin.unrealized_conversion_cast %185 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %187 = llvm.call @xsmm_unary_dispatch(%45, %45, %41, %41, %43, %41, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%33, %34) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.extractvalue %91[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %210 = llvm.extractvalue %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %211 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %212 = llvm.insertvalue %209, %211[0] : !llvm.struct<(ptr, ptr, i64)> 
        %213 = llvm.insertvalue %210, %212[1] : !llvm.struct<(ptr, ptr, i64)> 
        %214 = llvm.mlir.constant(0 : index) : i64
        %215 = llvm.insertvalue %214, %213[2] : !llvm.struct<(ptr, ptr, i64)> 
        %216 = llvm.extractvalue %91[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = llvm.extractvalue %91[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.extractvalue %91[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.extractvalue %91[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.extractvalue %91[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %221 = llvm.mul %arg7, %12  : i64
        %222 = llvm.mul %arg6, %14  : i64
        %223 = llvm.add %221, %222  : i64
        %224 = builtin.unrealized_conversion_cast %223 : i64 to index
        %225 = builtin.unrealized_conversion_cast %224 : index to i64
        %226 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %227 = llvm.extractvalue %215[0] : !llvm.struct<(ptr, ptr, i64)> 
        %228 = llvm.extractvalue %215[1] : !llvm.struct<(ptr, ptr, i64)> 
        %229 = llvm.insertvalue %227, %226[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %230 = llvm.insertvalue %228, %229[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %231 = llvm.insertvalue %225, %230[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %232 = llvm.mlir.constant(32 : index) : i64
        %233 = llvm.insertvalue %232, %231[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %234 = llvm.mlir.constant(128 : index) : i64
        %235 = llvm.insertvalue %234, %233[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %236 = llvm.mlir.constant(32 : index) : i64
        %237 = llvm.insertvalue %236, %235[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %238 = llvm.mlir.constant(1 : index) : i64
        %239 = llvm.insertvalue %238, %237[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %240 = builtin.unrealized_conversion_cast %239 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %241 = builtin.unrealized_conversion_cast %240 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %242 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %243 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %244 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %245 = llvm.insertvalue %242, %244[0] : !llvm.struct<(ptr, ptr, i64)> 
        %246 = llvm.insertvalue %243, %245[1] : !llvm.struct<(ptr, ptr, i64)> 
        %247 = llvm.mlir.constant(0 : index) : i64
        %248 = llvm.insertvalue %247, %246[2] : !llvm.struct<(ptr, ptr, i64)> 
        %249 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %250 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %251 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %252 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %253 = llvm.extractvalue %185[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %254 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %255 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %256 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %257 = llvm.extractvalue %185[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %258 = llvm.mul %arg6, %15  : i64
        %259 = llvm.mul %arg7, %13  : i64
        %260 = llvm.add %258, %259  : i64
        %261 = builtin.unrealized_conversion_cast %260 : i64 to index
        %262 = builtin.unrealized_conversion_cast %261 : index to i64
        %263 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %264 = llvm.extractvalue %248[0] : !llvm.struct<(ptr, ptr, i64)> 
        %265 = llvm.extractvalue %248[1] : !llvm.struct<(ptr, ptr, i64)> 
        %266 = llvm.insertvalue %264, %263[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %267 = llvm.insertvalue %265, %266[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %268 = llvm.insertvalue %262, %267[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %269 = llvm.mlir.constant(32 : index) : i64
        %270 = llvm.insertvalue %269, %268[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %271 = llvm.mlir.constant(32 : index) : i64
        %272 = llvm.insertvalue %271, %270[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %273 = llvm.mlir.constant(32 : index) : i64
        %274 = llvm.insertvalue %273, %272[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %275 = llvm.mlir.constant(1 : index) : i64
        %276 = llvm.insertvalue %275, %274[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %277 = builtin.unrealized_conversion_cast %276 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %278 = builtin.unrealized_conversion_cast %277 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %279 = llvm.extractvalue %241[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %280 = builtin.unrealized_conversion_cast %279 : i64 to index
        %281 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %282 = llvm.ptrtoint %281 : !llvm.ptr to i64
        %283 = builtin.unrealized_conversion_cast %282 : i64 to index
        %284 = llvm.inttoptr %282 : i64 to !llvm.ptr<f32>
        %285 = llvm.extractvalue %278[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %286 = builtin.unrealized_conversion_cast %285 : i64 to index
        %287 = llvm.extractvalue %278[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %288 = llvm.ptrtoint %287 : !llvm.ptr to i64
        %289 = builtin.unrealized_conversion_cast %288 : i64 to index
        %290 = llvm.inttoptr %288 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %187, %284, %279, %290, %285) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %188 = llvm.extractvalue %54[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %189 = llvm.extractvalue %54[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %190 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %191 = llvm.insertvalue %188, %190[0] : !llvm.struct<(ptr, ptr, i64)> 
    %192 = llvm.insertvalue %189, %191[1] : !llvm.struct<(ptr, ptr, i64)> 
    %193 = llvm.mlir.constant(0 : index) : i64
    %194 = llvm.insertvalue %193, %192[2] : !llvm.struct<(ptr, ptr, i64)> 
    %195 = llvm.extractvalue %54[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %196 = llvm.extractvalue %54[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %197 = llvm.extractvalue %54[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %198 = llvm.call @xsmm_brgemm_dispatch(%45, %41, %41, %41, %41, %41, %41, %38, %38, %37) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %199 = llvm.call @xsmm_binary_dispatch(%45, %45, %41, %41, %41, %41, %41, %37) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %200 = llvm.call @xsmm_unary_dispatch(%36, %45, %41, %41, %41, %43, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%35, %33) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.mlir.zero : !llvm.ptr
        %210 = llvm.getelementptr %209[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %211 = llvm.ptrtoint %210 : !llvm.ptr to i64
        %212 = llvm.add %211, %27  : i64
        %213 = llvm.call @malloc(%212) : (i64) -> !llvm.ptr
        %214 = llvm.ptrtoint %213 : !llvm.ptr to i64
        %215 = llvm.sub %27, %28  : i64
        %216 = llvm.add %214, %215  : i64
        %217 = llvm.urem %216, %27  : i64
        %218 = llvm.sub %216, %217  : i64
        %219 = llvm.inttoptr %218 : i64 to !llvm.ptr
        %220 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %221 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %222 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %223 = llvm.insertvalue %220, %222[0] : !llvm.struct<(ptr, ptr, i64)> 
        %224 = llvm.insertvalue %221, %223[1] : !llvm.struct<(ptr, ptr, i64)> 
        %225 = llvm.mlir.constant(0 : index) : i64
        %226 = llvm.insertvalue %225, %224[2] : !llvm.struct<(ptr, ptr, i64)> 
        %227 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %228 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %229 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %230 = llvm.extractvalue %160[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %160[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %233 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %234 = llvm.extractvalue %160[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %235 = llvm.extractvalue %160[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %236 = llvm.mul %arg6, %15  : i64
        %237 = builtin.unrealized_conversion_cast %236 : i64 to index
        %238 = builtin.unrealized_conversion_cast %237 : index to i64
        %239 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %240 = llvm.extractvalue %226[0] : !llvm.struct<(ptr, ptr, i64)> 
        %241 = llvm.extractvalue %226[1] : !llvm.struct<(ptr, ptr, i64)> 
        %242 = llvm.insertvalue %240, %239[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %243 = llvm.insertvalue %241, %242[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %244 = llvm.insertvalue %238, %243[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %245 = llvm.mlir.constant(8192 : index) : i64
        %246 = llvm.insertvalue %245, %244[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %247 = llvm.mlir.constant(1024 : index) : i64
        %248 = llvm.insertvalue %247, %246[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %249 = llvm.mlir.constant(32 : index) : i64
        %250 = llvm.insertvalue %249, %248[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %251 = llvm.mlir.constant(32 : index) : i64
        %252 = llvm.insertvalue %251, %250[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %253 = llvm.mlir.constant(32 : index) : i64
        %254 = llvm.insertvalue %253, %252[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %255 = llvm.mlir.constant(1 : index) : i64
        %256 = llvm.insertvalue %255, %254[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %257 = builtin.unrealized_conversion_cast %256 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %258 = builtin.unrealized_conversion_cast %257 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %259 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %260 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %261 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %262 = llvm.insertvalue %259, %261[0] : !llvm.struct<(ptr, ptr, i64)> 
        %263 = llvm.insertvalue %260, %262[1] : !llvm.struct<(ptr, ptr, i64)> 
        %264 = llvm.mlir.constant(0 : index) : i64
        %265 = llvm.insertvalue %264, %263[2] : !llvm.struct<(ptr, ptr, i64)> 
        %266 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %267 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %268 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %269 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %270 = llvm.extractvalue %185[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %271 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %272 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %273 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %274 = llvm.extractvalue %185[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %275 = llvm.mul %arg7, %15  : i64
        %276 = builtin.unrealized_conversion_cast %275 : i64 to index
        %277 = builtin.unrealized_conversion_cast %276 : index to i64
        %278 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %279 = llvm.extractvalue %265[0] : !llvm.struct<(ptr, ptr, i64)> 
        %280 = llvm.extractvalue %265[1] : !llvm.struct<(ptr, ptr, i64)> 
        %281 = llvm.insertvalue %279, %278[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %282 = llvm.insertvalue %280, %281[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %283 = llvm.insertvalue %277, %282[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %284 = llvm.mlir.constant(8192 : index) : i64
        %285 = llvm.insertvalue %284, %283[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %286 = llvm.mlir.constant(1024 : index) : i64
        %287 = llvm.insertvalue %286, %285[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %288 = llvm.mlir.constant(32 : index) : i64
        %289 = llvm.insertvalue %288, %287[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %290 = llvm.mlir.constant(32 : index) : i64
        %291 = llvm.insertvalue %290, %289[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %292 = llvm.mlir.constant(32 : index) : i64
        %293 = llvm.insertvalue %292, %291[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %294 = llvm.mlir.constant(1 : index) : i64
        %295 = llvm.insertvalue %294, %293[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %296 = builtin.unrealized_conversion_cast %295 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %297 = builtin.unrealized_conversion_cast %296 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %298 = llvm.extractvalue %258[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %299 = builtin.unrealized_conversion_cast %298 : i64 to index
        %300 = llvm.extractvalue %258[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %301 = llvm.ptrtoint %300 : !llvm.ptr to i64
        %302 = builtin.unrealized_conversion_cast %301 : i64 to index
        %303 = llvm.inttoptr %301 : i64 to !llvm.ptr<f32>
        %304 = llvm.extractvalue %297[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %305 = builtin.unrealized_conversion_cast %304 : i64 to index
        %306 = llvm.extractvalue %297[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %307 = llvm.ptrtoint %306 : !llvm.ptr to i64
        %308 = builtin.unrealized_conversion_cast %307 : i64 to index
        %309 = llvm.inttoptr %307 : i64 to !llvm.ptr<f32>
        %310 = llvm.ptrtoint %219 : !llvm.ptr to i64
        %311 = builtin.unrealized_conversion_cast %310 : i64 to index
        %312 = llvm.inttoptr %310 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_brgemm_invoke(%45, %198, %303, %298, %309, %304, %312, %16, %32) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) -> ()
        %313 = llvm.mul %arg7, %14  : i64
        %314 = builtin.unrealized_conversion_cast %313 : i64 to index
        %315 = builtin.unrealized_conversion_cast %314 : index to i64
        %316 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %317 = llvm.extractvalue %194[0] : !llvm.struct<(ptr, ptr, i64)> 
        %318 = llvm.extractvalue %194[1] : !llvm.struct<(ptr, ptr, i64)> 
        %319 = llvm.insertvalue %317, %316[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %320 = llvm.insertvalue %318, %319[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %321 = llvm.insertvalue %315, %320[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %322 = llvm.mlir.constant(32 : index) : i64
        %323 = llvm.insertvalue %322, %321[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %324 = llvm.mlir.constant(1 : index) : i64
        %325 = llvm.insertvalue %324, %323[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %326 = builtin.unrealized_conversion_cast %325 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<32xf32, strided<[1], offset: ?>>
        %327 = builtin.unrealized_conversion_cast %326 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %328 = llvm.extractvalue %327[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %329 = builtin.unrealized_conversion_cast %328 : i64 to index
        %330 = llvm.extractvalue %327[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %331 = llvm.ptrtoint %330 : !llvm.ptr to i64
        %332 = builtin.unrealized_conversion_cast %331 : i64 to index
        %333 = llvm.inttoptr %331 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_binary_invoke(%45, %199, %333, %328, %312, %16, %312, %16) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        %334 = llvm.extractvalue %132[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %335 = llvm.extractvalue %132[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %336 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %337 = llvm.insertvalue %334, %336[0] : !llvm.struct<(ptr, ptr, i64)> 
        %338 = llvm.insertvalue %335, %337[1] : !llvm.struct<(ptr, ptr, i64)> 
        %339 = llvm.mlir.constant(0 : index) : i64
        %340 = llvm.insertvalue %339, %338[2] : !llvm.struct<(ptr, ptr, i64)> 
        %341 = llvm.extractvalue %132[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %342 = llvm.extractvalue %132[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %343 = llvm.extractvalue %132[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %344 = llvm.extractvalue %132[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %345 = llvm.extractvalue %132[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %346 = llvm.mul %arg6, %12  : i64
        %347 = llvm.mul %arg7, %14  : i64
        %348 = llvm.add %346, %347  : i64
        %349 = builtin.unrealized_conversion_cast %348 : i64 to index
        %350 = builtin.unrealized_conversion_cast %349 : index to i64
        %351 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %352 = llvm.extractvalue %340[0] : !llvm.struct<(ptr, ptr, i64)> 
        %353 = llvm.extractvalue %340[1] : !llvm.struct<(ptr, ptr, i64)> 
        %354 = llvm.insertvalue %352, %351[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %355 = llvm.insertvalue %353, %354[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %356 = llvm.insertvalue %350, %355[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %357 = llvm.mlir.constant(32 : index) : i64
        %358 = llvm.insertvalue %357, %356[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %359 = llvm.mlir.constant(128 : index) : i64
        %360 = llvm.insertvalue %359, %358[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %361 = llvm.mlir.constant(32 : index) : i64
        %362 = llvm.insertvalue %361, %360[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %363 = llvm.mlir.constant(1 : index) : i64
        %364 = llvm.insertvalue %363, %362[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %365 = builtin.unrealized_conversion_cast %364 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %366 = builtin.unrealized_conversion_cast %365 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %367 = llvm.extractvalue %366[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %368 = builtin.unrealized_conversion_cast %367 : i64 to index
        %369 = llvm.extractvalue %366[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %370 = llvm.ptrtoint %369 : !llvm.ptr to i64
        %371 = builtin.unrealized_conversion_cast %370 : i64 to index
        %372 = llvm.inttoptr %370 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %200, %312, %16, %372, %367) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.call @free(%213) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%77) : (!llvm.ptr) -> ()
    llvm.call @free(%142) : (!llvm.ptr) -> ()
    llvm.call @free(%167) : (!llvm.ptr) -> ()
    %201 = llvm.alloca %28 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %132, %201 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %202 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %203 = llvm.insertvalue %24, %202[0] : !llvm.struct<(i64, ptr)> 
    %204 = llvm.insertvalue %201, %203[1] : !llvm.struct<(i64, ptr)> 
    %205 = builtin.unrealized_conversion_cast %204 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %206 = llvm.extractvalue %204[0] : !llvm.struct<(i64, ptr)> 
    %207 = llvm.extractvalue %204[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @refbackend_consume_func_return_mrf32(%206, %207) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_MLP(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg0 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %4 = llvm.extractvalue %3[0] : !llvm.struct<(i64, ptr)> 
    %5 = llvm.extractvalue %3[1] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(i64, ptr)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @MLP(%1, %2, %4, %5, %7, %8) : (i64, !llvm.ptr, i64, !llvm.ptr, i64, !llvm.ptr) -> ()
    llvm.return
  }
}


// -----// IR Dump Before ConvertFuncToLLVMPass (convert-func-to-llvm) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func private @refbackend_consume_func_return_mrf32(%arg0: i64, %arg1: !llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.mlir.constant(1 : index) : i64
    %4 = llvm.alloca %3 x !llvm.struct<(i64, ptr)> : (i64) -> !llvm.ptr
    llvm.store %2, %4 : !llvm.struct<(i64, ptr)>, !llvm.ptr
    llvm.call @_mlir_ciface_refbackend_consume_func_return_mrf32(%4) : (!llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_refbackend_consume_func_return_mrf32(!llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"}
  llvm.func @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @MLP(%arg0: i64, %arg1: !llvm.ptr, %arg2: i64, %arg3: !llvm.ptr, %arg4: i64, %arg5: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = builtin.unrealized_conversion_cast %2 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %4 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %5 = llvm.insertvalue %arg2, %4[0] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.insertvalue %arg3, %5[1] : !llvm.struct<(i64, ptr)> 
    %7 = builtin.unrealized_conversion_cast %6 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %8 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %9 = llvm.insertvalue %arg4, %8[0] : !llvm.struct<(i64, ptr)> 
    %10 = llvm.insertvalue %arg5, %9[1] : !llvm.struct<(i64, ptr)> 
    %11 = builtin.unrealized_conversion_cast %10 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %12 = llvm.mlir.constant(4096 : index) : i64
    %13 = llvm.mlir.constant(1024 : index) : i64
    %14 = llvm.mlir.constant(32 : index) : i64
    %15 = llvm.mlir.constant(8388608 : index) : i64
    %16 = llvm.mlir.constant(0 : index) : i64
    %17 = llvm.mlir.constant(128 : index) : i64
    %18 = llvm.mlir.constant(1 : index) : i64
    %19 = llvm.mlir.constant(262144 : index) : i64
    %20 = llvm.mlir.constant(4 : index) : i64
    %21 = llvm.mlir.constant(8388608 : index) : i64
    %22 = llvm.mlir.constant(1024 : index) : i64
    %23 = llvm.mlir.constant(32 : index) : i64
    %24 = llvm.mlir.constant(2 : index) : i64
    %25 = llvm.mlir.constant(8192 : index) : i64
    %26 = llvm.mlir.constant(0 : index) : i64
    %27 = llvm.mlir.constant(64 : index) : i64
    %28 = llvm.mlir.constant(1 : index) : i64
    %29 = llvm.mlir.constant(128 : index) : i64
    %30 = llvm.mlir.constant(262144 : index) : i64
    %31 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %32 = llvm.mlir.constant(8192 : i64) : i64
    %33 = llvm.mlir.constant(4 : index) : i64
    %34 = llvm.mlir.constant(8192 : index) : i64
    %35 = llvm.mlir.constant(2 : index) : i64
    %36 = llvm.mlir.constant(5 : i64) : i64
    %37 = llvm.mlir.constant(4 : i64) : i64
    %38 = llvm.mlir.constant(1024 : i64) : i64
    %39 = llvm.mlir.constant(0 : i64) : i64
    %40 = llvm.mlir.constant(262144 : i64) : i64
    %41 = llvm.mlir.constant(32 : i64) : i64
    %42 = llvm.mlir.constant(8 : i64) : i64
    %43 = llvm.mlir.constant(128 : i64) : i64
    %44 = llvm.mlir.constant(64 : i64) : i64
    %45 = llvm.mlir.constant(1 : i64) : i64
    %46 = llvm.mlir.constant(2 : i64) : i64
    %47 = builtin.unrealized_conversion_cast %3 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %48 = builtin.unrealized_conversion_cast %7 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %49 = builtin.unrealized_conversion_cast %11 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %50 = llvm.extractvalue %47[1] : !llvm.struct<(i64, ptr)> 
    %51 = llvm.load %50 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %52 = builtin.unrealized_conversion_cast %51 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %53 = llvm.extractvalue %48[1] : !llvm.struct<(i64, ptr)> 
    %54 = llvm.load %53 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %56 = llvm.extractvalue %49[1] : !llvm.struct<(i64, ptr)> 
    %57 = llvm.load %56 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %58 = builtin.unrealized_conversion_cast %57 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %59 = llvm.extractvalue %57[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %60 = llvm.extractvalue %57[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %61 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %62 = llvm.insertvalue %59, %61[0] : !llvm.struct<(ptr, ptr, i64)> 
    %63 = llvm.insertvalue %60, %62[1] : !llvm.struct<(ptr, ptr, i64)> 
    %64 = llvm.mlir.constant(0 : index) : i64
    %65 = llvm.insertvalue %64, %63[2] : !llvm.struct<(ptr, ptr, i64)> 
    %66 = llvm.extractvalue %57[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %67 = llvm.extractvalue %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %68 = llvm.extractvalue %57[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %69 = llvm.extractvalue %57[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %70 = llvm.extractvalue %57[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %71 = llvm.extractvalue %57[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %72 = llvm.extractvalue %57[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %73 = llvm.mlir.zero : !llvm.ptr
    %74 = llvm.getelementptr %73[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %75 = llvm.ptrtoint %74 : !llvm.ptr to i64
    %76 = llvm.add %75, %27  : i64
    %77 = llvm.call @malloc(%76) : (i64) -> !llvm.ptr
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.sub %27, %28  : i64
    %80 = llvm.add %78, %79  : i64
    %81 = llvm.urem %80, %27  : i64
    %82 = llvm.sub %80, %81  : i64
    %83 = llvm.inttoptr %82 : i64 to !llvm.ptr
    %84 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %85 = llvm.insertvalue %77, %84[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %86 = llvm.insertvalue %83, %85[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %87 = llvm.insertvalue %26, %86[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %88 = llvm.insertvalue %30, %87[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %89 = llvm.insertvalue %29, %88[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.insertvalue %29, %89[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.insertvalue %28, %90[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = builtin.unrealized_conversion_cast %91 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    llvm.br ^bb1(%16 : i64)
  ^bb1(%93: i64):  // 2 preds: ^bb0, ^bb5
    %94 = builtin.unrealized_conversion_cast %93 : i64 to index
    %95 = llvm.icmp "slt" %93, %17 : i64
    llvm.cond_br %95, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    %96 = builtin.unrealized_conversion_cast %94 : index to i64
    llvm.br ^bb3(%16 : i64)
  ^bb3(%97: i64):  // 2 preds: ^bb2, ^bb4
    %98 = builtin.unrealized_conversion_cast %97 : i64 to index
    %99 = llvm.icmp "slt" %97, %19 : i64
    llvm.cond_br %99, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %100 = builtin.unrealized_conversion_cast %98 : index to i64
    %101 = llvm.extractvalue %51[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %102 = llvm.mlir.constant(262144 : index) : i64
    %103 = llvm.mul %96, %102  : i64
    %104 = llvm.add %103, %100  : i64
    %105 = llvm.getelementptr %101[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %106 = llvm.load %105 : !llvm.ptr -> f32
    %107 = llvm.extractvalue %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %108 = llvm.mlir.constant(128 : index) : i64
    %109 = llvm.mul %100, %108  : i64
    %110 = llvm.add %109, %96  : i64
    %111 = llvm.getelementptr %107[%110] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %106, %111 : f32, !llvm.ptr
    %112 = llvm.add %97, %18  : i64
    llvm.br ^bb3(%112 : i64)
  ^bb5:  // pred: ^bb3
    %113 = llvm.add %93, %18  : i64
    llvm.br ^bb1(%113 : i64)
  ^bb6:  // pred: ^bb1
    %114 = llvm.mlir.zero : !llvm.ptr
    %115 = llvm.getelementptr %114[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %116 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %117 = llvm.add %116, %27  : i64
    %118 = llvm.call @malloc(%117) : (i64) -> !llvm.ptr
    %119 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %120 = llvm.sub %27, %28  : i64
    %121 = llvm.add %119, %120  : i64
    %122 = llvm.urem %121, %27  : i64
    %123 = llvm.sub %121, %122  : i64
    %124 = llvm.inttoptr %123 : i64 to !llvm.ptr
    %125 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %126 = llvm.insertvalue %118, %125[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %127 = llvm.insertvalue %124, %126[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %128 = llvm.insertvalue %26, %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %129 = llvm.insertvalue %27, %128[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %130 = llvm.insertvalue %29, %129[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.insertvalue %29, %130[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %132 = llvm.insertvalue %28, %131[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = builtin.unrealized_conversion_cast %132 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %134 = llvm.call @xsmm_unary_dispatch(%46, %45, %44, %43, %45, %43, %42) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %135 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %136 = builtin.unrealized_conversion_cast %135 : i64 to index
    %137 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
    llvm.call @xsmm_unary_scalar_invoke(%45, %134, %31, %137, %16) : (i64, i64, f32, !llvm.ptr<f32>, i64) -> ()
    %138 = llvm.mlir.zero : !llvm.ptr
    %139 = llvm.getelementptr %138[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
    %141 = llvm.add %140, %27  : i64
    %142 = llvm.call @malloc(%141) : (i64) -> !llvm.ptr
    %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
    %144 = llvm.sub %27, %28  : i64
    %145 = llvm.add %143, %144  : i64
    %146 = llvm.urem %145, %27  : i64
    %147 = llvm.sub %145, %146  : i64
    %148 = llvm.inttoptr %147 : i64 to !llvm.ptr
    %149 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %150 = llvm.insertvalue %142, %149[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %151 = llvm.insertvalue %148, %150[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %152 = llvm.insertvalue %26, %151[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %153 = llvm.insertvalue %24, %152[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %154 = llvm.insertvalue %25, %153[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %155 = llvm.insertvalue %23, %154[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %156 = llvm.insertvalue %23, %155[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %157 = llvm.insertvalue %21, %156[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %158 = llvm.insertvalue %22, %157[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %159 = llvm.insertvalue %23, %158[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %160 = llvm.insertvalue %28, %159[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %161 = builtin.unrealized_conversion_cast %160 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %162 = llvm.call @xsmm_unary_dispatch(%45, %45, %41, %41, %40, %41, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%35, %34) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.mul %arg6, %15  : i64
        %210 = llvm.mul %arg7, %14  : i64
        %211 = llvm.add %209, %210  : i64
        %212 = builtin.unrealized_conversion_cast %211 : i64 to index
        %213 = builtin.unrealized_conversion_cast %212 : index to i64
        %214 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %215 = llvm.extractvalue %65[0] : !llvm.struct<(ptr, ptr, i64)> 
        %216 = llvm.extractvalue %65[1] : !llvm.struct<(ptr, ptr, i64)> 
        %217 = llvm.insertvalue %215, %214[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.insertvalue %216, %217[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.insertvalue %213, %218[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.mlir.constant(32 : index) : i64
        %221 = llvm.insertvalue %220, %219[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.mlir.constant(262144 : index) : i64
        %223 = llvm.insertvalue %222, %221[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %224 = llvm.mlir.constant(32 : index) : i64
        %225 = llvm.insertvalue %224, %223[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %226 = llvm.mlir.constant(1 : index) : i64
        %227 = llvm.insertvalue %226, %225[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = builtin.unrealized_conversion_cast %227 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
        %229 = builtin.unrealized_conversion_cast %228 : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %230 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %233 = llvm.insertvalue %230, %232[0] : !llvm.struct<(ptr, ptr, i64)> 
        %234 = llvm.insertvalue %231, %233[1] : !llvm.struct<(ptr, ptr, i64)> 
        %235 = llvm.mlir.constant(0 : index) : i64
        %236 = llvm.insertvalue %235, %234[2] : !llvm.struct<(ptr, ptr, i64)> 
        %237 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %238 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %239 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %240 = llvm.extractvalue %160[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %241 = llvm.extractvalue %160[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %242 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %243 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %244 = llvm.extractvalue %160[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %245 = llvm.extractvalue %160[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %246 = llvm.mul %arg6, %15  : i64
        %247 = llvm.mul %arg7, %13  : i64
        %248 = llvm.add %246, %247  : i64
        %249 = builtin.unrealized_conversion_cast %248 : i64 to index
        %250 = builtin.unrealized_conversion_cast %249 : index to i64
        %251 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %252 = llvm.extractvalue %236[0] : !llvm.struct<(ptr, ptr, i64)> 
        %253 = llvm.extractvalue %236[1] : !llvm.struct<(ptr, ptr, i64)> 
        %254 = llvm.insertvalue %252, %251[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %255 = llvm.insertvalue %253, %254[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %256 = llvm.insertvalue %250, %255[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %257 = llvm.mlir.constant(32 : index) : i64
        %258 = llvm.insertvalue %257, %256[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %259 = llvm.mlir.constant(32 : index) : i64
        %260 = llvm.insertvalue %259, %258[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %261 = llvm.mlir.constant(32 : index) : i64
        %262 = llvm.insertvalue %261, %260[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %263 = llvm.mlir.constant(1 : index) : i64
        %264 = llvm.insertvalue %263, %262[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %265 = builtin.unrealized_conversion_cast %264 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %266 = builtin.unrealized_conversion_cast %265 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %267 = llvm.extractvalue %229[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %268 = builtin.unrealized_conversion_cast %267 : i64 to index
        %269 = llvm.extractvalue %229[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %270 = llvm.ptrtoint %269 : !llvm.ptr to i64
        %271 = builtin.unrealized_conversion_cast %270 : i64 to index
        %272 = llvm.inttoptr %270 : i64 to !llvm.ptr<f32>
        %273 = llvm.extractvalue %266[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %274 = builtin.unrealized_conversion_cast %273 : i64 to index
        %275 = llvm.extractvalue %266[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %276 = llvm.ptrtoint %275 : !llvm.ptr to i64
        %277 = builtin.unrealized_conversion_cast %276 : i64 to index
        %278 = llvm.inttoptr %276 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %162, %272, %267, %278, %273) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %163 = llvm.mlir.zero : !llvm.ptr
    %164 = llvm.getelementptr %163[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %165 = llvm.ptrtoint %164 : !llvm.ptr to i64
    %166 = llvm.add %165, %27  : i64
    %167 = llvm.call @malloc(%166) : (i64) -> !llvm.ptr
    %168 = llvm.ptrtoint %167 : !llvm.ptr to i64
    %169 = llvm.sub %27, %28  : i64
    %170 = llvm.add %168, %169  : i64
    %171 = llvm.urem %170, %27  : i64
    %172 = llvm.sub %170, %171  : i64
    %173 = llvm.inttoptr %172 : i64 to !llvm.ptr
    %174 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %175 = llvm.insertvalue %167, %174[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %176 = llvm.insertvalue %173, %175[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %177 = llvm.insertvalue %26, %176[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %178 = llvm.insertvalue %20, %177[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %179 = llvm.insertvalue %25, %178[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %180 = llvm.insertvalue %23, %179[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %181 = llvm.insertvalue %23, %180[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %182 = llvm.insertvalue %21, %181[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %183 = llvm.insertvalue %22, %182[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %184 = llvm.insertvalue %23, %183[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %185 = llvm.insertvalue %28, %184[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %186 = builtin.unrealized_conversion_cast %185 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %187 = llvm.call @xsmm_unary_dispatch(%45, %45, %41, %41, %43, %41, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%33, %34) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.extractvalue %91[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %210 = llvm.extractvalue %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %211 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %212 = llvm.insertvalue %209, %211[0] : !llvm.struct<(ptr, ptr, i64)> 
        %213 = llvm.insertvalue %210, %212[1] : !llvm.struct<(ptr, ptr, i64)> 
        %214 = llvm.mlir.constant(0 : index) : i64
        %215 = llvm.insertvalue %214, %213[2] : !llvm.struct<(ptr, ptr, i64)> 
        %216 = llvm.extractvalue %91[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = llvm.extractvalue %91[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.extractvalue %91[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.extractvalue %91[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.extractvalue %91[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %221 = llvm.mul %arg7, %12  : i64
        %222 = llvm.mul %arg6, %14  : i64
        %223 = llvm.add %221, %222  : i64
        %224 = builtin.unrealized_conversion_cast %223 : i64 to index
        %225 = builtin.unrealized_conversion_cast %224 : index to i64
        %226 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %227 = llvm.extractvalue %215[0] : !llvm.struct<(ptr, ptr, i64)> 
        %228 = llvm.extractvalue %215[1] : !llvm.struct<(ptr, ptr, i64)> 
        %229 = llvm.insertvalue %227, %226[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %230 = llvm.insertvalue %228, %229[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %231 = llvm.insertvalue %225, %230[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %232 = llvm.mlir.constant(32 : index) : i64
        %233 = llvm.insertvalue %232, %231[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %234 = llvm.mlir.constant(128 : index) : i64
        %235 = llvm.insertvalue %234, %233[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %236 = llvm.mlir.constant(32 : index) : i64
        %237 = llvm.insertvalue %236, %235[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %238 = llvm.mlir.constant(1 : index) : i64
        %239 = llvm.insertvalue %238, %237[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %240 = builtin.unrealized_conversion_cast %239 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %241 = builtin.unrealized_conversion_cast %240 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %242 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %243 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %244 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %245 = llvm.insertvalue %242, %244[0] : !llvm.struct<(ptr, ptr, i64)> 
        %246 = llvm.insertvalue %243, %245[1] : !llvm.struct<(ptr, ptr, i64)> 
        %247 = llvm.mlir.constant(0 : index) : i64
        %248 = llvm.insertvalue %247, %246[2] : !llvm.struct<(ptr, ptr, i64)> 
        %249 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %250 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %251 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %252 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %253 = llvm.extractvalue %185[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %254 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %255 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %256 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %257 = llvm.extractvalue %185[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %258 = llvm.mul %arg6, %15  : i64
        %259 = llvm.mul %arg7, %13  : i64
        %260 = llvm.add %258, %259  : i64
        %261 = builtin.unrealized_conversion_cast %260 : i64 to index
        %262 = builtin.unrealized_conversion_cast %261 : index to i64
        %263 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %264 = llvm.extractvalue %248[0] : !llvm.struct<(ptr, ptr, i64)> 
        %265 = llvm.extractvalue %248[1] : !llvm.struct<(ptr, ptr, i64)> 
        %266 = llvm.insertvalue %264, %263[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %267 = llvm.insertvalue %265, %266[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %268 = llvm.insertvalue %262, %267[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %269 = llvm.mlir.constant(32 : index) : i64
        %270 = llvm.insertvalue %269, %268[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %271 = llvm.mlir.constant(32 : index) : i64
        %272 = llvm.insertvalue %271, %270[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %273 = llvm.mlir.constant(32 : index) : i64
        %274 = llvm.insertvalue %273, %272[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %275 = llvm.mlir.constant(1 : index) : i64
        %276 = llvm.insertvalue %275, %274[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %277 = builtin.unrealized_conversion_cast %276 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %278 = builtin.unrealized_conversion_cast %277 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %279 = llvm.extractvalue %241[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %280 = builtin.unrealized_conversion_cast %279 : i64 to index
        %281 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %282 = llvm.ptrtoint %281 : !llvm.ptr to i64
        %283 = builtin.unrealized_conversion_cast %282 : i64 to index
        %284 = llvm.inttoptr %282 : i64 to !llvm.ptr<f32>
        %285 = llvm.extractvalue %278[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %286 = builtin.unrealized_conversion_cast %285 : i64 to index
        %287 = llvm.extractvalue %278[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %288 = llvm.ptrtoint %287 : !llvm.ptr to i64
        %289 = builtin.unrealized_conversion_cast %288 : i64 to index
        %290 = llvm.inttoptr %288 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %187, %284, %279, %290, %285) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %188 = llvm.extractvalue %54[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %189 = llvm.extractvalue %54[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %190 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %191 = llvm.insertvalue %188, %190[0] : !llvm.struct<(ptr, ptr, i64)> 
    %192 = llvm.insertvalue %189, %191[1] : !llvm.struct<(ptr, ptr, i64)> 
    %193 = llvm.mlir.constant(0 : index) : i64
    %194 = llvm.insertvalue %193, %192[2] : !llvm.struct<(ptr, ptr, i64)> 
    %195 = llvm.extractvalue %54[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %196 = llvm.extractvalue %54[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %197 = llvm.extractvalue %54[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %198 = llvm.call @xsmm_brgemm_dispatch(%45, %41, %41, %41, %41, %41, %41, %38, %38, %37) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %199 = llvm.call @xsmm_binary_dispatch(%45, %45, %41, %41, %41, %41, %41, %37) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %200 = llvm.call @xsmm_unary_dispatch(%36, %45, %41, %41, %41, %43, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%35, %33) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.mlir.zero : !llvm.ptr
        %210 = llvm.getelementptr %209[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %211 = llvm.ptrtoint %210 : !llvm.ptr to i64
        %212 = llvm.add %211, %27  : i64
        %213 = llvm.call @malloc(%212) : (i64) -> !llvm.ptr
        %214 = llvm.ptrtoint %213 : !llvm.ptr to i64
        %215 = llvm.sub %27, %28  : i64
        %216 = llvm.add %214, %215  : i64
        %217 = llvm.urem %216, %27  : i64
        %218 = llvm.sub %216, %217  : i64
        %219 = llvm.inttoptr %218 : i64 to !llvm.ptr
        %220 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %221 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %222 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %223 = llvm.insertvalue %220, %222[0] : !llvm.struct<(ptr, ptr, i64)> 
        %224 = llvm.insertvalue %221, %223[1] : !llvm.struct<(ptr, ptr, i64)> 
        %225 = llvm.mlir.constant(0 : index) : i64
        %226 = llvm.insertvalue %225, %224[2] : !llvm.struct<(ptr, ptr, i64)> 
        %227 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %228 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %229 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %230 = llvm.extractvalue %160[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %160[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %233 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %234 = llvm.extractvalue %160[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %235 = llvm.extractvalue %160[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %236 = llvm.mul %arg6, %15  : i64
        %237 = builtin.unrealized_conversion_cast %236 : i64 to index
        %238 = builtin.unrealized_conversion_cast %237 : index to i64
        %239 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %240 = llvm.extractvalue %226[0] : !llvm.struct<(ptr, ptr, i64)> 
        %241 = llvm.extractvalue %226[1] : !llvm.struct<(ptr, ptr, i64)> 
        %242 = llvm.insertvalue %240, %239[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %243 = llvm.insertvalue %241, %242[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %244 = llvm.insertvalue %238, %243[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %245 = llvm.mlir.constant(8192 : index) : i64
        %246 = llvm.insertvalue %245, %244[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %247 = llvm.mlir.constant(1024 : index) : i64
        %248 = llvm.insertvalue %247, %246[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %249 = llvm.mlir.constant(32 : index) : i64
        %250 = llvm.insertvalue %249, %248[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %251 = llvm.mlir.constant(32 : index) : i64
        %252 = llvm.insertvalue %251, %250[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %253 = llvm.mlir.constant(32 : index) : i64
        %254 = llvm.insertvalue %253, %252[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %255 = llvm.mlir.constant(1 : index) : i64
        %256 = llvm.insertvalue %255, %254[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %257 = builtin.unrealized_conversion_cast %256 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %258 = builtin.unrealized_conversion_cast %257 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %259 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %260 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %261 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %262 = llvm.insertvalue %259, %261[0] : !llvm.struct<(ptr, ptr, i64)> 
        %263 = llvm.insertvalue %260, %262[1] : !llvm.struct<(ptr, ptr, i64)> 
        %264 = llvm.mlir.constant(0 : index) : i64
        %265 = llvm.insertvalue %264, %263[2] : !llvm.struct<(ptr, ptr, i64)> 
        %266 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %267 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %268 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %269 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %270 = llvm.extractvalue %185[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %271 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %272 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %273 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %274 = llvm.extractvalue %185[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %275 = llvm.mul %arg7, %15  : i64
        %276 = builtin.unrealized_conversion_cast %275 : i64 to index
        %277 = builtin.unrealized_conversion_cast %276 : index to i64
        %278 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %279 = llvm.extractvalue %265[0] : !llvm.struct<(ptr, ptr, i64)> 
        %280 = llvm.extractvalue %265[1] : !llvm.struct<(ptr, ptr, i64)> 
        %281 = llvm.insertvalue %279, %278[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %282 = llvm.insertvalue %280, %281[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %283 = llvm.insertvalue %277, %282[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %284 = llvm.mlir.constant(8192 : index) : i64
        %285 = llvm.insertvalue %284, %283[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %286 = llvm.mlir.constant(1024 : index) : i64
        %287 = llvm.insertvalue %286, %285[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %288 = llvm.mlir.constant(32 : index) : i64
        %289 = llvm.insertvalue %288, %287[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %290 = llvm.mlir.constant(32 : index) : i64
        %291 = llvm.insertvalue %290, %289[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %292 = llvm.mlir.constant(32 : index) : i64
        %293 = llvm.insertvalue %292, %291[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %294 = llvm.mlir.constant(1 : index) : i64
        %295 = llvm.insertvalue %294, %293[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %296 = builtin.unrealized_conversion_cast %295 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %297 = builtin.unrealized_conversion_cast %296 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %298 = llvm.extractvalue %258[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %299 = builtin.unrealized_conversion_cast %298 : i64 to index
        %300 = llvm.extractvalue %258[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %301 = llvm.ptrtoint %300 : !llvm.ptr to i64
        %302 = builtin.unrealized_conversion_cast %301 : i64 to index
        %303 = llvm.inttoptr %301 : i64 to !llvm.ptr<f32>
        %304 = llvm.extractvalue %297[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %305 = builtin.unrealized_conversion_cast %304 : i64 to index
        %306 = llvm.extractvalue %297[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %307 = llvm.ptrtoint %306 : !llvm.ptr to i64
        %308 = builtin.unrealized_conversion_cast %307 : i64 to index
        %309 = llvm.inttoptr %307 : i64 to !llvm.ptr<f32>
        %310 = llvm.ptrtoint %219 : !llvm.ptr to i64
        %311 = builtin.unrealized_conversion_cast %310 : i64 to index
        %312 = llvm.inttoptr %310 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_brgemm_invoke(%45, %198, %303, %298, %309, %304, %312, %16, %32) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) -> ()
        %313 = llvm.mul %arg7, %14  : i64
        %314 = builtin.unrealized_conversion_cast %313 : i64 to index
        %315 = builtin.unrealized_conversion_cast %314 : index to i64
        %316 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %317 = llvm.extractvalue %194[0] : !llvm.struct<(ptr, ptr, i64)> 
        %318 = llvm.extractvalue %194[1] : !llvm.struct<(ptr, ptr, i64)> 
        %319 = llvm.insertvalue %317, %316[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %320 = llvm.insertvalue %318, %319[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %321 = llvm.insertvalue %315, %320[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %322 = llvm.mlir.constant(32 : index) : i64
        %323 = llvm.insertvalue %322, %321[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %324 = llvm.mlir.constant(1 : index) : i64
        %325 = llvm.insertvalue %324, %323[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %326 = builtin.unrealized_conversion_cast %325 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<32xf32, strided<[1], offset: ?>>
        %327 = builtin.unrealized_conversion_cast %326 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %328 = llvm.extractvalue %327[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %329 = builtin.unrealized_conversion_cast %328 : i64 to index
        %330 = llvm.extractvalue %327[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %331 = llvm.ptrtoint %330 : !llvm.ptr to i64
        %332 = builtin.unrealized_conversion_cast %331 : i64 to index
        %333 = llvm.inttoptr %331 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_binary_invoke(%45, %199, %333, %328, %312, %16, %312, %16) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        %334 = llvm.extractvalue %132[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %335 = llvm.extractvalue %132[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %336 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %337 = llvm.insertvalue %334, %336[0] : !llvm.struct<(ptr, ptr, i64)> 
        %338 = llvm.insertvalue %335, %337[1] : !llvm.struct<(ptr, ptr, i64)> 
        %339 = llvm.mlir.constant(0 : index) : i64
        %340 = llvm.insertvalue %339, %338[2] : !llvm.struct<(ptr, ptr, i64)> 
        %341 = llvm.extractvalue %132[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %342 = llvm.extractvalue %132[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %343 = llvm.extractvalue %132[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %344 = llvm.extractvalue %132[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %345 = llvm.extractvalue %132[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %346 = llvm.mul %arg6, %12  : i64
        %347 = llvm.mul %arg7, %14  : i64
        %348 = llvm.add %346, %347  : i64
        %349 = builtin.unrealized_conversion_cast %348 : i64 to index
        %350 = builtin.unrealized_conversion_cast %349 : index to i64
        %351 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %352 = llvm.extractvalue %340[0] : !llvm.struct<(ptr, ptr, i64)> 
        %353 = llvm.extractvalue %340[1] : !llvm.struct<(ptr, ptr, i64)> 
        %354 = llvm.insertvalue %352, %351[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %355 = llvm.insertvalue %353, %354[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %356 = llvm.insertvalue %350, %355[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %357 = llvm.mlir.constant(32 : index) : i64
        %358 = llvm.insertvalue %357, %356[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %359 = llvm.mlir.constant(128 : index) : i64
        %360 = llvm.insertvalue %359, %358[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %361 = llvm.mlir.constant(32 : index) : i64
        %362 = llvm.insertvalue %361, %360[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %363 = llvm.mlir.constant(1 : index) : i64
        %364 = llvm.insertvalue %363, %362[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %365 = builtin.unrealized_conversion_cast %364 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %366 = builtin.unrealized_conversion_cast %365 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %367 = llvm.extractvalue %366[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %368 = builtin.unrealized_conversion_cast %367 : i64 to index
        %369 = llvm.extractvalue %366[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %370 = llvm.ptrtoint %369 : !llvm.ptr to i64
        %371 = builtin.unrealized_conversion_cast %370 : i64 to index
        %372 = llvm.inttoptr %370 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %200, %312, %16, %372, %367) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.call @free(%213) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%77) : (!llvm.ptr) -> ()
    llvm.call @free(%142) : (!llvm.ptr) -> ()
    llvm.call @free(%167) : (!llvm.ptr) -> ()
    %201 = llvm.alloca %28 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %132, %201 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %202 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %203 = llvm.insertvalue %24, %202[0] : !llvm.struct<(i64, ptr)> 
    %204 = llvm.insertvalue %201, %203[1] : !llvm.struct<(i64, ptr)> 
    %205 = builtin.unrealized_conversion_cast %204 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %206 = llvm.extractvalue %204[0] : !llvm.struct<(i64, ptr)> 
    %207 = llvm.extractvalue %204[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @refbackend_consume_func_return_mrf32(%206, %207) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_MLP(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg0 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %4 = llvm.extractvalue %3[0] : !llvm.struct<(i64, ptr)> 
    %5 = llvm.extractvalue %3[1] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(i64, ptr)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @MLP(%1, %2, %4, %5, %7, %8) : (i64, !llvm.ptr, i64, !llvm.ptr, i64, !llvm.ptr) -> ()
    llvm.return
  }
}


// -----// IR Dump Before ReconcileUnrealizedCasts (reconcile-unrealized-casts) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func private @refbackend_consume_func_return_mrf32(%arg0: i64, %arg1: !llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.mlir.constant(1 : index) : i64
    %4 = llvm.alloca %3 x !llvm.struct<(i64, ptr)> : (i64) -> !llvm.ptr
    llvm.store %2, %4 : !llvm.struct<(i64, ptr)>, !llvm.ptr
    llvm.call @_mlir_ciface_refbackend_consume_func_return_mrf32(%4) : (!llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_refbackend_consume_func_return_mrf32(!llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"}
  llvm.func @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @MLP(%arg0: i64, %arg1: !llvm.ptr, %arg2: i64, %arg3: !llvm.ptr, %arg4: i64, %arg5: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = builtin.unrealized_conversion_cast %2 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %4 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %5 = llvm.insertvalue %arg2, %4[0] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.insertvalue %arg3, %5[1] : !llvm.struct<(i64, ptr)> 
    %7 = builtin.unrealized_conversion_cast %6 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %8 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %9 = llvm.insertvalue %arg4, %8[0] : !llvm.struct<(i64, ptr)> 
    %10 = llvm.insertvalue %arg5, %9[1] : !llvm.struct<(i64, ptr)> 
    %11 = builtin.unrealized_conversion_cast %10 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %12 = llvm.mlir.constant(4096 : index) : i64
    %13 = llvm.mlir.constant(1024 : index) : i64
    %14 = llvm.mlir.constant(32 : index) : i64
    %15 = llvm.mlir.constant(8388608 : index) : i64
    %16 = llvm.mlir.constant(0 : index) : i64
    %17 = llvm.mlir.constant(128 : index) : i64
    %18 = llvm.mlir.constant(1 : index) : i64
    %19 = llvm.mlir.constant(262144 : index) : i64
    %20 = llvm.mlir.constant(4 : index) : i64
    %21 = llvm.mlir.constant(8388608 : index) : i64
    %22 = llvm.mlir.constant(1024 : index) : i64
    %23 = llvm.mlir.constant(32 : index) : i64
    %24 = llvm.mlir.constant(2 : index) : i64
    %25 = llvm.mlir.constant(8192 : index) : i64
    %26 = llvm.mlir.constant(0 : index) : i64
    %27 = llvm.mlir.constant(64 : index) : i64
    %28 = llvm.mlir.constant(1 : index) : i64
    %29 = llvm.mlir.constant(128 : index) : i64
    %30 = llvm.mlir.constant(262144 : index) : i64
    %31 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %32 = llvm.mlir.constant(8192 : i64) : i64
    %33 = llvm.mlir.constant(4 : index) : i64
    %34 = llvm.mlir.constant(8192 : index) : i64
    %35 = llvm.mlir.constant(2 : index) : i64
    %36 = llvm.mlir.constant(5 : i64) : i64
    %37 = llvm.mlir.constant(4 : i64) : i64
    %38 = llvm.mlir.constant(1024 : i64) : i64
    %39 = llvm.mlir.constant(0 : i64) : i64
    %40 = llvm.mlir.constant(262144 : i64) : i64
    %41 = llvm.mlir.constant(32 : i64) : i64
    %42 = llvm.mlir.constant(8 : i64) : i64
    %43 = llvm.mlir.constant(128 : i64) : i64
    %44 = llvm.mlir.constant(64 : i64) : i64
    %45 = llvm.mlir.constant(1 : i64) : i64
    %46 = llvm.mlir.constant(2 : i64) : i64
    %47 = builtin.unrealized_conversion_cast %3 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %48 = builtin.unrealized_conversion_cast %7 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %49 = builtin.unrealized_conversion_cast %11 : memref<*xf32> to !llvm.struct<(i64, ptr)>
    %50 = llvm.extractvalue %47[1] : !llvm.struct<(i64, ptr)> 
    %51 = llvm.load %50 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %52 = builtin.unrealized_conversion_cast %51 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<128x262144xf32>
    %53 = llvm.extractvalue %48[1] : !llvm.struct<(i64, ptr)> 
    %54 = llvm.load %53 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %55 = builtin.unrealized_conversion_cast %54 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<128xf32>
    %56 = llvm.extractvalue %49[1] : !llvm.struct<(i64, ptr)> 
    %57 = llvm.load %56 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %58 = builtin.unrealized_conversion_cast %57 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<64x512x512xf32>
    %59 = llvm.extractvalue %57[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %60 = llvm.extractvalue %57[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %61 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %62 = llvm.insertvalue %59, %61[0] : !llvm.struct<(ptr, ptr, i64)> 
    %63 = llvm.insertvalue %60, %62[1] : !llvm.struct<(ptr, ptr, i64)> 
    %64 = llvm.mlir.constant(0 : index) : i64
    %65 = llvm.insertvalue %64, %63[2] : !llvm.struct<(ptr, ptr, i64)> 
    %66 = llvm.extractvalue %57[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %67 = llvm.extractvalue %57[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %68 = llvm.extractvalue %57[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %69 = llvm.extractvalue %57[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %70 = llvm.extractvalue %57[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %71 = llvm.extractvalue %57[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %72 = llvm.extractvalue %57[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %73 = llvm.mlir.zero : !llvm.ptr
    %74 = llvm.getelementptr %73[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %75 = llvm.ptrtoint %74 : !llvm.ptr to i64
    %76 = llvm.add %75, %27  : i64
    %77 = llvm.call @malloc(%76) : (i64) -> !llvm.ptr
    %78 = llvm.ptrtoint %77 : !llvm.ptr to i64
    %79 = llvm.sub %27, %28  : i64
    %80 = llvm.add %78, %79  : i64
    %81 = llvm.urem %80, %27  : i64
    %82 = llvm.sub %80, %81  : i64
    %83 = llvm.inttoptr %82 : i64 to !llvm.ptr
    %84 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %85 = llvm.insertvalue %77, %84[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %86 = llvm.insertvalue %83, %85[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %87 = llvm.insertvalue %26, %86[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %88 = llvm.insertvalue %30, %87[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %89 = llvm.insertvalue %29, %88[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.insertvalue %29, %89[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %91 = llvm.insertvalue %28, %90[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = builtin.unrealized_conversion_cast %91 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<262144x128xf32>
    llvm.br ^bb1(%16 : i64)
  ^bb1(%93: i64):  // 2 preds: ^bb0, ^bb5
    %94 = builtin.unrealized_conversion_cast %93 : i64 to index
    %95 = llvm.icmp "slt" %93, %17 : i64
    llvm.cond_br %95, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    %96 = builtin.unrealized_conversion_cast %94 : index to i64
    llvm.br ^bb3(%16 : i64)
  ^bb3(%97: i64):  // 2 preds: ^bb2, ^bb4
    %98 = builtin.unrealized_conversion_cast %97 : i64 to index
    %99 = llvm.icmp "slt" %97, %19 : i64
    llvm.cond_br %99, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %100 = builtin.unrealized_conversion_cast %98 : index to i64
    %101 = llvm.extractvalue %51[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %102 = llvm.mlir.constant(262144 : index) : i64
    %103 = llvm.mul %96, %102  : i64
    %104 = llvm.add %103, %100  : i64
    %105 = llvm.getelementptr %101[%104] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %106 = llvm.load %105 : !llvm.ptr -> f32
    %107 = llvm.extractvalue %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %108 = llvm.mlir.constant(128 : index) : i64
    %109 = llvm.mul %100, %108  : i64
    %110 = llvm.add %109, %96  : i64
    %111 = llvm.getelementptr %107[%110] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %106, %111 : f32, !llvm.ptr
    %112 = llvm.add %97, %18  : i64
    llvm.br ^bb3(%112 : i64)
  ^bb5:  // pred: ^bb3
    %113 = llvm.add %93, %18  : i64
    llvm.br ^bb1(%113 : i64)
  ^bb6:  // pred: ^bb1
    %114 = llvm.mlir.zero : !llvm.ptr
    %115 = llvm.getelementptr %114[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %116 = llvm.ptrtoint %115 : !llvm.ptr to i64
    %117 = llvm.add %116, %27  : i64
    %118 = llvm.call @malloc(%117) : (i64) -> !llvm.ptr
    %119 = llvm.ptrtoint %118 : !llvm.ptr to i64
    %120 = llvm.sub %27, %28  : i64
    %121 = llvm.add %119, %120  : i64
    %122 = llvm.urem %121, %27  : i64
    %123 = llvm.sub %121, %122  : i64
    %124 = llvm.inttoptr %123 : i64 to !llvm.ptr
    %125 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %126 = llvm.insertvalue %118, %125[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %127 = llvm.insertvalue %124, %126[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %128 = llvm.insertvalue %26, %127[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %129 = llvm.insertvalue %27, %128[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %130 = llvm.insertvalue %29, %129[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.insertvalue %29, %130[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %132 = llvm.insertvalue %28, %131[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = builtin.unrealized_conversion_cast %132 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<64x128xf32>
    %134 = llvm.call @xsmm_unary_dispatch(%46, %45, %44, %43, %45, %43, %42) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %135 = llvm.ptrtoint %124 : !llvm.ptr to i64
    %136 = builtin.unrealized_conversion_cast %135 : i64 to index
    %137 = llvm.inttoptr %135 : i64 to !llvm.ptr<f32>
    llvm.call @xsmm_unary_scalar_invoke(%45, %134, %31, %137, %16) : (i64, i64, f32, !llvm.ptr<f32>, i64) -> ()
    %138 = llvm.mlir.zero : !llvm.ptr
    %139 = llvm.getelementptr %138[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %140 = llvm.ptrtoint %139 : !llvm.ptr to i64
    %141 = llvm.add %140, %27  : i64
    %142 = llvm.call @malloc(%141) : (i64) -> !llvm.ptr
    %143 = llvm.ptrtoint %142 : !llvm.ptr to i64
    %144 = llvm.sub %27, %28  : i64
    %145 = llvm.add %143, %144  : i64
    %146 = llvm.urem %145, %27  : i64
    %147 = llvm.sub %145, %146  : i64
    %148 = llvm.inttoptr %147 : i64 to !llvm.ptr
    %149 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %150 = llvm.insertvalue %142, %149[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %151 = llvm.insertvalue %148, %150[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %152 = llvm.insertvalue %26, %151[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %153 = llvm.insertvalue %24, %152[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %154 = llvm.insertvalue %25, %153[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %155 = llvm.insertvalue %23, %154[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %156 = llvm.insertvalue %23, %155[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %157 = llvm.insertvalue %21, %156[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %158 = llvm.insertvalue %22, %157[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %159 = llvm.insertvalue %23, %158[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %160 = llvm.insertvalue %28, %159[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %161 = builtin.unrealized_conversion_cast %160 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<2x8192x32x32xf32>
    %162 = llvm.call @xsmm_unary_dispatch(%45, %45, %41, %41, %40, %41, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%35, %34) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.mul %arg6, %15  : i64
        %210 = llvm.mul %arg7, %14  : i64
        %211 = llvm.add %209, %210  : i64
        %212 = builtin.unrealized_conversion_cast %211 : i64 to index
        %213 = builtin.unrealized_conversion_cast %212 : index to i64
        %214 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %215 = llvm.extractvalue %65[0] : !llvm.struct<(ptr, ptr, i64)> 
        %216 = llvm.extractvalue %65[1] : !llvm.struct<(ptr, ptr, i64)> 
        %217 = llvm.insertvalue %215, %214[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.insertvalue %216, %217[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.insertvalue %213, %218[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.mlir.constant(32 : index) : i64
        %221 = llvm.insertvalue %220, %219[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.mlir.constant(262144 : index) : i64
        %223 = llvm.insertvalue %222, %221[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %224 = llvm.mlir.constant(32 : index) : i64
        %225 = llvm.insertvalue %224, %223[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %226 = llvm.mlir.constant(1 : index) : i64
        %227 = llvm.insertvalue %226, %225[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = builtin.unrealized_conversion_cast %227 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[262144, 1], offset: ?>>
        %229 = builtin.unrealized_conversion_cast %228 : memref<32x32xf32, strided<[262144, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %230 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %233 = llvm.insertvalue %230, %232[0] : !llvm.struct<(ptr, ptr, i64)> 
        %234 = llvm.insertvalue %231, %233[1] : !llvm.struct<(ptr, ptr, i64)> 
        %235 = llvm.mlir.constant(0 : index) : i64
        %236 = llvm.insertvalue %235, %234[2] : !llvm.struct<(ptr, ptr, i64)> 
        %237 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %238 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %239 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %240 = llvm.extractvalue %160[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %241 = llvm.extractvalue %160[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %242 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %243 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %244 = llvm.extractvalue %160[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %245 = llvm.extractvalue %160[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %246 = llvm.mul %arg6, %15  : i64
        %247 = llvm.mul %arg7, %13  : i64
        %248 = llvm.add %246, %247  : i64
        %249 = builtin.unrealized_conversion_cast %248 : i64 to index
        %250 = builtin.unrealized_conversion_cast %249 : index to i64
        %251 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %252 = llvm.extractvalue %236[0] : !llvm.struct<(ptr, ptr, i64)> 
        %253 = llvm.extractvalue %236[1] : !llvm.struct<(ptr, ptr, i64)> 
        %254 = llvm.insertvalue %252, %251[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %255 = llvm.insertvalue %253, %254[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %256 = llvm.insertvalue %250, %255[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %257 = llvm.mlir.constant(32 : index) : i64
        %258 = llvm.insertvalue %257, %256[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %259 = llvm.mlir.constant(32 : index) : i64
        %260 = llvm.insertvalue %259, %258[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %261 = llvm.mlir.constant(32 : index) : i64
        %262 = llvm.insertvalue %261, %260[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %263 = llvm.mlir.constant(1 : index) : i64
        %264 = llvm.insertvalue %263, %262[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %265 = builtin.unrealized_conversion_cast %264 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %266 = builtin.unrealized_conversion_cast %265 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %267 = llvm.extractvalue %229[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %268 = builtin.unrealized_conversion_cast %267 : i64 to index
        %269 = llvm.extractvalue %229[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %270 = llvm.ptrtoint %269 : !llvm.ptr to i64
        %271 = builtin.unrealized_conversion_cast %270 : i64 to index
        %272 = llvm.inttoptr %270 : i64 to !llvm.ptr<f32>
        %273 = llvm.extractvalue %266[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %274 = builtin.unrealized_conversion_cast %273 : i64 to index
        %275 = llvm.extractvalue %266[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %276 = llvm.ptrtoint %275 : !llvm.ptr to i64
        %277 = builtin.unrealized_conversion_cast %276 : i64 to index
        %278 = llvm.inttoptr %276 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %162, %272, %267, %278, %273) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %163 = llvm.mlir.zero : !llvm.ptr
    %164 = llvm.getelementptr %163[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %165 = llvm.ptrtoint %164 : !llvm.ptr to i64
    %166 = llvm.add %165, %27  : i64
    %167 = llvm.call @malloc(%166) : (i64) -> !llvm.ptr
    %168 = llvm.ptrtoint %167 : !llvm.ptr to i64
    %169 = llvm.sub %27, %28  : i64
    %170 = llvm.add %168, %169  : i64
    %171 = llvm.urem %170, %27  : i64
    %172 = llvm.sub %170, %171  : i64
    %173 = llvm.inttoptr %172 : i64 to !llvm.ptr
    %174 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %175 = llvm.insertvalue %167, %174[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %176 = llvm.insertvalue %173, %175[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %177 = llvm.insertvalue %26, %176[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %178 = llvm.insertvalue %20, %177[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %179 = llvm.insertvalue %25, %178[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %180 = llvm.insertvalue %23, %179[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %181 = llvm.insertvalue %23, %180[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %182 = llvm.insertvalue %21, %181[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %183 = llvm.insertvalue %22, %182[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %184 = llvm.insertvalue %23, %183[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %185 = llvm.insertvalue %28, %184[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %186 = builtin.unrealized_conversion_cast %185 : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> to memref<4x8192x32x32xf32>
    %187 = llvm.call @xsmm_unary_dispatch(%45, %45, %41, %41, %43, %41, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%33, %34) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.extractvalue %91[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %210 = llvm.extractvalue %91[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %211 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %212 = llvm.insertvalue %209, %211[0] : !llvm.struct<(ptr, ptr, i64)> 
        %213 = llvm.insertvalue %210, %212[1] : !llvm.struct<(ptr, ptr, i64)> 
        %214 = llvm.mlir.constant(0 : index) : i64
        %215 = llvm.insertvalue %214, %213[2] : !llvm.struct<(ptr, ptr, i64)> 
        %216 = llvm.extractvalue %91[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = llvm.extractvalue %91[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.extractvalue %91[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.extractvalue %91[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.extractvalue %91[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %221 = llvm.mul %arg7, %12  : i64
        %222 = llvm.mul %arg6, %14  : i64
        %223 = llvm.add %221, %222  : i64
        %224 = builtin.unrealized_conversion_cast %223 : i64 to index
        %225 = builtin.unrealized_conversion_cast %224 : index to i64
        %226 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %227 = llvm.extractvalue %215[0] : !llvm.struct<(ptr, ptr, i64)> 
        %228 = llvm.extractvalue %215[1] : !llvm.struct<(ptr, ptr, i64)> 
        %229 = llvm.insertvalue %227, %226[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %230 = llvm.insertvalue %228, %229[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %231 = llvm.insertvalue %225, %230[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %232 = llvm.mlir.constant(32 : index) : i64
        %233 = llvm.insertvalue %232, %231[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %234 = llvm.mlir.constant(128 : index) : i64
        %235 = llvm.insertvalue %234, %233[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %236 = llvm.mlir.constant(32 : index) : i64
        %237 = llvm.insertvalue %236, %235[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %238 = llvm.mlir.constant(1 : index) : i64
        %239 = llvm.insertvalue %238, %237[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %240 = builtin.unrealized_conversion_cast %239 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %241 = builtin.unrealized_conversion_cast %240 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %242 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %243 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %244 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %245 = llvm.insertvalue %242, %244[0] : !llvm.struct<(ptr, ptr, i64)> 
        %246 = llvm.insertvalue %243, %245[1] : !llvm.struct<(ptr, ptr, i64)> 
        %247 = llvm.mlir.constant(0 : index) : i64
        %248 = llvm.insertvalue %247, %246[2] : !llvm.struct<(ptr, ptr, i64)> 
        %249 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %250 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %251 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %252 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %253 = llvm.extractvalue %185[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %254 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %255 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %256 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %257 = llvm.extractvalue %185[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %258 = llvm.mul %arg6, %15  : i64
        %259 = llvm.mul %arg7, %13  : i64
        %260 = llvm.add %258, %259  : i64
        %261 = builtin.unrealized_conversion_cast %260 : i64 to index
        %262 = builtin.unrealized_conversion_cast %261 : index to i64
        %263 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %264 = llvm.extractvalue %248[0] : !llvm.struct<(ptr, ptr, i64)> 
        %265 = llvm.extractvalue %248[1] : !llvm.struct<(ptr, ptr, i64)> 
        %266 = llvm.insertvalue %264, %263[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %267 = llvm.insertvalue %265, %266[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %268 = llvm.insertvalue %262, %267[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %269 = llvm.mlir.constant(32 : index) : i64
        %270 = llvm.insertvalue %269, %268[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %271 = llvm.mlir.constant(32 : index) : i64
        %272 = llvm.insertvalue %271, %270[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %273 = llvm.mlir.constant(32 : index) : i64
        %274 = llvm.insertvalue %273, %272[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %275 = llvm.mlir.constant(1 : index) : i64
        %276 = llvm.insertvalue %275, %274[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %277 = builtin.unrealized_conversion_cast %276 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[32, 1], offset: ?>>
        %278 = builtin.unrealized_conversion_cast %277 : memref<32x32xf32, strided<[32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %279 = llvm.extractvalue %241[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %280 = builtin.unrealized_conversion_cast %279 : i64 to index
        %281 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %282 = llvm.ptrtoint %281 : !llvm.ptr to i64
        %283 = builtin.unrealized_conversion_cast %282 : i64 to index
        %284 = llvm.inttoptr %282 : i64 to !llvm.ptr<f32>
        %285 = llvm.extractvalue %278[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %286 = builtin.unrealized_conversion_cast %285 : i64 to index
        %287 = llvm.extractvalue %278[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %288 = llvm.ptrtoint %287 : !llvm.ptr to i64
        %289 = builtin.unrealized_conversion_cast %288 : i64 to index
        %290 = llvm.inttoptr %288 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %187, %284, %279, %290, %285) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %188 = llvm.extractvalue %54[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %189 = llvm.extractvalue %54[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %190 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %191 = llvm.insertvalue %188, %190[0] : !llvm.struct<(ptr, ptr, i64)> 
    %192 = llvm.insertvalue %189, %191[1] : !llvm.struct<(ptr, ptr, i64)> 
    %193 = llvm.mlir.constant(0 : index) : i64
    %194 = llvm.insertvalue %193, %192[2] : !llvm.struct<(ptr, ptr, i64)> 
    %195 = llvm.extractvalue %54[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %196 = llvm.extractvalue %54[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %197 = llvm.extractvalue %54[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %198 = llvm.call @xsmm_brgemm_dispatch(%45, %41, %41, %41, %41, %41, %41, %38, %38, %37) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %199 = llvm.call @xsmm_binary_dispatch(%45, %45, %41, %41, %41, %41, %41, %37) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %200 = llvm.call @xsmm_unary_dispatch(%36, %45, %41, %41, %41, %43, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%16, %16) to (%35, %33) step (%18, %18) {
        %208 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %209 = llvm.mlir.zero : !llvm.ptr
        %210 = llvm.getelementptr %209[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %211 = llvm.ptrtoint %210 : !llvm.ptr to i64
        %212 = llvm.add %211, %27  : i64
        %213 = llvm.call @malloc(%212) : (i64) -> !llvm.ptr
        %214 = llvm.ptrtoint %213 : !llvm.ptr to i64
        %215 = llvm.sub %27, %28  : i64
        %216 = llvm.add %214, %215  : i64
        %217 = llvm.urem %216, %27  : i64
        %218 = llvm.sub %216, %217  : i64
        %219 = llvm.inttoptr %218 : i64 to !llvm.ptr
        %220 = llvm.extractvalue %160[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %221 = llvm.extractvalue %160[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %222 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %223 = llvm.insertvalue %220, %222[0] : !llvm.struct<(ptr, ptr, i64)> 
        %224 = llvm.insertvalue %221, %223[1] : !llvm.struct<(ptr, ptr, i64)> 
        %225 = llvm.mlir.constant(0 : index) : i64
        %226 = llvm.insertvalue %225, %224[2] : !llvm.struct<(ptr, ptr, i64)> 
        %227 = llvm.extractvalue %160[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %228 = llvm.extractvalue %160[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %229 = llvm.extractvalue %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %230 = llvm.extractvalue %160[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %231 = llvm.extractvalue %160[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %232 = llvm.extractvalue %160[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %233 = llvm.extractvalue %160[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %234 = llvm.extractvalue %160[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %235 = llvm.extractvalue %160[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %236 = llvm.mul %arg6, %15  : i64
        %237 = builtin.unrealized_conversion_cast %236 : i64 to index
        %238 = builtin.unrealized_conversion_cast %237 : index to i64
        %239 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %240 = llvm.extractvalue %226[0] : !llvm.struct<(ptr, ptr, i64)> 
        %241 = llvm.extractvalue %226[1] : !llvm.struct<(ptr, ptr, i64)> 
        %242 = llvm.insertvalue %240, %239[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %243 = llvm.insertvalue %241, %242[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %244 = llvm.insertvalue %238, %243[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %245 = llvm.mlir.constant(8192 : index) : i64
        %246 = llvm.insertvalue %245, %244[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %247 = llvm.mlir.constant(1024 : index) : i64
        %248 = llvm.insertvalue %247, %246[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %249 = llvm.mlir.constant(32 : index) : i64
        %250 = llvm.insertvalue %249, %248[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %251 = llvm.mlir.constant(32 : index) : i64
        %252 = llvm.insertvalue %251, %250[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %253 = llvm.mlir.constant(32 : index) : i64
        %254 = llvm.insertvalue %253, %252[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %255 = llvm.mlir.constant(1 : index) : i64
        %256 = llvm.insertvalue %255, %254[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %257 = builtin.unrealized_conversion_cast %256 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %258 = builtin.unrealized_conversion_cast %257 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %259 = llvm.extractvalue %185[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %260 = llvm.extractvalue %185[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %261 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %262 = llvm.insertvalue %259, %261[0] : !llvm.struct<(ptr, ptr, i64)> 
        %263 = llvm.insertvalue %260, %262[1] : !llvm.struct<(ptr, ptr, i64)> 
        %264 = llvm.mlir.constant(0 : index) : i64
        %265 = llvm.insertvalue %264, %263[2] : !llvm.struct<(ptr, ptr, i64)> 
        %266 = llvm.extractvalue %185[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %267 = llvm.extractvalue %185[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %268 = llvm.extractvalue %185[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %269 = llvm.extractvalue %185[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %270 = llvm.extractvalue %185[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %271 = llvm.extractvalue %185[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %272 = llvm.extractvalue %185[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %273 = llvm.extractvalue %185[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %274 = llvm.extractvalue %185[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
        %275 = llvm.mul %arg7, %15  : i64
        %276 = builtin.unrealized_conversion_cast %275 : i64 to index
        %277 = builtin.unrealized_conversion_cast %276 : index to i64
        %278 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %279 = llvm.extractvalue %265[0] : !llvm.struct<(ptr, ptr, i64)> 
        %280 = llvm.extractvalue %265[1] : !llvm.struct<(ptr, ptr, i64)> 
        %281 = llvm.insertvalue %279, %278[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %282 = llvm.insertvalue %280, %281[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %283 = llvm.insertvalue %277, %282[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %284 = llvm.mlir.constant(8192 : index) : i64
        %285 = llvm.insertvalue %284, %283[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %286 = llvm.mlir.constant(1024 : index) : i64
        %287 = llvm.insertvalue %286, %285[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %288 = llvm.mlir.constant(32 : index) : i64
        %289 = llvm.insertvalue %288, %287[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %290 = llvm.mlir.constant(32 : index) : i64
        %291 = llvm.insertvalue %290, %289[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %292 = llvm.mlir.constant(32 : index) : i64
        %293 = llvm.insertvalue %292, %291[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %294 = llvm.mlir.constant(1 : index) : i64
        %295 = llvm.insertvalue %294, %293[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %296 = builtin.unrealized_conversion_cast %295 : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> to memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>>
        %297 = builtin.unrealized_conversion_cast %296 : memref<8192x32x32xf32, strided<[1024, 32, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %298 = llvm.extractvalue %258[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %299 = builtin.unrealized_conversion_cast %298 : i64 to index
        %300 = llvm.extractvalue %258[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %301 = llvm.ptrtoint %300 : !llvm.ptr to i64
        %302 = builtin.unrealized_conversion_cast %301 : i64 to index
        %303 = llvm.inttoptr %301 : i64 to !llvm.ptr<f32>
        %304 = llvm.extractvalue %297[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %305 = builtin.unrealized_conversion_cast %304 : i64 to index
        %306 = llvm.extractvalue %297[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %307 = llvm.ptrtoint %306 : !llvm.ptr to i64
        %308 = builtin.unrealized_conversion_cast %307 : i64 to index
        %309 = llvm.inttoptr %307 : i64 to !llvm.ptr<f32>
        %310 = llvm.ptrtoint %219 : !llvm.ptr to i64
        %311 = builtin.unrealized_conversion_cast %310 : i64 to index
        %312 = llvm.inttoptr %310 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_brgemm_invoke(%45, %198, %303, %298, %309, %304, %312, %16, %32) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) -> ()
        %313 = llvm.mul %arg7, %14  : i64
        %314 = builtin.unrealized_conversion_cast %313 : i64 to index
        %315 = builtin.unrealized_conversion_cast %314 : index to i64
        %316 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %317 = llvm.extractvalue %194[0] : !llvm.struct<(ptr, ptr, i64)> 
        %318 = llvm.extractvalue %194[1] : !llvm.struct<(ptr, ptr, i64)> 
        %319 = llvm.insertvalue %317, %316[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %320 = llvm.insertvalue %318, %319[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %321 = llvm.insertvalue %315, %320[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %322 = llvm.mlir.constant(32 : index) : i64
        %323 = llvm.insertvalue %322, %321[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %324 = llvm.mlir.constant(1 : index) : i64
        %325 = llvm.insertvalue %324, %323[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %326 = builtin.unrealized_conversion_cast %325 : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> to memref<32xf32, strided<[1], offset: ?>>
        %327 = builtin.unrealized_conversion_cast %326 : memref<32xf32, strided<[1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %328 = llvm.extractvalue %327[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %329 = builtin.unrealized_conversion_cast %328 : i64 to index
        %330 = llvm.extractvalue %327[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %331 = llvm.ptrtoint %330 : !llvm.ptr to i64
        %332 = builtin.unrealized_conversion_cast %331 : i64 to index
        %333 = llvm.inttoptr %331 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_binary_invoke(%45, %199, %333, %328, %312, %16, %312, %16) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        %334 = llvm.extractvalue %132[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %335 = llvm.extractvalue %132[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %336 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %337 = llvm.insertvalue %334, %336[0] : !llvm.struct<(ptr, ptr, i64)> 
        %338 = llvm.insertvalue %335, %337[1] : !llvm.struct<(ptr, ptr, i64)> 
        %339 = llvm.mlir.constant(0 : index) : i64
        %340 = llvm.insertvalue %339, %338[2] : !llvm.struct<(ptr, ptr, i64)> 
        %341 = llvm.extractvalue %132[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %342 = llvm.extractvalue %132[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %343 = llvm.extractvalue %132[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %344 = llvm.extractvalue %132[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %345 = llvm.extractvalue %132[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %346 = llvm.mul %arg6, %12  : i64
        %347 = llvm.mul %arg7, %14  : i64
        %348 = llvm.add %346, %347  : i64
        %349 = builtin.unrealized_conversion_cast %348 : i64 to index
        %350 = builtin.unrealized_conversion_cast %349 : index to i64
        %351 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %352 = llvm.extractvalue %340[0] : !llvm.struct<(ptr, ptr, i64)> 
        %353 = llvm.extractvalue %340[1] : !llvm.struct<(ptr, ptr, i64)> 
        %354 = llvm.insertvalue %352, %351[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %355 = llvm.insertvalue %353, %354[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %356 = llvm.insertvalue %350, %355[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %357 = llvm.mlir.constant(32 : index) : i64
        %358 = llvm.insertvalue %357, %356[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %359 = llvm.mlir.constant(128 : index) : i64
        %360 = llvm.insertvalue %359, %358[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %361 = llvm.mlir.constant(32 : index) : i64
        %362 = llvm.insertvalue %361, %360[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %363 = llvm.mlir.constant(1 : index) : i64
        %364 = llvm.insertvalue %363, %362[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %365 = builtin.unrealized_conversion_cast %364 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> to memref<32x32xf32, strided<[128, 1], offset: ?>>
        %366 = builtin.unrealized_conversion_cast %365 : memref<32x32xf32, strided<[128, 1], offset: ?>> to !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %367 = llvm.extractvalue %366[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %368 = builtin.unrealized_conversion_cast %367 : i64 to index
        %369 = llvm.extractvalue %366[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %370 = llvm.ptrtoint %369 : !llvm.ptr to i64
        %371 = builtin.unrealized_conversion_cast %370 : i64 to index
        %372 = llvm.inttoptr %370 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%45, %200, %312, %16, %372, %367) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.call @free(%213) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %208 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%77) : (!llvm.ptr) -> ()
    llvm.call @free(%142) : (!llvm.ptr) -> ()
    llvm.call @free(%167) : (!llvm.ptr) -> ()
    %201 = llvm.alloca %28 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %132, %201 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %202 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %203 = llvm.insertvalue %24, %202[0] : !llvm.struct<(i64, ptr)> 
    %204 = llvm.insertvalue %201, %203[1] : !llvm.struct<(i64, ptr)> 
    %205 = builtin.unrealized_conversion_cast %204 : !llvm.struct<(i64, ptr)> to memref<*xf32>
    %206 = llvm.extractvalue %204[0] : !llvm.struct<(i64, ptr)> 
    %207 = llvm.extractvalue %204[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @refbackend_consume_func_return_mrf32(%206, %207) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_MLP(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg0 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %4 = llvm.extractvalue %3[0] : !llvm.struct<(i64, ptr)> 
    %5 = llvm.extractvalue %3[1] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(i64, ptr)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @MLP(%1, %2, %4, %5, %7, %8) : (i64, !llvm.ptr, i64, !llvm.ptr, i64, !llvm.ptr) -> ()
    llvm.return
  }
}


// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func private @refbackend_consume_func_return_mrf32(%arg0: i64, %arg1: !llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.mlir.constant(1 : index) : i64
    %4 = llvm.alloca %3 x !llvm.struct<(i64, ptr)> : (i64) -> !llvm.ptr
    llvm.store %2, %4 : !llvm.struct<(i64, ptr)>, !llvm.ptr
    llvm.call @_mlir_ciface_refbackend_consume_func_return_mrf32(%4) : (!llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_refbackend_consume_func_return_mrf32(!llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"}
  llvm.func @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @MLP(%arg0: i64, %arg1: !llvm.ptr, %arg2: i64, %arg3: !llvm.ptr, %arg4: i64, %arg5: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %4 = llvm.insertvalue %arg2, %3[0] : !llvm.struct<(i64, ptr)> 
    %5 = llvm.insertvalue %arg3, %4[1] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %7 = llvm.insertvalue %arg4, %6[0] : !llvm.struct<(i64, ptr)> 
    %8 = llvm.insertvalue %arg5, %7[1] : !llvm.struct<(i64, ptr)> 
    %9 = llvm.mlir.constant(4096 : index) : i64
    %10 = llvm.mlir.constant(1024 : index) : i64
    %11 = llvm.mlir.constant(32 : index) : i64
    %12 = llvm.mlir.constant(8388608 : index) : i64
    %13 = llvm.mlir.constant(0 : index) : i64
    %14 = llvm.mlir.constant(128 : index) : i64
    %15 = llvm.mlir.constant(1 : index) : i64
    %16 = llvm.mlir.constant(262144 : index) : i64
    %17 = llvm.mlir.constant(4 : index) : i64
    %18 = llvm.mlir.constant(8388608 : index) : i64
    %19 = llvm.mlir.constant(1024 : index) : i64
    %20 = llvm.mlir.constant(32 : index) : i64
    %21 = llvm.mlir.constant(2 : index) : i64
    %22 = llvm.mlir.constant(8192 : index) : i64
    %23 = llvm.mlir.constant(0 : index) : i64
    %24 = llvm.mlir.constant(64 : index) : i64
    %25 = llvm.mlir.constant(1 : index) : i64
    %26 = llvm.mlir.constant(128 : index) : i64
    %27 = llvm.mlir.constant(262144 : index) : i64
    %28 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %29 = llvm.mlir.constant(8192 : i64) : i64
    %30 = llvm.mlir.constant(4 : index) : i64
    %31 = llvm.mlir.constant(8192 : index) : i64
    %32 = llvm.mlir.constant(2 : index) : i64
    %33 = llvm.mlir.constant(5 : i64) : i64
    %34 = llvm.mlir.constant(4 : i64) : i64
    %35 = llvm.mlir.constant(1024 : i64) : i64
    %36 = llvm.mlir.constant(0 : i64) : i64
    %37 = llvm.mlir.constant(262144 : i64) : i64
    %38 = llvm.mlir.constant(32 : i64) : i64
    %39 = llvm.mlir.constant(8 : i64) : i64
    %40 = llvm.mlir.constant(128 : i64) : i64
    %41 = llvm.mlir.constant(64 : i64) : i64
    %42 = llvm.mlir.constant(1 : i64) : i64
    %43 = llvm.mlir.constant(2 : i64) : i64
    %44 = llvm.extractvalue %2[1] : !llvm.struct<(i64, ptr)> 
    %45 = llvm.load %44 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %46 = llvm.extractvalue %5[1] : !llvm.struct<(i64, ptr)> 
    %47 = llvm.load %46 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %48 = llvm.extractvalue %8[1] : !llvm.struct<(i64, ptr)> 
    %49 = llvm.load %48 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %50 = llvm.extractvalue %49[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %51 = llvm.extractvalue %49[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %52 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %53 = llvm.insertvalue %50, %52[0] : !llvm.struct<(ptr, ptr, i64)> 
    %54 = llvm.insertvalue %51, %53[1] : !llvm.struct<(ptr, ptr, i64)> 
    %55 = llvm.mlir.constant(0 : index) : i64
    %56 = llvm.insertvalue %55, %54[2] : !llvm.struct<(ptr, ptr, i64)> 
    %57 = llvm.extractvalue %49[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %58 = llvm.extractvalue %49[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %59 = llvm.extractvalue %49[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %60 = llvm.extractvalue %49[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %61 = llvm.extractvalue %49[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %62 = llvm.extractvalue %49[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %63 = llvm.extractvalue %49[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %64 = llvm.mlir.zero : !llvm.ptr
    %65 = llvm.getelementptr %64[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %66 = llvm.ptrtoint %65 : !llvm.ptr to i64
    %67 = llvm.add %66, %24  : i64
    %68 = llvm.call @malloc(%67) : (i64) -> !llvm.ptr
    %69 = llvm.ptrtoint %68 : !llvm.ptr to i64
    %70 = llvm.sub %24, %25  : i64
    %71 = llvm.add %69, %70  : i64
    %72 = llvm.urem %71, %24  : i64
    %73 = llvm.sub %71, %72  : i64
    %74 = llvm.inttoptr %73 : i64 to !llvm.ptr
    %75 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %76 = llvm.insertvalue %68, %75[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %77 = llvm.insertvalue %74, %76[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %78 = llvm.insertvalue %23, %77[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %79 = llvm.insertvalue %27, %78[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %80 = llvm.insertvalue %26, %79[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %81 = llvm.insertvalue %26, %80[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %82 = llvm.insertvalue %25, %81[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%13 : i64)
  ^bb1(%83: i64):  // 2 preds: ^bb0, ^bb5
    %84 = llvm.icmp "slt" %83, %14 : i64
    llvm.cond_br %84, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    llvm.br ^bb3(%13 : i64)
  ^bb3(%85: i64):  // 2 preds: ^bb2, ^bb4
    %86 = llvm.icmp "slt" %85, %16 : i64
    llvm.cond_br %86, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %87 = llvm.extractvalue %45[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %88 = llvm.mlir.constant(262144 : index) : i64
    %89 = llvm.mul %83, %88  : i64
    %90 = llvm.add %89, %85  : i64
    %91 = llvm.getelementptr %87[%90] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %92 = llvm.load %91 : !llvm.ptr -> f32
    %93 = llvm.mlir.constant(128 : index) : i64
    %94 = llvm.mul %85, %93  : i64
    %95 = llvm.add %94, %83  : i64
    %96 = llvm.getelementptr %74[%95] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %92, %96 : f32, !llvm.ptr
    %97 = llvm.add %85, %15  : i64
    llvm.br ^bb3(%97 : i64)
  ^bb5:  // pred: ^bb3
    %98 = llvm.add %83, %15  : i64
    llvm.br ^bb1(%98 : i64)
  ^bb6:  // pred: ^bb1
    %99 = llvm.mlir.zero : !llvm.ptr
    %100 = llvm.getelementptr %99[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %101 = llvm.ptrtoint %100 : !llvm.ptr to i64
    %102 = llvm.add %101, %24  : i64
    %103 = llvm.call @malloc(%102) : (i64) -> !llvm.ptr
    %104 = llvm.ptrtoint %103 : !llvm.ptr to i64
    %105 = llvm.sub %24, %25  : i64
    %106 = llvm.add %104, %105  : i64
    %107 = llvm.urem %106, %24  : i64
    %108 = llvm.sub %106, %107  : i64
    %109 = llvm.inttoptr %108 : i64 to !llvm.ptr
    %110 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %111 = llvm.insertvalue %103, %110[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %112 = llvm.insertvalue %109, %111[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %113 = llvm.insertvalue %23, %112[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %114 = llvm.insertvalue %24, %113[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %115 = llvm.insertvalue %26, %114[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %116 = llvm.insertvalue %26, %115[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %117 = llvm.insertvalue %25, %116[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %118 = llvm.call @xsmm_unary_dispatch(%43, %42, %41, %40, %42, %40, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = llvm.ptrtoint %109 : !llvm.ptr to i64
    %120 = llvm.inttoptr %119 : i64 to !llvm.ptr<f32>
    llvm.call @xsmm_unary_scalar_invoke(%42, %118, %28, %120, %13) : (i64, i64, f32, !llvm.ptr<f32>, i64) -> ()
    %121 = llvm.mlir.zero : !llvm.ptr
    %122 = llvm.getelementptr %121[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %123 = llvm.ptrtoint %122 : !llvm.ptr to i64
    %124 = llvm.add %123, %24  : i64
    %125 = llvm.call @malloc(%124) : (i64) -> !llvm.ptr
    %126 = llvm.ptrtoint %125 : !llvm.ptr to i64
    %127 = llvm.sub %24, %25  : i64
    %128 = llvm.add %126, %127  : i64
    %129 = llvm.urem %128, %24  : i64
    %130 = llvm.sub %128, %129  : i64
    %131 = llvm.inttoptr %130 : i64 to !llvm.ptr
    %132 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %133 = llvm.insertvalue %125, %132[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %134 = llvm.insertvalue %131, %133[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %135 = llvm.insertvalue %23, %134[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %136 = llvm.insertvalue %21, %135[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %137 = llvm.insertvalue %22, %136[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %138 = llvm.insertvalue %20, %137[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %139 = llvm.insertvalue %20, %138[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %140 = llvm.insertvalue %18, %139[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %141 = llvm.insertvalue %19, %140[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %142 = llvm.insertvalue %20, %141[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %143 = llvm.insertvalue %25, %142[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %144 = llvm.call @xsmm_unary_dispatch(%42, %42, %38, %38, %37, %38, %36) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%13, %13) to (%32, %31) step (%15, %15) {
        %186 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %187 = llvm.mul %arg6, %12  : i64
        %188 = llvm.mul %arg7, %11  : i64
        %189 = llvm.add %187, %188  : i64
        %190 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %191 = llvm.insertvalue %50, %190[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %192 = llvm.insertvalue %51, %191[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %193 = llvm.insertvalue %189, %192[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %194 = llvm.mlir.constant(32 : index) : i64
        %195 = llvm.insertvalue %194, %193[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %196 = llvm.mlir.constant(262144 : index) : i64
        %197 = llvm.insertvalue %196, %195[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %198 = llvm.mlir.constant(32 : index) : i64
        %199 = llvm.insertvalue %198, %197[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %200 = llvm.mlir.constant(1 : index) : i64
        %201 = llvm.insertvalue %200, %199[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %202 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %203 = llvm.insertvalue %125, %202[0] : !llvm.struct<(ptr, ptr, i64)> 
        %204 = llvm.insertvalue %131, %203[1] : !llvm.struct<(ptr, ptr, i64)> 
        %205 = llvm.mlir.constant(0 : index) : i64
        %206 = llvm.insertvalue %205, %204[2] : !llvm.struct<(ptr, ptr, i64)> 
        %207 = llvm.mul %arg6, %12  : i64
        %208 = llvm.mul %arg7, %10  : i64
        %209 = llvm.add %207, %208  : i64
        %210 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %211 = llvm.insertvalue %125, %210[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %212 = llvm.insertvalue %131, %211[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %213 = llvm.insertvalue %209, %212[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %214 = llvm.mlir.constant(32 : index) : i64
        %215 = llvm.insertvalue %214, %213[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %216 = llvm.mlir.constant(32 : index) : i64
        %217 = llvm.insertvalue %216, %215[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.mlir.constant(32 : index) : i64
        %219 = llvm.insertvalue %218, %217[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.mlir.constant(1 : index) : i64
        %221 = llvm.insertvalue %220, %219[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.extractvalue %201[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %223 = llvm.extractvalue %201[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %224 = llvm.ptrtoint %223 : !llvm.ptr to i64
        %225 = llvm.inttoptr %224 : i64 to !llvm.ptr<f32>
        %226 = llvm.extractvalue %221[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = llvm.extractvalue %221[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = llvm.ptrtoint %227 : !llvm.ptr to i64
        %229 = llvm.inttoptr %228 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%42, %144, %225, %222, %229, %226) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %186 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %145 = llvm.mlir.zero : !llvm.ptr
    %146 = llvm.getelementptr %145[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %147 = llvm.ptrtoint %146 : !llvm.ptr to i64
    %148 = llvm.add %147, %24  : i64
    %149 = llvm.call @malloc(%148) : (i64) -> !llvm.ptr
    %150 = llvm.ptrtoint %149 : !llvm.ptr to i64
    %151 = llvm.sub %24, %25  : i64
    %152 = llvm.add %150, %151  : i64
    %153 = llvm.urem %152, %24  : i64
    %154 = llvm.sub %152, %153  : i64
    %155 = llvm.inttoptr %154 : i64 to !llvm.ptr
    %156 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %157 = llvm.insertvalue %149, %156[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %158 = llvm.insertvalue %155, %157[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %159 = llvm.insertvalue %23, %158[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %160 = llvm.insertvalue %17, %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %161 = llvm.insertvalue %22, %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %162 = llvm.insertvalue %20, %161[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %163 = llvm.insertvalue %20, %162[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %164 = llvm.insertvalue %18, %163[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %165 = llvm.insertvalue %19, %164[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %166 = llvm.insertvalue %20, %165[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %167 = llvm.insertvalue %25, %166[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %168 = llvm.call @xsmm_unary_dispatch(%42, %42, %38, %38, %40, %38, %36) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%13, %13) to (%30, %31) step (%15, %15) {
        %186 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %187 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %188 = llvm.insertvalue %68, %187[0] : !llvm.struct<(ptr, ptr, i64)> 
        %189 = llvm.insertvalue %74, %188[1] : !llvm.struct<(ptr, ptr, i64)> 
        %190 = llvm.mlir.constant(0 : index) : i64
        %191 = llvm.insertvalue %190, %189[2] : !llvm.struct<(ptr, ptr, i64)> 
        %192 = llvm.mul %arg7, %9  : i64
        %193 = llvm.mul %arg6, %11  : i64
        %194 = llvm.add %192, %193  : i64
        %195 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %196 = llvm.insertvalue %68, %195[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %197 = llvm.insertvalue %74, %196[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %198 = llvm.insertvalue %194, %197[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %199 = llvm.mlir.constant(32 : index) : i64
        %200 = llvm.insertvalue %199, %198[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %201 = llvm.mlir.constant(128 : index) : i64
        %202 = llvm.insertvalue %201, %200[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %203 = llvm.mlir.constant(32 : index) : i64
        %204 = llvm.insertvalue %203, %202[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %205 = llvm.mlir.constant(1 : index) : i64
        %206 = llvm.insertvalue %205, %204[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %207 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %208 = llvm.insertvalue %149, %207[0] : !llvm.struct<(ptr, ptr, i64)> 
        %209 = llvm.insertvalue %155, %208[1] : !llvm.struct<(ptr, ptr, i64)> 
        %210 = llvm.mlir.constant(0 : index) : i64
        %211 = llvm.insertvalue %210, %209[2] : !llvm.struct<(ptr, ptr, i64)> 
        %212 = llvm.mul %arg6, %12  : i64
        %213 = llvm.mul %arg7, %10  : i64
        %214 = llvm.add %212, %213  : i64
        %215 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %216 = llvm.insertvalue %149, %215[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = llvm.insertvalue %155, %216[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.insertvalue %214, %217[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.mlir.constant(32 : index) : i64
        %220 = llvm.insertvalue %219, %218[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %221 = llvm.mlir.constant(32 : index) : i64
        %222 = llvm.insertvalue %221, %220[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %223 = llvm.mlir.constant(32 : index) : i64
        %224 = llvm.insertvalue %223, %222[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %225 = llvm.mlir.constant(1 : index) : i64
        %226 = llvm.insertvalue %225, %224[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = llvm.extractvalue %206[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = llvm.extractvalue %206[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %229 = llvm.ptrtoint %228 : !llvm.ptr to i64
        %230 = llvm.inttoptr %229 : i64 to !llvm.ptr<f32>
        %231 = llvm.extractvalue %226[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %232 = llvm.extractvalue %226[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %233 = llvm.ptrtoint %232 : !llvm.ptr to i64
        %234 = llvm.inttoptr %233 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%42, %168, %230, %227, %234, %231) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %186 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %169 = llvm.extractvalue %47[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %170 = llvm.extractvalue %47[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %171 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %172 = llvm.insertvalue %169, %171[0] : !llvm.struct<(ptr, ptr, i64)> 
    %173 = llvm.insertvalue %170, %172[1] : !llvm.struct<(ptr, ptr, i64)> 
    %174 = llvm.mlir.constant(0 : index) : i64
    %175 = llvm.insertvalue %174, %173[2] : !llvm.struct<(ptr, ptr, i64)> 
    %176 = llvm.extractvalue %47[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %177 = llvm.extractvalue %47[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %178 = llvm.extractvalue %47[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %179 = llvm.call @xsmm_brgemm_dispatch(%42, %38, %38, %38, %38, %38, %38, %35, %35, %34) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %180 = llvm.call @xsmm_binary_dispatch(%42, %42, %38, %38, %38, %38, %38, %34) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %181 = llvm.call @xsmm_unary_dispatch(%33, %42, %38, %38, %38, %40, %36) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%13, %13) to (%32, %30) step (%15, %15) {
        %186 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %187 = llvm.mlir.zero : !llvm.ptr
        %188 = llvm.getelementptr %187[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %189 = llvm.ptrtoint %188 : !llvm.ptr to i64
        %190 = llvm.add %189, %24  : i64
        %191 = llvm.call @malloc(%190) : (i64) -> !llvm.ptr
        %192 = llvm.ptrtoint %191 : !llvm.ptr to i64
        %193 = llvm.sub %24, %25  : i64
        %194 = llvm.add %192, %193  : i64
        %195 = llvm.urem %194, %24  : i64
        %196 = llvm.sub %194, %195  : i64
        %197 = llvm.inttoptr %196 : i64 to !llvm.ptr
        %198 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %199 = llvm.insertvalue %125, %198[0] : !llvm.struct<(ptr, ptr, i64)> 
        %200 = llvm.insertvalue %131, %199[1] : !llvm.struct<(ptr, ptr, i64)> 
        %201 = llvm.mlir.constant(0 : index) : i64
        %202 = llvm.insertvalue %201, %200[2] : !llvm.struct<(ptr, ptr, i64)> 
        %203 = llvm.mul %arg6, %12  : i64
        %204 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %205 = llvm.insertvalue %125, %204[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %206 = llvm.insertvalue %131, %205[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %207 = llvm.insertvalue %203, %206[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %208 = llvm.mlir.constant(8192 : index) : i64
        %209 = llvm.insertvalue %208, %207[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %210 = llvm.mlir.constant(1024 : index) : i64
        %211 = llvm.insertvalue %210, %209[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %212 = llvm.mlir.constant(32 : index) : i64
        %213 = llvm.insertvalue %212, %211[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %214 = llvm.mlir.constant(32 : index) : i64
        %215 = llvm.insertvalue %214, %213[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %216 = llvm.mlir.constant(32 : index) : i64
        %217 = llvm.insertvalue %216, %215[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %218 = llvm.mlir.constant(1 : index) : i64
        %219 = llvm.insertvalue %218, %217[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %220 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %221 = llvm.insertvalue %149, %220[0] : !llvm.struct<(ptr, ptr, i64)> 
        %222 = llvm.insertvalue %155, %221[1] : !llvm.struct<(ptr, ptr, i64)> 
        %223 = llvm.mlir.constant(0 : index) : i64
        %224 = llvm.insertvalue %223, %222[2] : !llvm.struct<(ptr, ptr, i64)> 
        %225 = llvm.mul %arg7, %12  : i64
        %226 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %227 = llvm.insertvalue %149, %226[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %228 = llvm.insertvalue %155, %227[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %229 = llvm.insertvalue %225, %228[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %230 = llvm.mlir.constant(8192 : index) : i64
        %231 = llvm.insertvalue %230, %229[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %232 = llvm.mlir.constant(1024 : index) : i64
        %233 = llvm.insertvalue %232, %231[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %234 = llvm.mlir.constant(32 : index) : i64
        %235 = llvm.insertvalue %234, %233[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %236 = llvm.mlir.constant(32 : index) : i64
        %237 = llvm.insertvalue %236, %235[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %238 = llvm.mlir.constant(32 : index) : i64
        %239 = llvm.insertvalue %238, %237[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %240 = llvm.mlir.constant(1 : index) : i64
        %241 = llvm.insertvalue %240, %239[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %242 = llvm.extractvalue %219[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %243 = llvm.extractvalue %219[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %244 = llvm.ptrtoint %243 : !llvm.ptr to i64
        %245 = llvm.inttoptr %244 : i64 to !llvm.ptr<f32>
        %246 = llvm.extractvalue %241[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %247 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %248 = llvm.ptrtoint %247 : !llvm.ptr to i64
        %249 = llvm.inttoptr %248 : i64 to !llvm.ptr<f32>
        %250 = llvm.ptrtoint %197 : !llvm.ptr to i64
        %251 = llvm.inttoptr %250 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_brgemm_invoke(%42, %179, %245, %242, %249, %246, %251, %13, %29) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) -> ()
        %252 = llvm.mul %arg7, %11  : i64
        %253 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %254 = llvm.insertvalue %169, %253[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %255 = llvm.insertvalue %170, %254[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %256 = llvm.insertvalue %252, %255[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %257 = llvm.mlir.constant(32 : index) : i64
        %258 = llvm.insertvalue %257, %256[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %259 = llvm.mlir.constant(1 : index) : i64
        %260 = llvm.insertvalue %259, %258[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %261 = llvm.extractvalue %260[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %262 = llvm.extractvalue %260[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %263 = llvm.ptrtoint %262 : !llvm.ptr to i64
        %264 = llvm.inttoptr %263 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_binary_invoke(%42, %180, %264, %261, %251, %13, %251, %13) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        %265 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %266 = llvm.insertvalue %103, %265[0] : !llvm.struct<(ptr, ptr, i64)> 
        %267 = llvm.insertvalue %109, %266[1] : !llvm.struct<(ptr, ptr, i64)> 
        %268 = llvm.mlir.constant(0 : index) : i64
        %269 = llvm.insertvalue %268, %267[2] : !llvm.struct<(ptr, ptr, i64)> 
        %270 = llvm.mul %arg6, %9  : i64
        %271 = llvm.mul %arg7, %11  : i64
        %272 = llvm.add %270, %271  : i64
        %273 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %274 = llvm.insertvalue %103, %273[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %275 = llvm.insertvalue %109, %274[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %276 = llvm.insertvalue %272, %275[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %277 = llvm.mlir.constant(32 : index) : i64
        %278 = llvm.insertvalue %277, %276[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %279 = llvm.mlir.constant(128 : index) : i64
        %280 = llvm.insertvalue %279, %278[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %281 = llvm.mlir.constant(32 : index) : i64
        %282 = llvm.insertvalue %281, %280[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %283 = llvm.mlir.constant(1 : index) : i64
        %284 = llvm.insertvalue %283, %282[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %285 = llvm.extractvalue %284[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %286 = llvm.extractvalue %284[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %287 = llvm.ptrtoint %286 : !llvm.ptr to i64
        %288 = llvm.inttoptr %287 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%42, %181, %251, %13, %288, %285) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.call @free(%191) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %186 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%68) : (!llvm.ptr) -> ()
    llvm.call @free(%125) : (!llvm.ptr) -> ()
    llvm.call @free(%149) : (!llvm.ptr) -> ()
    %182 = llvm.alloca %25 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %117, %182 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %183 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %184 = llvm.insertvalue %21, %183[0] : !llvm.struct<(i64, ptr)> 
    %185 = llvm.insertvalue %182, %184[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @refbackend_consume_func_return_mrf32(%21, %182) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_MLP(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg0 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %4 = llvm.extractvalue %3[0] : !llvm.struct<(i64, ptr)> 
    %5 = llvm.extractvalue %3[1] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(i64, ptr)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @MLP(%1, %2, %4, %5, %7, %8) : (i64, !llvm.ptr, i64, !llvm.ptr, i64, !llvm.ptr) -> ()
    llvm.return
  }
}


// -----// IR Dump Before SymbolDCE (symbol-dce) ('builtin.module' operation) //----- //
module attributes {torch.debug_module_name = "MLP"} {
  llvm.func @free(!llvm.ptr)
  llvm.func @malloc(i64) -> !llvm.ptr
  llvm.func private @refbackend_consume_func_return_mrf32(%arg0: i64, %arg1: !llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.mlir.constant(1 : index) : i64
    %4 = llvm.alloca %3 x !llvm.struct<(i64, ptr)> : (i64) -> !llvm.ptr
    llvm.store %2, %4 : !llvm.struct<(i64, ptr)>, !llvm.ptr
    llvm.call @_mlir_ciface_refbackend_consume_func_return_mrf32(%4) : (!llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_refbackend_consume_func_return_mrf32(!llvm.ptr) attributes {llvm.emit_c_interface, sym_visibility = "private"}
  llvm.func @xsmm_unary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_invoke(i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_dispatch(i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_binary_dispatch(i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_brgemm_dispatch(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 attributes {sym_visibility = "private"}
  llvm.func @xsmm_unary_scalar_invoke(i64, i64, f32, !llvm.ptr<f32>, i64) attributes {sym_visibility = "private"}
  llvm.func @MLP(%arg0: i64, %arg1: !llvm.ptr, %arg2: i64, %arg3: !llvm.ptr, %arg4: i64, %arg5: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %1 = llvm.insertvalue %arg0, %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.insertvalue %arg1, %1[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %4 = llvm.insertvalue %arg2, %3[0] : !llvm.struct<(i64, ptr)> 
    %5 = llvm.insertvalue %arg3, %4[1] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %7 = llvm.insertvalue %arg4, %6[0] : !llvm.struct<(i64, ptr)> 
    %8 = llvm.insertvalue %arg5, %7[1] : !llvm.struct<(i64, ptr)> 
    %9 = llvm.mlir.constant(4096 : index) : i64
    %10 = llvm.mlir.constant(1024 : index) : i64
    %11 = llvm.mlir.constant(32 : index) : i64
    %12 = llvm.mlir.constant(8388608 : index) : i64
    %13 = llvm.mlir.constant(0 : index) : i64
    %14 = llvm.mlir.constant(128 : index) : i64
    %15 = llvm.mlir.constant(1 : index) : i64
    %16 = llvm.mlir.constant(262144 : index) : i64
    %17 = llvm.mlir.constant(4 : index) : i64
    %18 = llvm.mlir.constant(8388608 : index) : i64
    %19 = llvm.mlir.constant(1024 : index) : i64
    %20 = llvm.mlir.constant(32 : index) : i64
    %21 = llvm.mlir.constant(2 : index) : i64
    %22 = llvm.mlir.constant(8192 : index) : i64
    %23 = llvm.mlir.constant(0 : index) : i64
    %24 = llvm.mlir.constant(64 : index) : i64
    %25 = llvm.mlir.constant(1 : index) : i64
    %26 = llvm.mlir.constant(128 : index) : i64
    %27 = llvm.mlir.constant(262144 : index) : i64
    %28 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %29 = llvm.mlir.constant(8192 : i64) : i64
    %30 = llvm.mlir.constant(4 : index) : i64
    %31 = llvm.mlir.constant(8192 : index) : i64
    %32 = llvm.mlir.constant(2 : index) : i64
    %33 = llvm.mlir.constant(5 : i64) : i64
    %34 = llvm.mlir.constant(4 : i64) : i64
    %35 = llvm.mlir.constant(1024 : i64) : i64
    %36 = llvm.mlir.constant(0 : i64) : i64
    %37 = llvm.mlir.constant(262144 : i64) : i64
    %38 = llvm.mlir.constant(32 : i64) : i64
    %39 = llvm.mlir.constant(8 : i64) : i64
    %40 = llvm.mlir.constant(128 : i64) : i64
    %41 = llvm.mlir.constant(64 : i64) : i64
    %42 = llvm.mlir.constant(1 : i64) : i64
    %43 = llvm.mlir.constant(2 : i64) : i64
    %44 = llvm.extractvalue %2[1] : !llvm.struct<(i64, ptr)> 
    %45 = llvm.load %44 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %46 = llvm.extractvalue %5[1] : !llvm.struct<(i64, ptr)> 
    %47 = llvm.load %46 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
    %48 = llvm.extractvalue %8[1] : !llvm.struct<(i64, ptr)> 
    %49 = llvm.load %48 : !llvm.ptr -> !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
    %50 = llvm.extractvalue %49[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %51 = llvm.extractvalue %49[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %52 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %53 = llvm.insertvalue %50, %52[0] : !llvm.struct<(ptr, ptr, i64)> 
    %54 = llvm.insertvalue %51, %53[1] : !llvm.struct<(ptr, ptr, i64)> 
    %55 = llvm.mlir.constant(0 : index) : i64
    %56 = llvm.insertvalue %55, %54[2] : !llvm.struct<(ptr, ptr, i64)> 
    %57 = llvm.extractvalue %49[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %58 = llvm.extractvalue %49[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %59 = llvm.extractvalue %49[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %60 = llvm.extractvalue %49[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %61 = llvm.extractvalue %49[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %62 = llvm.extractvalue %49[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %63 = llvm.extractvalue %49[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
    %64 = llvm.mlir.zero : !llvm.ptr
    %65 = llvm.getelementptr %64[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %66 = llvm.ptrtoint %65 : !llvm.ptr to i64
    %67 = llvm.add %66, %24  : i64
    %68 = llvm.call @malloc(%67) : (i64) -> !llvm.ptr
    %69 = llvm.ptrtoint %68 : !llvm.ptr to i64
    %70 = llvm.sub %24, %25  : i64
    %71 = llvm.add %69, %70  : i64
    %72 = llvm.urem %71, %24  : i64
    %73 = llvm.sub %71, %72  : i64
    %74 = llvm.inttoptr %73 : i64 to !llvm.ptr
    %75 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %76 = llvm.insertvalue %68, %75[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %77 = llvm.insertvalue %74, %76[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %78 = llvm.insertvalue %23, %77[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %79 = llvm.insertvalue %27, %78[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %80 = llvm.insertvalue %26, %79[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %81 = llvm.insertvalue %26, %80[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %82 = llvm.insertvalue %25, %81[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    llvm.br ^bb1(%13 : i64)
  ^bb1(%83: i64):  // 2 preds: ^bb0, ^bb5
    %84 = llvm.icmp "slt" %83, %14 : i64
    llvm.cond_br %84, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    llvm.br ^bb3(%13 : i64)
  ^bb3(%85: i64):  // 2 preds: ^bb2, ^bb4
    %86 = llvm.icmp "slt" %85, %16 : i64
    llvm.cond_br %86, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %87 = llvm.extractvalue %45[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %88 = llvm.mlir.constant(262144 : index) : i64
    %89 = llvm.mul %83, %88  : i64
    %90 = llvm.add %89, %85  : i64
    %91 = llvm.getelementptr %87[%90] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %92 = llvm.load %91 : !llvm.ptr -> f32
    %93 = llvm.mlir.constant(128 : index) : i64
    %94 = llvm.mul %85, %93  : i64
    %95 = llvm.add %94, %83  : i64
    %96 = llvm.getelementptr %74[%95] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %92, %96 : f32, !llvm.ptr
    %97 = llvm.add %85, %15  : i64
    llvm.br ^bb3(%97 : i64)
  ^bb5:  // pred: ^bb3
    %98 = llvm.add %83, %15  : i64
    llvm.br ^bb1(%98 : i64)
  ^bb6:  // pred: ^bb1
    %99 = llvm.mlir.zero : !llvm.ptr
    %100 = llvm.getelementptr %99[8192] : (!llvm.ptr) -> !llvm.ptr, f32
    %101 = llvm.ptrtoint %100 : !llvm.ptr to i64
    %102 = llvm.add %101, %24  : i64
    %103 = llvm.call @malloc(%102) : (i64) -> !llvm.ptr
    %104 = llvm.ptrtoint %103 : !llvm.ptr to i64
    %105 = llvm.sub %24, %25  : i64
    %106 = llvm.add %104, %105  : i64
    %107 = llvm.urem %106, %24  : i64
    %108 = llvm.sub %106, %107  : i64
    %109 = llvm.inttoptr %108 : i64 to !llvm.ptr
    %110 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
    %111 = llvm.insertvalue %103, %110[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %112 = llvm.insertvalue %109, %111[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %113 = llvm.insertvalue %23, %112[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %114 = llvm.insertvalue %24, %113[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %115 = llvm.insertvalue %26, %114[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %116 = llvm.insertvalue %26, %115[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %117 = llvm.insertvalue %25, %116[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
    %118 = llvm.call @xsmm_unary_dispatch(%43, %42, %41, %40, %42, %40, %39) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    %119 = llvm.ptrtoint %109 : !llvm.ptr to i64
    %120 = llvm.inttoptr %119 : i64 to !llvm.ptr<f32>
    llvm.call @xsmm_unary_scalar_invoke(%42, %118, %28, %120, %13) : (i64, i64, f32, !llvm.ptr<f32>, i64) -> ()
    %121 = llvm.mlir.zero : !llvm.ptr
    %122 = llvm.getelementptr %121[16777216] : (!llvm.ptr) -> !llvm.ptr, f32
    %123 = llvm.ptrtoint %122 : !llvm.ptr to i64
    %124 = llvm.add %123, %24  : i64
    %125 = llvm.call @malloc(%124) : (i64) -> !llvm.ptr
    %126 = llvm.ptrtoint %125 : !llvm.ptr to i64
    %127 = llvm.sub %24, %25  : i64
    %128 = llvm.add %126, %127  : i64
    %129 = llvm.urem %128, %24  : i64
    %130 = llvm.sub %128, %129  : i64
    %131 = llvm.inttoptr %130 : i64 to !llvm.ptr
    %132 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %133 = llvm.insertvalue %125, %132[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %134 = llvm.insertvalue %131, %133[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %135 = llvm.insertvalue %23, %134[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %136 = llvm.insertvalue %21, %135[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %137 = llvm.insertvalue %22, %136[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %138 = llvm.insertvalue %20, %137[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %139 = llvm.insertvalue %20, %138[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %140 = llvm.insertvalue %18, %139[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %141 = llvm.insertvalue %19, %140[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %142 = llvm.insertvalue %20, %141[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %143 = llvm.insertvalue %25, %142[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %144 = llvm.call @xsmm_unary_dispatch(%42, %42, %38, %38, %37, %38, %36) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%13, %13) to (%32, %31) step (%15, %15) {
        %186 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %187 = llvm.mul %arg6, %12  : i64
        %188 = llvm.mul %arg7, %11  : i64
        %189 = llvm.add %187, %188  : i64
        %190 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %191 = llvm.insertvalue %50, %190[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %192 = llvm.insertvalue %51, %191[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %193 = llvm.insertvalue %189, %192[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %194 = llvm.mlir.constant(32 : index) : i64
        %195 = llvm.insertvalue %194, %193[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %196 = llvm.mlir.constant(262144 : index) : i64
        %197 = llvm.insertvalue %196, %195[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %198 = llvm.mlir.constant(32 : index) : i64
        %199 = llvm.insertvalue %198, %197[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %200 = llvm.mlir.constant(1 : index) : i64
        %201 = llvm.insertvalue %200, %199[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %202 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %203 = llvm.insertvalue %125, %202[0] : !llvm.struct<(ptr, ptr, i64)> 
        %204 = llvm.insertvalue %131, %203[1] : !llvm.struct<(ptr, ptr, i64)> 
        %205 = llvm.mlir.constant(0 : index) : i64
        %206 = llvm.insertvalue %205, %204[2] : !llvm.struct<(ptr, ptr, i64)> 
        %207 = llvm.mul %arg6, %12  : i64
        %208 = llvm.mul %arg7, %10  : i64
        %209 = llvm.add %207, %208  : i64
        %210 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %211 = llvm.insertvalue %125, %210[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %212 = llvm.insertvalue %131, %211[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %213 = llvm.insertvalue %209, %212[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %214 = llvm.mlir.constant(32 : index) : i64
        %215 = llvm.insertvalue %214, %213[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %216 = llvm.mlir.constant(32 : index) : i64
        %217 = llvm.insertvalue %216, %215[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.mlir.constant(32 : index) : i64
        %219 = llvm.insertvalue %218, %217[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %220 = llvm.mlir.constant(1 : index) : i64
        %221 = llvm.insertvalue %220, %219[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %222 = llvm.extractvalue %201[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %223 = llvm.extractvalue %201[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %224 = llvm.ptrtoint %223 : !llvm.ptr to i64
        %225 = llvm.inttoptr %224 : i64 to !llvm.ptr<f32>
        %226 = llvm.extractvalue %221[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = llvm.extractvalue %221[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = llvm.ptrtoint %227 : !llvm.ptr to i64
        %229 = llvm.inttoptr %228 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%42, %144, %225, %222, %229, %226) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %186 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %145 = llvm.mlir.zero : !llvm.ptr
    %146 = llvm.getelementptr %145[33554432] : (!llvm.ptr) -> !llvm.ptr, f32
    %147 = llvm.ptrtoint %146 : !llvm.ptr to i64
    %148 = llvm.add %147, %24  : i64
    %149 = llvm.call @malloc(%148) : (i64) -> !llvm.ptr
    %150 = llvm.ptrtoint %149 : !llvm.ptr to i64
    %151 = llvm.sub %24, %25  : i64
    %152 = llvm.add %150, %151  : i64
    %153 = llvm.urem %152, %24  : i64
    %154 = llvm.sub %152, %153  : i64
    %155 = llvm.inttoptr %154 : i64 to !llvm.ptr
    %156 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)>
    %157 = llvm.insertvalue %149, %156[0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %158 = llvm.insertvalue %155, %157[1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %159 = llvm.insertvalue %23, %158[2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %160 = llvm.insertvalue %17, %159[3, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %161 = llvm.insertvalue %22, %160[3, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %162 = llvm.insertvalue %20, %161[3, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %163 = llvm.insertvalue %20, %162[3, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %164 = llvm.insertvalue %18, %163[4, 0] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %165 = llvm.insertvalue %19, %164[4, 1] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %166 = llvm.insertvalue %20, %165[4, 2] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %167 = llvm.insertvalue %25, %166[4, 3] : !llvm.struct<(ptr, ptr, i64, array<4 x i64>, array<4 x i64>)> 
    %168 = llvm.call @xsmm_unary_dispatch(%42, %42, %38, %38, %40, %38, %36) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%13, %13) to (%30, %31) step (%15, %15) {
        %186 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %187 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %188 = llvm.insertvalue %68, %187[0] : !llvm.struct<(ptr, ptr, i64)> 
        %189 = llvm.insertvalue %74, %188[1] : !llvm.struct<(ptr, ptr, i64)> 
        %190 = llvm.mlir.constant(0 : index) : i64
        %191 = llvm.insertvalue %190, %189[2] : !llvm.struct<(ptr, ptr, i64)> 
        %192 = llvm.mul %arg7, %9  : i64
        %193 = llvm.mul %arg6, %11  : i64
        %194 = llvm.add %192, %193  : i64
        %195 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %196 = llvm.insertvalue %68, %195[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %197 = llvm.insertvalue %74, %196[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %198 = llvm.insertvalue %194, %197[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %199 = llvm.mlir.constant(32 : index) : i64
        %200 = llvm.insertvalue %199, %198[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %201 = llvm.mlir.constant(128 : index) : i64
        %202 = llvm.insertvalue %201, %200[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %203 = llvm.mlir.constant(32 : index) : i64
        %204 = llvm.insertvalue %203, %202[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %205 = llvm.mlir.constant(1 : index) : i64
        %206 = llvm.insertvalue %205, %204[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %207 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %208 = llvm.insertvalue %149, %207[0] : !llvm.struct<(ptr, ptr, i64)> 
        %209 = llvm.insertvalue %155, %208[1] : !llvm.struct<(ptr, ptr, i64)> 
        %210 = llvm.mlir.constant(0 : index) : i64
        %211 = llvm.insertvalue %210, %209[2] : !llvm.struct<(ptr, ptr, i64)> 
        %212 = llvm.mul %arg6, %12  : i64
        %213 = llvm.mul %arg7, %10  : i64
        %214 = llvm.add %212, %213  : i64
        %215 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %216 = llvm.insertvalue %149, %215[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %217 = llvm.insertvalue %155, %216[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %218 = llvm.insertvalue %214, %217[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %219 = llvm.mlir.constant(32 : index) : i64
        %220 = llvm.insertvalue %219, %218[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %221 = llvm.mlir.constant(32 : index) : i64
        %222 = llvm.insertvalue %221, %220[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %223 = llvm.mlir.constant(32 : index) : i64
        %224 = llvm.insertvalue %223, %222[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %225 = llvm.mlir.constant(1 : index) : i64
        %226 = llvm.insertvalue %225, %224[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %227 = llvm.extractvalue %206[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %228 = llvm.extractvalue %206[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %229 = llvm.ptrtoint %228 : !llvm.ptr to i64
        %230 = llvm.inttoptr %229 : i64 to !llvm.ptr<f32>
        %231 = llvm.extractvalue %226[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %232 = llvm.extractvalue %226[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %233 = llvm.ptrtoint %232 : !llvm.ptr to i64
        %234 = llvm.inttoptr %233 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%42, %168, %230, %227, %234, %231) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.intr.stackrestore %186 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    %169 = llvm.extractvalue %47[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %170 = llvm.extractvalue %47[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %171 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
    %172 = llvm.insertvalue %169, %171[0] : !llvm.struct<(ptr, ptr, i64)> 
    %173 = llvm.insertvalue %170, %172[1] : !llvm.struct<(ptr, ptr, i64)> 
    %174 = llvm.mlir.constant(0 : index) : i64
    %175 = llvm.insertvalue %174, %173[2] : !llvm.struct<(ptr, ptr, i64)> 
    %176 = llvm.extractvalue %47[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %177 = llvm.extractvalue %47[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %178 = llvm.extractvalue %47[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
    %179 = llvm.call @xsmm_brgemm_dispatch(%42, %38, %38, %38, %38, %38, %38, %35, %35, %34) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %180 = llvm.call @xsmm_binary_dispatch(%42, %42, %38, %38, %38, %38, %38, %34) : (i64, i64, i64, i64, i64, i64, i64, i64) -> i64
    %181 = llvm.call @xsmm_unary_dispatch(%33, %42, %38, %38, %38, %40, %36) : (i64, i64, i64, i64, i64, i64, i64) -> i64
    omp.parallel {
      omp.wsloop for  (%arg6, %arg7) : i64 = (%13, %13) to (%32, %30) step (%15, %15) {
        %186 = llvm.intr.stacksave : !llvm.ptr
        llvm.br ^bb1
      ^bb1:  // pred: ^bb0
        %187 = llvm.mlir.zero : !llvm.ptr
        %188 = llvm.getelementptr %187[1024] : (!llvm.ptr) -> !llvm.ptr, f32
        %189 = llvm.ptrtoint %188 : !llvm.ptr to i64
        %190 = llvm.add %189, %24  : i64
        %191 = llvm.call @malloc(%190) : (i64) -> !llvm.ptr
        %192 = llvm.ptrtoint %191 : !llvm.ptr to i64
        %193 = llvm.sub %24, %25  : i64
        %194 = llvm.add %192, %193  : i64
        %195 = llvm.urem %194, %24  : i64
        %196 = llvm.sub %194, %195  : i64
        %197 = llvm.inttoptr %196 : i64 to !llvm.ptr
        %198 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %199 = llvm.insertvalue %125, %198[0] : !llvm.struct<(ptr, ptr, i64)> 
        %200 = llvm.insertvalue %131, %199[1] : !llvm.struct<(ptr, ptr, i64)> 
        %201 = llvm.mlir.constant(0 : index) : i64
        %202 = llvm.insertvalue %201, %200[2] : !llvm.struct<(ptr, ptr, i64)> 
        %203 = llvm.mul %arg6, %12  : i64
        %204 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %205 = llvm.insertvalue %125, %204[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %206 = llvm.insertvalue %131, %205[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %207 = llvm.insertvalue %203, %206[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %208 = llvm.mlir.constant(8192 : index) : i64
        %209 = llvm.insertvalue %208, %207[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %210 = llvm.mlir.constant(1024 : index) : i64
        %211 = llvm.insertvalue %210, %209[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %212 = llvm.mlir.constant(32 : index) : i64
        %213 = llvm.insertvalue %212, %211[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %214 = llvm.mlir.constant(32 : index) : i64
        %215 = llvm.insertvalue %214, %213[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %216 = llvm.mlir.constant(32 : index) : i64
        %217 = llvm.insertvalue %216, %215[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %218 = llvm.mlir.constant(1 : index) : i64
        %219 = llvm.insertvalue %218, %217[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %220 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %221 = llvm.insertvalue %149, %220[0] : !llvm.struct<(ptr, ptr, i64)> 
        %222 = llvm.insertvalue %155, %221[1] : !llvm.struct<(ptr, ptr, i64)> 
        %223 = llvm.mlir.constant(0 : index) : i64
        %224 = llvm.insertvalue %223, %222[2] : !llvm.struct<(ptr, ptr, i64)> 
        %225 = llvm.mul %arg7, %12  : i64
        %226 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)>
        %227 = llvm.insertvalue %149, %226[0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %228 = llvm.insertvalue %155, %227[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %229 = llvm.insertvalue %225, %228[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %230 = llvm.mlir.constant(8192 : index) : i64
        %231 = llvm.insertvalue %230, %229[3, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %232 = llvm.mlir.constant(1024 : index) : i64
        %233 = llvm.insertvalue %232, %231[4, 0] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %234 = llvm.mlir.constant(32 : index) : i64
        %235 = llvm.insertvalue %234, %233[3, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %236 = llvm.mlir.constant(32 : index) : i64
        %237 = llvm.insertvalue %236, %235[4, 1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %238 = llvm.mlir.constant(32 : index) : i64
        %239 = llvm.insertvalue %238, %237[3, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %240 = llvm.mlir.constant(1 : index) : i64
        %241 = llvm.insertvalue %240, %239[4, 2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %242 = llvm.extractvalue %219[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %243 = llvm.extractvalue %219[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %244 = llvm.ptrtoint %243 : !llvm.ptr to i64
        %245 = llvm.inttoptr %244 : i64 to !llvm.ptr<f32>
        %246 = llvm.extractvalue %241[2] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %247 = llvm.extractvalue %241[1] : !llvm.struct<(ptr, ptr, i64, array<3 x i64>, array<3 x i64>)> 
        %248 = llvm.ptrtoint %247 : !llvm.ptr to i64
        %249 = llvm.inttoptr %248 : i64 to !llvm.ptr<f32>
        %250 = llvm.ptrtoint %197 : !llvm.ptr to i64
        %251 = llvm.inttoptr %250 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_brgemm_invoke(%42, %179, %245, %242, %249, %246, %251, %13, %29) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, i64) -> ()
        %252 = llvm.mul %arg7, %11  : i64
        %253 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)>
        %254 = llvm.insertvalue %169, %253[0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %255 = llvm.insertvalue %170, %254[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %256 = llvm.insertvalue %252, %255[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %257 = llvm.mlir.constant(32 : index) : i64
        %258 = llvm.insertvalue %257, %256[3, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %259 = llvm.mlir.constant(1 : index) : i64
        %260 = llvm.insertvalue %259, %258[4, 0] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %261 = llvm.extractvalue %260[2] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %262 = llvm.extractvalue %260[1] : !llvm.struct<(ptr, ptr, i64, array<1 x i64>, array<1 x i64>)> 
        %263 = llvm.ptrtoint %262 : !llvm.ptr to i64
        %264 = llvm.inttoptr %263 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_binary_invoke(%42, %180, %264, %261, %251, %13, %251, %13) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        %265 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64)>
        %266 = llvm.insertvalue %103, %265[0] : !llvm.struct<(ptr, ptr, i64)> 
        %267 = llvm.insertvalue %109, %266[1] : !llvm.struct<(ptr, ptr, i64)> 
        %268 = llvm.mlir.constant(0 : index) : i64
        %269 = llvm.insertvalue %268, %267[2] : !llvm.struct<(ptr, ptr, i64)> 
        %270 = llvm.mul %arg6, %9  : i64
        %271 = llvm.mul %arg7, %11  : i64
        %272 = llvm.add %270, %271  : i64
        %273 = llvm.mlir.undef : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>
        %274 = llvm.insertvalue %103, %273[0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %275 = llvm.insertvalue %109, %274[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %276 = llvm.insertvalue %272, %275[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %277 = llvm.mlir.constant(32 : index) : i64
        %278 = llvm.insertvalue %277, %276[3, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %279 = llvm.mlir.constant(128 : index) : i64
        %280 = llvm.insertvalue %279, %278[4, 0] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %281 = llvm.mlir.constant(32 : index) : i64
        %282 = llvm.insertvalue %281, %280[3, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %283 = llvm.mlir.constant(1 : index) : i64
        %284 = llvm.insertvalue %283, %282[4, 1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %285 = llvm.extractvalue %284[2] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %286 = llvm.extractvalue %284[1] : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> 
        %287 = llvm.ptrtoint %286 : !llvm.ptr to i64
        %288 = llvm.inttoptr %287 : i64 to !llvm.ptr<f32>
        llvm.call @xsmm_unary_invoke(%42, %181, %251, %13, %288, %285) : (i64, i64, !llvm.ptr<f32>, i64, !llvm.ptr<f32>, i64) -> ()
        llvm.call @free(%191) : (!llvm.ptr) -> ()
        llvm.intr.stackrestore %186 : !llvm.ptr
        llvm.br ^bb2
      ^bb2:  // pred: ^bb1
        omp.yield
      }
      omp.terminator
    }
    llvm.call @free(%68) : (!llvm.ptr) -> ()
    llvm.call @free(%125) : (!llvm.ptr) -> ()
    llvm.call @free(%149) : (!llvm.ptr) -> ()
    %182 = llvm.alloca %25 x !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)> : (i64) -> !llvm.ptr
    llvm.store %117, %182 : !llvm.struct<(ptr, ptr, i64, array<2 x i64>, array<2 x i64>)>, !llvm.ptr
    %183 = llvm.mlir.undef : !llvm.struct<(i64, ptr)>
    %184 = llvm.insertvalue %21, %183[0] : !llvm.struct<(i64, ptr)> 
    %185 = llvm.insertvalue %182, %184[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @refbackend_consume_func_return_mrf32(%21, %182) : (i64, !llvm.ptr) -> ()
    llvm.return
  }
  llvm.func @_mlir_ciface_MLP(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr) attributes {llvm.emit_c_interface} {
    %0 = llvm.load %arg0 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %1 = llvm.extractvalue %0[0] : !llvm.struct<(i64, ptr)> 
    %2 = llvm.extractvalue %0[1] : !llvm.struct<(i64, ptr)> 
    %3 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %4 = llvm.extractvalue %3[0] : !llvm.struct<(i64, ptr)> 
    %5 = llvm.extractvalue %3[1] : !llvm.struct<(i64, ptr)> 
    %6 = llvm.load %arg2 : !llvm.ptr -> !llvm.struct<(i64, ptr)>
    %7 = llvm.extractvalue %6[0] : !llvm.struct<(i64, ptr)> 
    %8 = llvm.extractvalue %6[1] : !llvm.struct<(i64, ptr)> 
    llvm.call @MLP(%1, %2, %4, %5, %7, %8) : (i64, !llvm.ptr, i64, !llvm.ptr, i64, !llvm.ptr) -> ()
    llvm.return
  }
}


